{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**해당 노트북은 [이수안 컴퓨터 연구소 | 토픽모델링](https://www.youtube.com/watch?v=Xt607xhpF6U&list=PL7ZVZgsnLwEEoHQAElEPg7l7T6nt25I3N&index=6)의 강의 자료 및 강의를 참조하고 [위키독스 - 딥러닝을 활용한 자연어처리 입문|19강 토픽모델링](https://wikidocs.net/30707)의 내용을 기반으로 하는 복습내용을 더하여 작성하였습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqJ4qz2uwHHH"
   },
   "source": [
    "# 토픽 모델링(Topic Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwtnnVf3DcnI"
   },
   "source": [
    "* 토픽 모델링은 문서 집합에서 주제를 찾아내기 위한 기술\n",
    "* 토픽 모델링은 '특정 주제에 관한 문서에서는 특정 단어가 자주 등장할 것이다'라는 직관을 기반\n",
    "* 예를 들어, 주제가 '개'인 문서에서는 개의 품종, 개의 특성을 나타내는 단어가 다른 문서에 비해 많이 등장\n",
    "* 주로 사용되는 토픽 모델링 방법은 잠재 의미 분석과 잠재 디리클레 할당 기법이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_DR7Dg5XLn-"
   },
   "source": [
    "## 잠재 의미 분석(Latent Semantic Analysis)\n",
    "\n",
    "* 잠재 의미 분석(LSA)은 주로 문서 색인의 의미 검색에 사용\n",
    "* 잠재 의미 인덱싱(Latent Semantic Indexing, LSI)로도 알려져 있음\n",
    "* LSA의 목표는 문서와 단어의 기반이 되는 잠재적인 토픽을 발견하는 것\n",
    "* 잠재적인 토픽은 문서에 있는 단어들의 분포를 주도한다고 가정\n",
    "\n",
    "* LSA 방법\n",
    "  + 문서 모음(DTM: Document term Metrix)에서 생성한 문서-단어 행렬(Document Term Matrix)에서 단어-토픽 행렬(Term-Topic Matrix)과 토픽-중요도 행렬(Topic-Importance Matrix), 그리고 토픽-문서 행렬(Topic-Document Matrix)로 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[복습내용]**\n",
    "* cf) Bow를 기반으로 하는 DTM이나 TF-IDF(문서내 빈도 - 역빈도)은 기본적으로 빈도 기반 표현방식이기 때문에 단어의 의미를 고려하지 못한다. \n",
    "    - 이를 해결하는 방법론으로 처음 LSA가 고안되었다. \n",
    "* LSA는 기본적으로 **특이값 분해(Sigular Value Decomposition, SVD)를 기반으로 한다. \n",
    "  \n",
    "##### 특이값 분해 (SVD, Singular Value Decomposition) \n",
    "cf) 여기에서, svd는 실수 백터 공간에 한정하여 설명하고 있다.   \n",
    "\n",
    "* SVD란 A가 (m x n)의 행렬일 때,\n",
    "$$A = U\\sum V^T$$ \n",
    "로 A를 분해하는 것을 의미한다.    \n",
    "    - U : (m x m ) 직교행렬\n",
    "    - ∑ : (m x n) 직사각 대각행렬, 이때, 대각행렬의 원소 값들이 **'특이값'** \n",
    "    - V^T : (n x n) 직교행렬\n",
    "* 위에서 설명한 SVD를 풀 SVD(full SVD)라고 합니다. 하지만 LSA의 경우 풀 SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD(truncated SVD)를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 절단된 SVD는 대각 행렬 Σ의 대각 원소의 값 중에서 상위값 t개만 남게 됩니다. 절단된 SVD를 수행하면 값의 손실이 일어나므로 기존의 행렬 A를 복구할 수 없습니다. 또한, U행렬과 V행렬의 t열까지만 남깁니다. 여기서 t는 우리가 찾고자하는 토픽의 수를 반영한 하이퍼파라미터값입니다. 하이퍼파라미터란 사용자가 직접 값을 선택하며 성능에 영향을 주는 매개변수를 말합니다. t를 선택하는 것은 쉽지 않은 일입니다. t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 노이즈를 제거할 수 있기 때문입니다.\n",
    "\n",
    "* 이렇게 일부 벡터들을 삭제하는 것을 데이터의 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게되면 당연히 풀 SVD를 하였을 때보다 직관적으로 계산 비용이 낮아지는 효과를 얻을 수 있습니다.\n",
    "\n",
    "* 하지만 계산 비용이 낮아지는 것 외에도 상대적으로 중요하지 않은 정보를 삭제하는 효과를 갖고 있는데, 이는 영상 처리 분야에서는 노이즈를 제거한다는 의미를 갖고 자연어 처리 분야에서는 설명력이 낮은 정보를 삭제하고 설명력이 높은 정보를 남긴다는 의미를 갖고 있습니다. 즉, 다시 말하면 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해줍니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [LSA의 장점]\n",
    "    - 쉽고 빠른 구현\n",
    "    - 잠재적의미를 이끌어내어 유사도계산등에서 좋은성능을보임. \n",
    "* [LSA의 단점]\n",
    "    - SVD특성상 이미계산되어있는 LSA에 새로운데이터를 추가해 계산하려고하면 보통처음부터 계산해야 한다. \n",
    "       -즉 새로운정보에 대해 업데이트가어렵다. 따라서 최근 Word2Vec 등의 신경망 기반 모형들이 각광을 받음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVQH1rH0bJLI"
   },
   "source": [
    "## 잠재 디리클레 할당(Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex3DjGiFKmk1"
   },
   "source": [
    "* 잠재 디레클레 할당(LDA)은 대표적인 토픽 모델링  알고리즘 중 하나\n",
    "\n",
    "* 잠재 디레클레 할당 방법\n",
    "  1. 사용자가 토픽이 개수를 지정해 알고리즘에 전달\n",
    "  2. 모든 단어들을 토픽 중 하나에 할당\n",
    "  3. 모든 문서의 모든 단어에 대해 단어 w가 가정에 의거, $p(t|d)$, $p(w|t)$에 따라 토픽을 재할당, 이를 반복, 이 때 가정은 자신만이 잘못된 토픽에 할당되어 있고 다른 모든 단어는 올바른 토픽에 할당된다는 것을 의미    \n",
    "\n",
    "* $p(t|d)$ - 문서 d의 단어들 중 토픽 t에 해당하는 비율\n",
    "* 해당 문서의 자주 등장하는 다른 단어의 토픽이 해당 단어의 토픽이 될 가능성이 높음을 의미    \n",
    "\n",
    "* $p(w|t)$- 단어 w를 가지고 있는 모든 문서들 중  토픽 t가 할당된 비율\n",
    "* 다른 문서에서 단어 w에 많이 할당된 토픽이 해당 단어의 토픽이 될 가능성이 높음을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[복습내용]** \n",
    "\n",
    "CF) LDA에서, 문서들은 여러 토픽(주제)들의 혼합으로 구성되어 있으며, 각각의 토픽들은 확률분포에 기반하여 단어들을 생성한다고 가정합니다. 따라서, LDA에서는 문서가 주어지면, 문서가 생성되던 단어를 역추적한다.   \n",
    "  \n",
    "**뭐라는 거니?**  \n",
    "사람들이 글을 쓸때의 방식이 \n",
    "1. 나는 이 문서를 작성하기위해서 이런 주제들을 넣을거고, \n",
    "2. 이런주제들을위해서는 이런단어들을넣을거야!   \n",
    "로 이루어진다는게 골자   \n",
    "  \n",
    "  \n",
    "1) 문서에 사용할 단어의 개수 N을 정합니다.\n",
    "- Ex) 5개의 단어를 정하였습니다.\n",
    "2) 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정합니다.\n",
    "- Ex) 위 예제와 같이 토픽이 2개라고 하였을 때 강아지 토픽을 60%, 과일 토픽을 40%와 같이 선택할 수 있습니다.\n",
    "3) 문서에 사용할 각 단어를 (아래와 같이) 정합니다.\n",
    "3-1) 토픽 분포에서 토픽 T를 확률적으로 고릅니다.\n",
    "- Ex) 60% 확률로 강아지 토픽을 선택하고, 40% 확률로 과일 토픽을 선택할 수 있습니다.\n",
    "3-2) 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고릅니다.\n",
    "- Ex) 강아지 토픽을 선택하였다면, 33% 확률로 강아지란 단어를 선택할 수 있습니다. 이제 3)을 반복하면서 문서를 완성합니다.\n",
    "\n",
    "이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engneering)을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA 수행 과정 \n",
    "1) 사용자는 알고리즘에게 토픽의 개수 k를 알려줍니다.  \n",
    "LDA는 토픽의 개수 k를 입력받으면, k개의 토픽이 M개의 전체 문서에 걸쳐 분포되어 있다고 가정합니다.\n",
    "\n",
    "2) 모든 단어를 k개 중 하나의 토픽에 할당합니다.  \n",
    "이제 LDA는 모든 문서의 모든 단어에 대해서 k개 중 하나의 토픽을 랜덤으로 할당합니다.  \n",
    "이 작업이 끝나면 각 문서는 토픽을 가지며, 토픽은 단어 분포를 가지는 상태입니다.  \n",
    "물론 랜덤으로 할당하였기 때문에 사실 이 결과는 전부 틀린 상태입니다.  \n",
    "(만약 한 단어가 한 문서에서 2회 이상 등장하였다면, 각 단어는 서로 다른 토픽에 할당되었을 수도 있습니다.)\n",
    "\n",
    "3) 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행합니다. (iterative)\n",
    "\n",
    "3-1) 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정합니다. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당됩니다.\n",
    "- p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\n",
    "- p(word w | topic t) : 각 토픽들 t에서 해당 단어 w의 분포\n",
    "\n",
    "이를 반복하면, 모든 할당이 완료된 수렴 상태가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LDA1.png](https://wikidocs.net/images/page/30708/lda1.PNG)\n",
    "\n",
    "위의 그림은 두 개의 문서 doc1과 doc2를 보여줍니다. 여기서는 doc1의 세번째 단어 apple의 토픽을 결정하고자 합니다.\n",
    "![LDA2.png](https://wikidocs.net/images/page/30708/lda3.PNG)\n",
    "우선 첫번째로 사용하는 기준은 문서 doc1의 단어들이 어떤 토픽에 해당하는지를 봅니다. doc1의 모든 단어들은 토픽 A와 토픽 B에 50 대 50의 비율로 할당되어져 있으므로, 이 기준에 따르면 단어 apple은 토픽 A 또는 토픽 B 둘 중 어디에도 속할 가능성이 있습니다.\n",
    "\n",
    "\n",
    "![LDA3.png](https://wikidocs.net/images/page/30708/lda2.PNG)\n",
    "두번째 기준은 단어 apple이 전체 문서에서 어떤 토픽에 할당되어져 있는지를 봅니다. 이 기준에 따르면 단어 apple은 토픽 B에 할당될 가능성이 높습니다. 이러한 두 가지 기준을 참고하여 LDA는 doc1의 apple을 어떤 토픽에 할당할지 결정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RxDTpTts3ma"
   },
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1yVomvgs6l3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E56d3KXroenQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCo9S9RlrfQg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xzZu6Iprj5B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi3PJ7dMsF8K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViSra6MYuM9x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6BleIc4Jwcs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKywEnyAJxhc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ8Zkrar1umI"
   },
   "source": [
    "## Gensim을 이용한 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7g3xjKA0WI-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhM28edm261U"
   },
   "source": [
    "### 잠재 의미 분석을 위한 `LsiModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqz7Xmp63LPa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWoLB_a164Gq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVPXSgtK97Ez"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbI05R9O3Kc4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxSqZmtV2QzN"
   },
   "source": [
    "### 잠재 디리클레 할당을 위한 `LdaModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2X_cCIRJbe1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2Yn8BwIJqwt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD8aNQIK2q-p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eziNq5h54tzF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLljp8WR3qHB"
   },
   "source": [
    "## 토픽 모델링 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6lhKMEwJqAd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRYHzP_9JuC3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "_6 토픽 모델링(Topic Modeling).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
