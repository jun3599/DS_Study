{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 기사 스크래핑 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(query, page_num=10):\n",
    "    \n",
    "    # 저장할 데이터 프레임을 만든다. \n",
    "    news_df = pd.DataFrame(columns=('Title','Link','Press','Datetime','Article'))\n",
    "    ind = 0\n",
    "    \n",
    "    # 파라미터로 질의 언어를 받아옴 \n",
    "    url_query = quote(query) # 받은 질의문을 utf-8으로 변환 \n",
    "    url = \"https://search.naver.com/search.naver?&where=news&sm=nws_hty&query=\" + url_query\n",
    "    \n",
    "   \n",
    "    for _ in range(0,page_num):\n",
    "            \n",
    "        # search_url = urllib.request.urlopen(url,headers={'User-Agent':'Mozilla/5.0'}).read() # 해당 유알엘을 읽기모드로 여는듯\n",
    "        search_url = requests.get(url,headers ={'User-Agent':'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(search_url.text,'html.parser') # 뷰숩으로 탐색기 열어주는듯\n",
    "    \n",
    "        links = soup.find_all('div',{'class':'info_group'})\n",
    "\n",
    " \n",
    "        for link in links: # 해당 테그를 가진 것들을 모두 들고와 반복작업 \n",
    "            #print(\"*********link*******\"+str(link))\n",
    "            # 발행 언론사 정보(글씨)를 가져옵니다. \n",
    "            press = link.find('a',{'class':'info press'}).get_text()\n",
    "            #press = link.find('a',{'class':'info press'}).find('span').get_text()\n",
    "\n",
    "\n",
    "            news_url = link.find('a',{'class':'info'}).get('href') # a테그를 가지면 네이버 뉴스 테그가 심어진것임\n",
    "            # 이때 그걸 가진것에서 --> href :: 뉴스 링크를 가져옴 \n",
    "\n",
    "            if (news_url == '#'): # 뉴스 유알엘이 없는경우 \n",
    "                print('url없음!',end=\"\")\n",
    "                continue\n",
    "                # 넘어가 버림 \n",
    "            else: \n",
    "                # 링크가 있는경우 누르고, 내용 가져오게 \n",
    "\n",
    "                # 링크 타고 이동  \n",
    "                news_link = requests.get(news_url,headers ={'User-Agent':'Mozilla/5.0'})\n",
    "                news_html = BeautifulSoup(news_link.text,'html.parser')\n",
    "                # print \n",
    "                #print(news_html)\n",
    "                #print(\"\\n\\n\\nn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "                try:\n",
    "                    # 제목 가져오기 \n",
    "                    title = news_html.find('h3',{'id':'articleTitle'}).get_text()\n",
    "                    #print(\"TITLE*********:\"+ title)\n",
    "\n",
    "                    # 기사 날짜 담아오기 \n",
    "                    datetime = news_html.find('span',{'class':'t11'}).get_text()\n",
    "\n",
    "                    # 기사 본문 가져오기 , 텍스트만 \n",
    "                    article = news_html.find('div',{'id':'articleBodyContents'}).get_text()\n",
    "\n",
    "                    news_df.loc[ind] = [title, news_url,press,datetime,article]\n",
    "                    ind += 1\n",
    "                    print(\"#\",end=\"\")\n",
    "                except:\n",
    "                    print('!',end=\"\")\n",
    "                    continue\n",
    "        try:\n",
    "            next_page = soup.find('a',{'class':'btn_next'}).get('href')\n",
    "            url = \"https://search.naver.com/search.naver\" + next_page \n",
    "        except:\n",
    "            print(\"@\")\n",
    "            break\n",
    "        \n",
    "    return news_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 언론사 , 작성일, 제목, 본문\n",
    "\n",
    "def get_news(url):\n",
    "    result = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(result.text, \"html.parser\")\n",
    "    press = soup.find('div',{'class':'article_header'}).find('img')['title']\n",
    "    date = soup.find('div',{'class':'sponsor'}).find('span',{'class':'t11'}).get_text();\n",
    "    title = soup.find('h3',{'id':'articleTitle'}).get_text();\n",
    "    before_content = soup.find('div',{'id':'articleBodyContents'});\n",
    "\n",
    "    content = \"\"\n",
    "    for item in before_content.contents:\n",
    "        stripped = str(item).strip()\n",
    "        if stripped == \"\":\n",
    "            continue\n",
    "        if stripped[0] not in [\"<\", \"/\"]:\n",
    "            content += str(item).strip()\n",
    "    content = content.replace(\"&apos;\", \"\")\n",
    "    content = content.replace(\"본문 내용TV플레이어\", \"\")\n",
    "    print(f'언론사: {press}\\n작성일: {date}\\n기사제목: {title}\\n본문: {content}\\n\\n')\n",
    "\n",
    "user_input = input(\"키워드: \")\n",
    "page_number = int(input(\"찾을 페이지 수: \"))\n",
    "print()\n",
    "\n",
    "for number in range(page_number):\n",
    "    print(f'***** {number+1}페이지 탐색 중 *****')\n",
    "    number = (number*10)+1\n",
    "    url = f'https://search.naver.com/search.naver?query={user_input}&where=news&start={number}'\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, \"html.parser\")\n",
    "    info_group = soup.find_all('div',{'class':\"info_group\"})\n",
    "    for info in info_group:\n",
    "        try:\n",
    "            news_url = info.find_all('a')[1]['href']\n",
    "            get_news(news_url)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성공 버전!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # 접속후 웹스크립트에 대한 접근을 위함\n",
    "from urllib.parse import quote  # 입력어를 인코딩\n",
    "import requests  # 페이지에 접근\n",
    "import pandas as pd  # 결과 저장\n",
    "\n",
    "\n",
    "def get_news(query=\"\", page_num=10, ds=\"2021.05.03\", de=\"2021.05.03\"):\n",
    "    result_df = pd.DataFrame(columns=('title', 'news_url', 'press', 'date', 'content'))\n",
    "    ind = 0\n",
    "\n",
    "    # 시작날짜, 메인함수에서 str(yyyy.mm.dd) 형식으로 받기!\n",
    "    # ds\n",
    "    # de\n",
    "\n",
    "    #print(type(query))\n",
    "    url = f'https://search.naver.com/search.naver?&where=news&sm=nws_hty&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds={ds}&de={de}&query={query}'\n",
    "    # sort=2 옵션은 오래된 순 정렬\n",
    "    #print(url)\n",
    "\n",
    "    while (True):\n",
    "\n",
    "        # 일단 해당 유알엘에 접속후 뷰리플숩으로 스크립트 열기\n",
    "        search_url = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(search_url.text, \"html.parser\")\n",
    "\n",
    "        links = soup.find_all('div', {'class': 'news_area'})\n",
    "\n",
    "        for link in links:\n",
    "            ## 제목 검사\n",
    "            ## 통과한것만 infogroup 가져오기\n",
    "            ## 탐색\n",
    "            title = link.find('a', {'class': 'news_tit'})['title'];\n",
    "            #print(f'title: {title}\\nquery in?: {query in title}')\n",
    "\n",
    "            if (query in title):\n",
    "                info_group = link.find_all('div', {'class': \"info_group\"})\n",
    "                for info in info_group:\n",
    "\n",
    "                    try:\n",
    "                        news_url = info.find_all('a')[1]['href']\n",
    "                        press, date, content = get_contents(news_url)\n",
    "                        result_df.loc[ind] = [title, news_url, press, date, content]\n",
    "                        ind += 1\n",
    "                        print(\"#\", end=\"\")\n",
    "                    except:\n",
    "                        print('!', end=\"\")\n",
    "                        pass\n",
    "            else:\n",
    "                print(\"$\", end=\"\")\n",
    "                pass\n",
    "\n",
    "        print(\"\\n==========================\\n\")\n",
    "\n",
    "        try:\n",
    "           next_page = soup.find('a', {'class': 'btn_next'}).get('href')\n",
    "           url = \"https://search.naver.com/search.naver\" + next_page\n",
    "        except:\n",
    "           print(\"\\n\")\n",
    "           print(\"--전체 페이지 확인 끝--\")\n",
    "           break\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def get_contents(news_url):\n",
    "    if (news_url == '#'):  # 뉴스 유알엘이 없는경우\n",
    "        print('url없음!', end=\"\")\n",
    "        pass\n",
    "        # 넘어가 버림\n",
    "    else:\n",
    "        # 링크가 있는경우 누르고, 내용 가져오게\n",
    "\n",
    "        # 링크 타고 이동\n",
    "        news_link = requests.get(news_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        news_html = BeautifulSoup(news_link.text, 'html.parser')\n",
    "        press = news_html.find('div', {'class': 'article_header'}).find('img')['title']\n",
    "        date = news_html.find('div', {'class': 'sponsor'}).find('span', {'class': 't11'}).get_text();\n",
    "\n",
    "        before_content = news_html.find('div', {'id': 'articleBodyContents'});\n",
    "\n",
    "        content = \"\"\n",
    "        for item in before_content.contents:\n",
    "            stripped = str(item).strip()\n",
    "            if stripped == \"\":\n",
    "                continue\n",
    "            if stripped[0] not in [\"<\", \"/\"]:\n",
    "                content += str(item).strip()\n",
    "        content = content.replace(\"&apos;\", \"\")\n",
    "        content = content.replace(\"본문 내용TV플레이어\", \"\")\n",
    "\n",
    "        return press, date, content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드: 카카오톡\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$!$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$##$$$\n",
      "==========================\n",
      "\n",
      "$!$$$$$!!$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "#$$$$$$!$$\n",
      "==========================\n",
      "\n",
      "$$$!$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$!$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$!$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$!$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$!$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$#$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$!!$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$!$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$!\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$#$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$#$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$!$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$!$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$$$$\n",
      "==========================\n",
      "\n",
      "$$$$$$$\n",
      "==========================\n",
      "\n",
      "\n",
      "\n",
      "--전체 페이지 확인 끝--\n",
      "                                     title  \\\n",
      "0              3명에게 '카카오톡 선물' 하면 5000원 받는다   \n",
      "1          카카오톡 선물하기, 감사 선물 보내면 ‘쇼핑포인트’ 준다   \n",
      "2                    캐세이퍼시픽, 카카오톡 공식 채널 개설   \n",
      "3                 \"카카오톡 선물하기 이용하면 포인트 드려요\"   \n",
      "4  카카오톡 선물하기, \"5월 맞이 감사 선물하고 쇼핑포인트도 받아가세요\"   \n",
      "5              카카오톡 친구 3명에게 선물하면 5000원 받는다   \n",
      "\n",
      "                                            news_url        press  \\\n",
      "0  https://news.naver.com/main/read.nhn?mode=LSD&...        아시아경제   \n",
      "1  https://news.naver.com/main/read.nhn?mode=LSD&...       디지털데일리   \n",
      "2  https://news.naver.com/main/read.nhn?mode=LSD&...       파이낸셜뉴스   \n",
      "3  https://news.naver.com/main/read.nhn?mode=LSD&...  ZDNet Korea   \n",
      "4  https://news.naver.com/main/read.nhn?mode=LSD&...        스포츠조선   \n",
      "5  https://news.naver.com/main/read.nhn?mode=LSD&...        머니투데이   \n",
      "\n",
      "                  date                                            content  \n",
      "0  2021.05.03. 오전 9:23  [아시아경제 부애리 기자] 카카오커머스가 3명에게 '카카오톡 선물하기'를 하면 50...  \n",
      "1  2021.05.03. 오전 9:23  [디지털데일리 권하영기자] 카카오커머스(대표 홍은택)는 카카오톡 선물하기에서 5월 ...  \n",
      "2  2021.05.03. 오전 9:57  [파이낸셜뉴스] 캐세이퍼시픽항공은 한국 고객들의 항공 이용 편의를 향상하고 소통을 ...  \n",
      "3  2021.05.03. 오후 1:40  (지디넷코리아=안희정  기자)카카오커머스(대표 홍은택)가 운영하는 카카오톡 선물하기...  \n",
      "4  2021.05.03. 오후 3:02  카카오커머스가 운영하는 카카오톡 선물하기에서 5월 감사의 달을 맞아 다양한 선물 상...  \n",
      "5  2021.05.03. 오후 3:23  [머니투데이 이동우 기자]카카오커머스는 5월 감사의 달을 맞아 카카오톡 선물하기로 ...  \n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"키워드: \")\n",
    "df = get_news(query=user_input)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('test_crawling.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
