{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ce93c5",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1ac23",
   "metadata": {},
   "source": [
    "# 학습 내용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07291633",
   "metadata": {},
   "source": [
    "## 신경망이란\n",
    "* Deep Learning이라는 개념은 Neural Network의 또 다른 이름 입니다. \n",
    "* 기본적으로 신경망 모형은 인간의 신경계를 모방하여, 기계가 사람의 사고를 구현할 수 있게 하는것에 목적을 두고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c8e20",
   "metadata": {},
   "source": [
    "## 퍼셉트론 \n",
    "* 퍼셉트론(perceptron)은 인공신경망의 한 종류로서, 1957년에 코넬 항공 연구소(Cornell Aeronautical Lab)의 프랑크 로젠블라트 (Frank Rosenblatt)에 의해 고안되었다. 이것은 가장 간단한 형태의 피드포워드(Feedforward) 네트워크 - 선형분류기- 로도 볼 수 있다.[위키백과 || 퍼셉트론](https://ko.wikipedia.org/wiki/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0)\n",
    "* 다시 말해, 우리가 앞서 살펴보던 하나의 회귀 모델을 퍼셉트론이라고 할 수 있다. \n",
    "* 하나의 퍼셉트론의 구조는 아래와 같이 구성되어 있다. \n",
    "![perceptron1.png](https://t1.daumcdn.net/cfile/tistory/261AC64E56C2D0E724)\n",
    "  이전에 보았던 모습처럼, 각 입력에 대해 각각의 가중치가 곱해져, 선형 방정식이 도출되며, 이 방정식을 일정한 활성화 함수의 입력으로 하여, 결과 값을 도출하는 구조이다.  \n",
    "  하지만, 하나의 퍼셉트론 연산을 통해 구할 수 있는 결과에는 한계가 있었다. 대표적인 예시로는 XOR문제이며, 이를 해결하기 위해 심층 신경망의 개념이 고안되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb861a",
   "metadata": {},
   "source": [
    "## 신경망(Neural Network) \n",
    "* 신경망은 여러개의 퍼셉트론을 순차적으로 연결하여, 하나의 모형으로 사용하는 기법이다. \n",
    "![neuralnetwork1.png](https://dnddnjs.gitbooks.io/rl/content/dqn14.png)\n",
    "* 이전 layer의 결과값이 다음 layer의 입력값으로 이를통한 결과 값이, 다음 층의 입력으로 도입되며, 최종적인 결과값을 도출하는 방식이다. \n",
    "* 위의 그림에서, 각각의 동그라미는 하나의 퍼셉트론을 의미하며, 각각의 퍼셉트론은 서로 다른 가중치W와 편향 b를 갖고 있다.\n",
    "* 신경망의 학습 과정은 순전파(forward propagation)와 역전파(backpropagation)과정을 통해 이루어진다. \n",
    "    - 순전파(forward propagation) : 입력으로 부터, 결과까지 순차적으로 값이 계산되며 이전의 입력의 결과가 다음 layer의 입력으로 전달되는 과정을 의미한다. \n",
    "    - 역전파(backpropagation) : 최종적으로 도출된 예측값과 실제값의 차이를 기반으로 계산된 cost값을 줄이기 위해, 제일 마지막 layer부터 입력 layer까지 순차적으로 가중치를 개선해 나가는 과정을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00dc78",
   "metadata": {},
   "source": [
    "![NN.png](https://blog.kakaocdn.net/dn/br8TAC/btqCFyHetkC/4aBXyx2wwkgmzlAdrEMi6K/img.png)\n",
    "* 위의 그림은 3개의 퍼셉트론을 연결한 2layer model의 예시이다. 여기서 눈여겨 볼 점은 1layer층의 두개의 퍼셉트론을 하나의 행렬 구조를 통해 나타낼 수 있다는 점이다. (이는 이전에 사용해봤던 Tensorflow의 sequencial 모형에서 Dense(128)이 사실은 하나의 행렬의 구조 안에 128개의 층이 함께 있다는것을 보여주는것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fa1c1",
   "metadata": {},
   "source": [
    "## Cost Minimization 과 Backpropagation\n",
    "* 이전에 살펴본 회귀, 분류 모형에서는 기본적으로, 실제값과 예측값의 차이를 통해 비용함수를 정의하고, 이에 학습률을 적용해 가중치를 갱신했습니다. \n",
    ".    \n",
    "* 단순선형 회귀모형에서의 가설함수와 비용함수 \n",
    "$$H(x) = W*x + b$$\n",
    "$$cost(W) = 1/m * \\sum_{i=1}^{m} (H(x_i) -y_i)^2$$\n",
    "(cost함수를 가중치W와 편향 b로 각각 미분하여 값을 갱신) \n",
    ".  \n",
    "* 하지만, 가중치가 수없이 많은 신경망에서는 가중치를 갱신하는것은 매우 복잡한 일입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c43df5",
   "metadata": {},
   "source": [
    "##### Backpropagation\n",
    "![back.png](https://joohyoungblog.files.wordpress.com/2019/05/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2019-05-03-e1848be185a9e18492e185ae-4.57.28-e1556870573600.png?w=600)\n",
    "* 역전파 알고리즘은 신경망 모형에서 가장 흔하게 사용되는 가중치 업데이트 방식입니다. \n",
    "* 예측이나 분류의 결과와 실제값의 차이를 통해 계산된 cost function을 각 노드의 변수로 편미분 하여 이를 최소화 합니다. \n",
    "* 편미분은 chain Rule을 통해 이루어집니다. \n",
    "![chain.png](https://slidetodoc.com/presentation_image/21ed0371295dd3978ced8de2d7082387/image-7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702f2a9",
   "metadata": {},
   "source": [
    "# XOR problem \n",
    "* 초기 신경망 이론은 논리 연산에 대한 구현 시도를 목적으로 했습니다.  \n",
    "  처음 기본적인 AND와 OR에 대한 학습은 성공적으로 구현하였으나, XOR 즉, 배타적논리합의 구현은 하나의 퍼셉트론으로 학습시킬 수 없다는 결론을 얻었습니다.   \n",
    "  이는, 추후 다층신경망의 등장으로 해결이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d7191",
   "metadata": {},
   "source": [
    "## Logistic Regression을 통한 구현\n",
    "* 초기 하나의 퍼셉트론을 통해 구현을 시도했던 방식을 실시해봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f061e",
   "metadata": {},
   "source": [
    "### 분석 환경 설정하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e348f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf \n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62417178",
   "metadata": {},
   "source": [
    "### 데이터셋 준비하기\n",
    "* x_data가 2차원 배열이기에 2차원 공간에 표현하여 x1과 x2를 기준으로 y_data 0과 1로 구분하는 예제입니다\n",
    "* 붉은색과 푸른색으로 0과 1을 표시해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34cf8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a511a",
   "metadata": {},
   "source": [
    "### 데이터를 tensorflow의 모형에 들어갈 수 있는 구조로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a104b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data,y_data)).batch(len(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db34be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
      "array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]])>, <tf.Tensor: shape=(4, 1), dtype=int32, numpy=\n",
      "array([[0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0]])>)]\n"
     ]
    }
   ],
   "source": [
    "print(list(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0feae",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices 함수는 tf.data.Dataset 를 생성하는 함수로 입력된 텐서로부터 slices를 생성합니다. 예를 들어 MNIST의 학습데이터 (60000, 28, 28)가 입력되면, 60000개의 slices로 만들고 각각의 slice는 28×28의 이미지 크기를 갖게 됩니다.  \n",
    ".batch()는 데이터 배치의 크기를 괄호안의 숫자의 크기로 지정하는 메소드 입니다.   \n",
    "[텐서플로우 dataset 강좌](https://www.youtube.com/watch?v=b74Y7mpb98o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b91030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692a83e",
   "metadata": {},
   "source": [
    "cast메소드는 인자로 받은 행렬 혹은 텐서의 내부에 있는 각 데이텀들의 자료형을 뒤에 나오는 데이터 형으로 변환하는 메소드입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c34967",
   "metadata": {},
   "source": [
    "### 초기 가중치 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee43cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.]\n",
      " [0.]], B = [0.]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros((2,1)), name='weight')\n",
    "b = tf.Variable(tf.zeros((1,)), name= 'bias')\n",
    "\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35e6d7",
   "metadata": {},
   "source": [
    "### 가설함수 정의 : sigmoid 함수를 가설로 선언합니다. \n",
    "\n",
    "* Sigmoid는 아래 그래프와 같이 0과 1의 값만을 리턴합니다 tf.sigmoid(tf.matmul(X, W) + b)와 같습니다\n",
    "$$ \\begin{align} sigmoid(x) & = \\frac{1}{1+e^{-x}} \\\\\\\\\\ \\end{align} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db026560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a866e",
   "metadata": {},
   "source": [
    "### Cost 함수 정의 \n",
    "* 가설을 검증할 cost함수를 정의 합니다. \n",
    "$$ \\begin{align} cost(h(x),y) & = −log(h(x)) & if & y=1 \\\\\\\\\\\n",
    "cost(h(x),y) & = -log(1−h(x)) & if & y=0 \\end{align} $$\n",
    "* 위 두수식을 합치면 아래과 같습니다 \n",
    "$$ \\begin{align} cost(h(x),y) & = −y log(h(x))−(1−y)log(1−h(x)) \\end{align} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0278c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fa6ec",
   "metadata": {},
   "source": [
    "### accuracy 함수 정의 \n",
    "* 추론한 값은 0.5를 기준(Sigmoid 그래프 참조)로 0과 1의 값을 리턴합니다.\n",
    "* Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaabd14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb6e69",
   "metadata": {},
   "source": [
    "### Gradient 값 계산 함수 구현 \n",
    "* GradientTape를 통해 경사값을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2cd6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a89330",
   "metadata": {},
   "source": [
    "### 전체 학습 모형 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d708c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6931\n",
      "Iter: 100, Loss: 0.6931\n",
      "Iter: 200, Loss: 0.6931\n",
      "Iter: 300, Loss: 0.6931\n",
      "Iter: 400, Loss: 0.6931\n",
      "Iter: 500, Loss: 0.6931\n",
      "Iter: 600, Loss: 0.6931\n",
      "Iter: 700, Loss: 0.6931\n",
      "Iter: 800, Loss: 0.6931\n",
      "Iter: 900, Loss: 0.6931\n",
      "Iter: 1000, Loss: 0.6931\n",
      "W = [[0.]\n",
      " [0.]], B = [0.]\n",
      "Testset Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in dataset:\n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(logistic_regression(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48247c42",
   "metadata": {},
   "source": [
    "Loss가 반복에도 감소하지 않음을 볼 수 있으며, 정확도가 언제나 최대 0.5임을 확인 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e5f03",
   "metadata": {},
   "source": [
    "## NN을 통한 구현 \n",
    "![nn.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRkaOBvADbVev4lzQmoQ1LiaSW0gxccHY1iiA&usqp=CAU)\n",
    "* 위의 그림과 같이 총 3개의 퍼셉트론으로 구성된 모형을 만듭니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b89eb",
   "metadata": {},
   "source": [
    "### 분석환경 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626c30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782385e2",
   "metadata": {},
   "source": [
    "### 데이터셋 준비하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f17176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c26b5",
   "metadata": {},
   "source": [
    "### 데이터를 tensorflow의 모형에 들어갈 수 있는 구조로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "933f69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d440146",
   "metadata": {},
   "source": [
    "### 초기 가중치 설정 \n",
    "* 3개의 퍼셉트론 각각에 가중치와 편향값을 전달합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4656c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1 -1 \n",
    "W1 = tf.Variable(tf.random.normal((2, 1)), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal((1,)), name='bias1')\n",
    "# layer 1 - 2\n",
    "W2 = tf.Variable(tf.random.normal((2, 1)), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal((1,)), name='bias2')\n",
    "\n",
    "# layer 2\n",
    "W3 = tf.Variable(tf.random.normal((2, 1)), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal((1,)), name='bias3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335fc66",
   "metadata": {},
   "source": [
    "### 모형 구성 및 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1292bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모형을 구현하는 함수 - concat과 reshape 이후 모습에 대해 보다 자세히 살펴보자 \n",
    "def neural_net(features):\n",
    "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "    layer3 = tf.concat([layer1, layer2],-1)\n",
    "    layer3 = tf.reshape(layer3, shape = [-1,2])\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
    "    return hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5310c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 위 함수의 결과를 직관적으로 이해하기 위한 확인작업입니다! \n",
    "\n",
    "# layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
    "# print(layer1) # 시그모이드를 거친 값은 각각의 자리의 확률값을 갖기 때문에 총 4행1열 \n",
    "\n",
    "# layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
    "# print(layer2)\n",
    "\n",
    "# layer3 = tf.concat([layer1, layer2],-1)\n",
    "# print(layer3)\n",
    "# # tf.concat은 두 텐서를 연결 짓는 함수로서, 뒤에 따라오는 옵션에 따라, 붙이는 방법이 달라집니다. \n",
    "# # 텐서의 구조가 (가장큰차원, 행, 열) 구조라면 \n",
    "# # (0 or -3)은 가장 큰 차원을 기준으로 합치며 \n",
    "# # (1 or -2)는 행을 기준으로 \n",
    "# # (2 or -1)은 열을 기준으로 데이터를 통합합니다. \n",
    "\n",
    "# layer3 = tf.reshape(layer3, shape = [-1,2])\n",
    "# print(layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313758ee",
   "metadata": {},
   "source": [
    "[tf.concat 설명](https://supermemi.tistory.com/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d1c5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용함수 정의\n",
    "# 가설함수를 통해 도출된 값과 실제 레이블의 차이를 통해 값을 도출합니다. \n",
    "def loss_fn(hypothesis, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae183dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도출된 비용의 기울기를 반영하는 옵티마이져 선언 \n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "881663a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e34fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(neural_net(features),labels)\n",
    "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cfe706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.7217\n",
      "Iter: 5000, Loss: 0.6913\n",
      "Iter: 10000, Loss: 0.6871\n",
      "Iter: 15000, Loss: 0.6819\n",
      "Iter: 20000, Loss: 0.6723\n",
      "Iter: 25000, Loss: 0.6551\n",
      "Testset Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# EPOCHS = 50000\n",
    "EPOCHS = 30000\n",
    "\n",
    "# 전체 데이터를 epochs 만큼 돌려줍니다. 한번의 epoeh에 데이터 전체가 반복적으로 입력됩니다. \n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in dataset:\n",
    "        # 위에서 선언한 함수를 이용해 데이터의 자료형을 변환합니다. \n",
    "        features, labels = preprocess_data(features, labels)\n",
    "        grads = grad(neural_net(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3]))\n",
    "        if step % 5000 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(neural_net(features),labels)))\n",
    "x_data, y_data = preprocess_data(x_data, y_data)\n",
    "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eafe31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for features, labels in dataset:\n",
    "#     print('features: ' ,features,'labels: ' ,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8815e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92407c60",
   "metadata": {},
   "source": [
    "# Relu 활성함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108aa03",
   "metadata": {},
   "source": [
    "* 기본적으로 신경망 모형은 실제값과 예측값의 차이를 통해 Cost의 값을 구하고,  \n",
    "  이를 미분한 Gradient값을 모형의 끝부터 가장 앞까지의 가중치에 반영하는  \n",
    "  **Backpropagation** 과정을 거쳐 학습을 진행합니다. \n",
    "  \n",
    "* 하지만, softmax 혹은 sigmoid함수를 이용할 경우 신경망의 깊이가 깊어질 수록  앞부분에 전달되는 gradient의 값이 0에 가까워져 학습이 불가능한 **기울기 손실 문제 (Vanishing Gradient)** 가 발생하기도 합니다. \n",
    "  \n",
    "  이는 sigmoid함수의 미분값의 특성에 의한 문제이며, 이를 해소하기 위해 Relu함수 뿐만 아닌 다양한 활성 함수를 이용하기도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b97eb",
   "metadata": {},
   "source": [
    "##### 왜 Relu인가? \n",
    "![sigmoid_d.png](https://miro.medium.com/max/381/1*qPi-T6gffkCHr0NV0iYF3w.png)\n",
    "* 위의 그림은 하나의 sigmoid 함수와 그 미분값의 그래프 입니다. 위에서 보이는 바와 같이, 시그모이드 함수의 양 끝단에서의 미분값은 0에 수렴합니다.  \n",
    "  따라서, 이러한 시그모이드 함수를 이용하는 층이 여러겹으로 쌓일수록, 0에 가까운 값이 많이 곱해지며, 결국 신경망의 앞부분에 전달되는 error term의 값이 0에 가까워 학습이 더이상 진행되지 않습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d91078",
   "metadata": {},
   "source": [
    "##### Relu\n",
    "![relu_d.png](https://blog.kakaocdn.net/dn/bMTr5U/btqEN9yAUyV/TZoRwPulghnYjFxKNiItJK/img.png)\n",
    "* 위의 그림은 Relu함수의 모습을 보여줍니다. 보이는 바와 같이 Relu함수는 0이상의 x에 대해서는 x의 값을 그대로 반환하며, 0이하의 값에서는 0을 반환하는 함수 입니다. \n",
    "* 따라서, 해당 함수의 기울기는 0이상에서는 1, 0이하에서는 0의 기울기를 갖습니다. \n",
    "* 하지만, 이 렐루 함수 또한 0이하인 구간에서는 0의 기울기를 갖기 때문에 이러한 경우에는 vanishing gradient 문제를 해결하지 못합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1970b3c",
   "metadata": {},
   "source": [
    "##### leaky Relu 와 다양한 활성화 함수 \n",
    "![active.png](https://thebook.io/img/080228/122.jpg) \n",
    "* 따라서 최근에는, 이러한 단점을 보완하는 다양한 종류의 활성함수를 사용하기도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab704a8c",
   "metadata": {},
   "source": [
    "## sigmoid 활성함수를 이용한 학습 \n",
    "* 숫자에 대한 이미지와 정답에 대한 데이터가 들어있는 minist 데이터를 분류합니다. \n",
    "* 우선 sigmoid 함수의 vanishing gradient problem을 확인해봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200ed18",
   "metadata": {},
   "source": [
    "### 주요 라이브러리 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af75de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "# 원핫인코딩을 위한 라이브러리 \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터셋을 포함하는 라이브러리 \n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from time import time \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7014eb1",
   "metadata": {},
   "source": [
    "### checkpoint 함수 구현 \n",
    "* 모형이 학습 중, 예기치 못한 일로 학습을 중단했을때, 종료 이전에 각각의 가중치와 loss값을 저장해두는 함수를 구현합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84be8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 저장된 값을 불러오는 load함수 \n",
    "def load(model, checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir) # checkpoint_dir에 기존에 학습해둔 모델이 있는지 확인 \n",
    "    if ckpt :\n",
    "        # 기존에 사용하던 모델이 있는 경우 \n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        checkpoint = tf.train.Checkpoint(dnn=model)\n",
    "        checkpoint.restore(save_path=os.path.join(checkpoint_dir, ckpt_name))\n",
    "        counter = int(ckpt_name.split('-')[1])\n",
    "        print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "640b83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 폴더 경로가 없으면, 해당 폴더를 생성하는 함수 \n",
    "def check_folder(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0a6bb",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n",
    "* 데이터는 tensorflow에서 제공하는 minist 데이터를 이용합니다. \n",
    "* mnist 데이터 설명 \n",
    "    - mnist데이터는 총 8만장의 이미지와 정답값으로 이루어져 있습니다. \n",
    "    - 한장의 이미지는 (28 x 28)의 그리드로 이루어져 있으며, 총 784칸으로 구성되어 있습니다.  \n",
    "    - 한 칸에는 밝기 값을 나타내는 (0~255) 사이의 숫자가 존재합니다.(흑백만 구분)  \n",
    "    ![mnist.png](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1536c",
   "metadata": {},
   "source": [
    "##### Tensor 구조 기초 \n",
    "![tensor1.png](https://mblogthumb-phinf.pstatic.net/MjAyMDA1MjVfMjQ0/MDAxNTkwMzcxNTgxMjgx.zVM_8O3aWx1DUjeKFx-FRXeHS1KVHWfdSiOBE2--1k4g.6z_U8ssbX1lwelBEqUxjtyutk2y6HSNTvk5a7pTYIWkg.PNG.nabilera1/image.png?type=w800)\n",
    "![tensor2.png](https://lh3.googleusercontent.com/proxy/kNPmmH8eN_V9OOYOFLTYnkLWjI9cdIplM7E03mdZG80bN41eaiU172TYP_YnXPkuBzG2jkbDtUiHsJHqHWZt5Mt9O9yf0i7VQxKxyC8c4g)\n",
    "\n",
    "![channel.png](https://cphinf.pstatic.net/mooc/20210716_235/1626420513368wU8Kt_PNG/mceclip0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7115b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    (train_data, train_labels),(test_data, test_labels) = mnist.load_data()\n",
    "    \n",
    "    # 데이터에 channel에 대한 차원을 추가 합니다. \n",
    "    # 기본적으로  이미지 분류에서는 한칸마다 각각의 rgb값을 담기 위한 (3개의 채널)을 갖지먼, 미니스트의 경우 흑백의 채널 1가지만 갖습니다. \n",
    "    train_data = np.expand_dims(train_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1]\n",
    "    test_data = np.expand_dims(test_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1]\n",
    "    \n",
    "    # 각각의 값의 범위가 0~255로 매우 크기 때문에 정규화를 진행합니다. \n",
    "    train_data, test_data = normalize(train_data, test_data)\n",
    "    \n",
    "    # 각각의 레이블값을 범주화 시킵니다(원핫인코딩 )\n",
    "    train_labels = to_categorical(train_labels, 10) # [N,] -> [N, 10]\n",
    "    test_labels = to_categorical(test_labels, 10) # [N,] -> [N, 10]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "548dcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 이용한 정규화 함수를 구현합니다. \n",
    "# 단순히 각각의 자리를 255로 나누어 0~1사의의 값으로 한정지어 줍니다. \n",
    "def normalize(train_data, test_data):\n",
    "    train_data = train_data.astype(np.float32) / 255.0\n",
    "    test_data = test_data.astype(np.float32) / 255.0\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39a1e8",
   "metadata": {},
   "source": [
    "### 수행중 성과 지표 함수 정의 \n",
    "* 비용함수와\n",
    "* accuracy 계산 함수\n",
    "* 기울기 함수를 구현합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27f850e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, images, labels):\n",
    "    # 모형에 학습용데이터(images)를 입력하여 예측값을 구합니다. \n",
    "    # 동시에 해당 데이터는 학습에 이용합니다. \n",
    "    logits = model(images, training = True)\n",
    "    \n",
    "    # 도출된 예측 레이블과 실제 레이블을 통해 cross enstropy를 계산합니다.\n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits,\n",
    "                                                                  y_true = labels,\n",
    "                                                                  from_logits = True))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4500525",
   "metadata": {},
   "source": [
    "* tf.keras.losses.categorical_crossentropy\n",
    "    - categorical 데이터 사이의 크로스엔트로피를 계산해줍니다. \n",
    "    - logit = True는 모델의 출력값이 sigmoid 또는 linear를 거쳐서 확률이 아닌 값이 나오게 된다면, logit=True라고 표현할 수 있습니다.(말 그대로 확률이 아니라 logit) \n",
    "    - 해당 클래스의 범위에서의 확률을 출력한다면, 이를 logit=False라고 표현할 수 있습니다\n",
    "    [상세설명 출처](https://hwiyong.tistory.com/335)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cd81922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 함수 구현 \n",
    "def accuracy_fn(model, images, labels):\n",
    "    logits = model(images, training=False) # 검증만을 위해 사용 -> false\n",
    "    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a22b5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기를 구해주는 함수 \n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables) \n",
    "# 각각의 변수에 대한 미분값을 모두 리턴 \n",
    "# model.variable 메소드는 각각의 가중치를 모두 리스트 형태로 반환하는듯 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afaec04",
   "metadata": {},
   "source": [
    "### 모형 구현하기 \n",
    "* 클레스와 함수로 모두 구현이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da0cdf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 함수 구현하기 \n",
    "def flatten():\n",
    "    return tf.keras.layers.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f5dbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense 층 선언\n",
    "def dense(label_dim, weight_init):\n",
    "    return tf.keras.layers.Dense(units=label_dim, use_bias=True, \n",
    "                                 kernel_initializer= weight_init)\n",
    "# units는 출력계층의 수 --> 기본적으로 마지막 레이어를 가정하여 ouyput dimension을 초기값으로 줍니다. \n",
    "# use_bias 는 편향값 b를 포함함을 의미합니다. 4\n",
    "# kernal_initalizer로는 추후 랜덤으로 계수값을 지정하는 함수를 전달합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc28ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성함수인 sigmoid 함수 선언 \n",
    "def sigmoid():\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.sigmoid)\n",
    "\n",
    "# 활성 노드층을 정의합니다. 내부의 인자로 sigmoid를 지정합니다/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4b308",
   "metadata": {},
   "source": [
    "#### Class를 활용한 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f3d6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_model_class(tf.keras.Model):\n",
    "    def __init__(self, label_dim):\n",
    "        super(create_model_class, self).__init__()\n",
    "        weight_init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(flatten())\n",
    "\n",
    "        for i in range(2):\n",
    "            self.model.add(dense(256, weight_init))\n",
    "            self.model.add(sigmoid())\n",
    "\n",
    "        self.model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f8465",
   "metadata": {},
   "source": [
    "class 를 통한 선언은 직관적인 이해가 다소 어려운점이 존재해, 함수 형태의 선언을 다시한번 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2436fc",
   "metadata": {},
   "source": [
    "#### 함수를 통한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45363ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_function(label_dim) :\n",
    "    weight_init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    # 모형의 입력을 직선형태로 전환해주는 메소드 \n",
    "    model.add(flatten())\n",
    "\n",
    "    for i in range(2) :\n",
    "        model.add(dense(256, weight_init))\n",
    "        model.add(sigmoid())\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2211bf9",
   "metadata": {},
   "source": [
    "Dense와 같이 분류를 위한 학습 레이어에서는 1차원 데이터로 바꾸어서 학습이 되어야 한다. 이때 Flatten Layer가 2차원 데이터를 1차원 데이터로 바꾸는 역할을 한다.\n",
    "\n",
    "![flatten.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Ft1t1z%2FbtqSZxFUCGY%2FCpIqnWBCezGpcqySMOWO50%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46990b40",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e10d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d4c27d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6d0ef",
   "metadata": {},
   "source": [
    "* 학습용 데이터\n",
    "    - 이미지수 60000개 \n",
    "    - 하나의 이미지 당, 가로 28 세로 28 \n",
    "    - 1칸당 1개의 값 \n",
    "* 평가용 데이터 \n",
    "    - 이미지 수 10000개 \n",
    "    - 나머지 사항은 동일 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee387be5",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c249a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습율 \n",
    "learning_rate = 0.001\n",
    "\n",
    "# batch size 정의 \n",
    "# 한번에 6만여건의 데이터를 넣어주면 컴퓨터 메모리에 큰 과부하를 불러옵니다. \n",
    "# 따라서 배치사이즈 만큼의 데이터만을 모델상에 올려줍니다. \n",
    "batch_size = 128\n",
    "\n",
    "# epoch 정하기 \n",
    "# 전체 훈련용 데이터를 모형에 몇번 넣어줄 것인가? \n",
    "training_epochs = 1 \n",
    "# 따라서, 전체 데이터를 128로 나누어 한번에 반복수를 결정합니다. \n",
    "training_iterations = len(train_x)//batch_size\n",
    "\n",
    "# 최종 출력 레이블의 수 \n",
    "label_dim = 10 \n",
    "\n",
    "# ??? \n",
    "train_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47f33981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 기록할 파일 경로에 대한 정보 \n",
    "\n",
    "\"\"\" Writer \"\"\"\n",
    "checkpoint_dir = 'checkpoints'\n",
    "logs_dir = 'logs'\n",
    "\n",
    "model_dir = 'nn_softmax'\n",
    "\n",
    "checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "check_folder(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\n",
    "logs_dir = os.path.join(logs_dir, model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e86436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'checkpoints'\n",
    "# logs_dir = 'logs'\n",
    "\n",
    "# model_dir = 'nn_softmax'\n",
    "\n",
    "# checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "# print(checkpoint_dir)\n",
    "\n",
    "# print(check_folder(checkpoint_dir))\n",
    "\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\n",
    "# print(checkpoint_prefix)\n",
    "\n",
    "# logs_dir = os.path.join(logs_dir, model_dir)\n",
    "# # print(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecdad4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Graph Input using Dataset API \"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4gAAAFICAYAAAASx9BTAAAgAElEQVR4AeydB7gU1fn/jRqjJhp/ieavKSYxtqBRoxEEQST2bmJMjL3E3nvvYsGKoogIglJEioIKiiIoCKKCCIJSBRFB4fbeOP/n8+6+e2fnzuzO3rt7C/d9n+fe2Zk5c8p3zsyc73nL2ciZGAKGgCFgCBgChoAhYAgYAoaAIWAIGALOuY0MBUPAEDAEDAFDwBAwBAwBQ8AQMAQMAUMABIwgWj8wBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBIwgWkcwBAwBQ8AQMAQMAUPAEDAEDAFDwBAQBFISxM9X1LpLB5Wl/Xt7bnUonN+sq0t7PWWMmFEVmkdJxfpIeTz7bmVoHpy46qXytPk8MLYiZR53j6lIm8f1w8pT5vHk25Vp8wCTyur1ofkMmVYVKY/VBXWhebz5WXWkPBasqg3NY/qimkh5fLioJjQP8o/Sz974LLyfrS6M1s9emhbez6pqovUz7l8quWF4+n5GP0olD4xL38+ufKksVRbu2UnR+hnPV5jwXEa5NyvWhfeziXOj9bM5K8L72SdLo/WzyQvC+9mSNdH62ZhPwvtIXkm0fjZgSuo+EgXTR99M3UdufSV9PyNNKqGMKHVZvz68jwycEq2fgV2YvPpJtH62eE14H5myIFof+XhpeB+J+s2jT4eJffMaIhP1m8c7OEwif/MKw/tZNr55fMuiPDP2zUseO2bjm8cYLpUwBoxyb+ybF7s3MxaHvwtT4Wzn2gcCKQnitIU1rkfPkrR/Qz8MH1AtXF2b9nrKeHxC+GCosGx9pDxuHpH65XH4g+nb8r/+qQfdZ/QtTVuX4x4tSdl7rhlanjYPMCmvCv9YPvR6RaQ8lq8N/1i+8H5lpDxSDaj44EbpI6nIHYP/KHkMfD980L1ibV2kPB4cF97PKqqj9bNrhqTuZ8c/mr6fnf5Maco+Qj9Mhwn9OZXcMiJaPysoDe9nT7wVrY8s/C584D5selXattDWqV+Ff6ze/SJaPxv9cfi7aO430d5FfVNMNK0pjNbPUg2GIFvp7i3nGeikkpOfTP8uOrl36n522aD0/Yy6pCKItDVKe5jECRMGdlHy4B6GCfc+Sh7vzAsnd1G/efTpMLFvXkNkWss3j29IlD6S628e+UepR66/eYwxotSD+5dK2uU3b3X4u6itfPPGzwl/F6a633aufSCQkiB+uarW3Tm6PO1fqoHddwV1aa+njFQEoqxyfaQ8UhFVbud9r1WkzeeZFIND8mAmNB0mqUgIeQz+oCptHpSRajb1tU+rI+XxQ3H4oOy9+dHySDVrP+vrmkj1mLUsfPCPZicdppynvmFCO6PkAW5hAt5R8uD+pRLuf7p80mkhISnp8rj31XCyS/34SKXLg/OlleEEkQmAKHmsyg/vZwy6o+SRSlMNMYiSx0dLwvsZkwhR8khlEcFkVZQ8XvkodR+Jkkf/yeETItzfR8en72fptJDPT07fz6hrKoJIW6O0J9VEBJhHyYN7GCbc+yh5pCKZ7fGbl2pCBKztm5fc47LxzeObGqWv2jcvefzZVr55jImj3F/eN2GSq28eViU6KWAEMQx9Ow4CKQmiQWQIGAKGgCFgCBgChoAhYAgYAm0fAcxrjSC2/fvYHC0wgtgcKFsZhoAhYAgYAoaAIWAIGAKGQAsigEXe2f1K5e+DFG4dLVhFK7qVIJBEEHGqxgTPHFdbyd2xahgChoAhYAgYAoaAIWAIGAKGgCHQjAgkEcRr48FTbhye2iG5GetnRRkChoAhYAgYAoaAIWAIGAKGgCFgCDQTAkYQmwloK8YQMAQMAUPAEDAEDAFDwBAwBAyB1o6AEcTWfoesfoaAIWAIGAKGgCFgCBgChoAhYAg0EwJGEJsJaCvGEDAEDAFDwBAwBAwBQ8AQMAQMgdaOgBHE1n6HrH6GgCFgCBgChoAhYAgYAoaAIWAINBMCRhCbCWgrZsNC4JvCQvfGwoWupi584e4Nq8XWGkPAEDAEDAFDwBAwBAyB9oBAVgjioM8+cz++5x63zQMPCGbnjxsn+8cMHZqEYdR0XPSn3r0lj0enT3cMxsmfv49WrkzKszE7tXV1buG6dW5xXp4rq65uTBY5uYa2aTvbK/Gg3fO+/z4n+GYr09KqKvfLhx5yG911l7vszTezlW2byKct3B+AXF1S4taUlLQJTLNRyajv1qjpqFMu38HZaLPlYQgYAoaAIZAZAqyDeE6/MvmzdRAzw669pc4KQRyzYIEMln/9yCOC3zVvvSX7/xk5MgnPqOm4aK9nnpE8nvv0U1dcWSm/GZB/uXZtUp6Z7HywfLnr+Nxz7if33pvIb5O773YHDhiQSTY5Szv9m28S9coWQTz71VcTeYKf92+L++7LWVsak/GwuXPdVvff73Z76qnGXN5s16wtK3Ob3n23YPmvESOardwoBR09ZEjSPf5/Dz8s/fvuKVNcRU1NlCxC07SV+7Nfv36CwfB580LbksmJvPJyx3tCn51+n37a4PJuAwcmzpPuV716uU79+7unP/7YVXpwX15QkJRO89TtQ9OmNcg7yoGo79ao6SgzF+/gKG2xNIaAIWAIGAK5QaCkYr3r0bNE/sbPaT0Kkty01nJtCgJZIYiTv/5aBj17PP201OXe99+X/Ytefz2pblHTcVH3F16QPEbOny956IC8sVqBhz/80G3sIUg7PvaY2/7hh92P7rrLbdlKiFIuCOLN777rdn/qKfnTQehvHn1U9vfp2zfp/rT0ziMffij3vLUTRHCasHixu3XSJPddcXFLw5ZUvhLEn/Xs6Ziw8fb5nXv3dt+Xlialz2Snrdwf2k5fzxZBHDp3ruSnz89xw4Y1gE0JIlYUv3/88aRJqKOGDHHr16+Xa1YVFyeex+169ZJ8mbDSZ/T5WbMa5B3lQNR3a9R0lJntd3CUdlgaQ8AQMAQMgdwhYAQxd9huaDknEcRPl9W4t+dWu1lfZ6ZpmLN6tQx0VBPXZ+ZM2YeceCVqOq45cfhwyeOdpUsli23jJn1VtbXeLCP9xpR0s3vukfwOf/HFpEF9QUWFe3HOnEQ+mLNCSj/85hvxLxs1f767a/JkSePVBHABGpm3lyxxT370kbvjvfdc748+clO+/jqRl/5Ac0meaCIYIJIOTYHfXFYJIoP6uvXrxceNshk0okVtqmC+yiAXLYJfqBt1pK4MZvGvQ+sUVHbU9rwfb/f8H35IFFdUWSnlUFZ1/F5SH/bPHDNG6gexYV//vmqE1hgtn16fKNw5B8Ycpy96BbPREV984R6YOtXdOXmye2LGDDd1xQpvEjF11jx1O/u775LSsJMJlnoxmD4+Y4ZjcmXI558n6g6GmYgSxGvfflsuw4R6wOzZbvO41vyCceOSsluSl+cwO7z/gw+kbH6DnVcyvT9R8tT8wa/vJ5+42997T+oweM4cwU/P6xaNOu8Cnhu0crN8uGMyrvdE23rVhAmJY6MD+rzmnW7731GjpF+qRo0JJb82VgkikwYIZuwPTp0q1/HMjfvqqwbF9Pzgg0S+DU5meCDquzVqOorP5js4w+ZYckPAEDAEDIEcIGAEMQegbqBZJhHExraRAeWFr78uRIk8ZqxcKftoWbwSNR3XvPDZZ5LHsvx8yYKB16WN9PdS4oG5XXkan0O0DgzoGPD1GDQoMcDjWOfnnxfipm3SQSHnvH9HvPSSDBA1HaSU85DIre+/Pyktg0QVJYiYWWK66M0TrVq6ums+YdtUBFHLhuT/4+WXU5YdtT1/j+MHyVWZu2ZNIm/IOcKA29tW/29IW6YC4dJ8vNf+M962KydMSByG1KtPoV7DlvvtFe0b3jSXvPGGN4n8zgRLiAQaKW+e3t/0wUzETxD12hsmTpQyMJUsqaqSw5OWLQss96c9e7q3PM9uJvcnap60GxN0b1v1N5MuXvm2qMh1ef75Bml556hmrte0aQ3Oa35s0dI1Rqjn/z34oOQNJqqRHb9oUVJ2+i5Qgqgnf/fYY3Itkw5+ySZBjPpujZqOumbzHexvu+0bAoaAIWAIND8CRhCbH/O2WmJWCGJrb/yeTz8tgzTVqqSqr5cEYH6KxgWCogNDNGsql48f7wii8+7Spe6z1asdvklqCvuyx/9JCRUD1R0eeUQ0VATwYZ8Bu5JgJRY6sD1h+HDHwF6JXWPNz7S+mk+QBjGTsqO2JypBxM8UrZCSN4g8+/rn1xZpe1JtMyGImASC+aGDB7uPv/3WofGEAKB58wqaaK0T5tRck4ogRrmP5Ec6+ijlch80b/xH2c9Ewggi2mqtDyQdIe/zxo4VLdsnq1aJ9lgnRTB5VMnk/kTNE/JBfXiu0FqiAQR7niF+e0X722EvvihaRDSFStq4FuH503ujPsbnvvZa4tizn3zizTLybzS41BMLBrT6B/TvL/v++x5GEDHn5nreE37JJkH05237hoAhYAgYAoaAHwEjiH5EbD8MgQ2eIKJhIBgLgzRM2RC0HGjy9O/n8eirnPMSRG/ACILbkAeailRyyODBku62uKkZaXWAC/FRMoip7C/imglM0RAG15TB3/UTJyaK6TpggBy7Lm42mDiR4Y+oBDFd2VHbE5UgajOy6eMWlSBi5qrkH41U1OBA+JVxn/xEgbZkch9PGz1a8sFMWQXzVvIO8nXTNGHbMIKI6bT2rbEB5o6aH2a1pAMTvzl3Y+9PUJ48H5SDv+ACjwmy1kO3Smzpu17/Y72evuiXbPogqub11FGjpBi04dSbSQWvBBFEiLVi7jcn51ojiF4E7bchYAgYAoZArhEwgphrhDec/Dd4gsggVwnAwLhGiEANaAQghgzg0BSqKEFkkOkVBnhoD/3+cBARCNUpI0c6TEs18ISXzCmhusXnk6naGg3m4yUW6p9HHdQUL4iMeOuY7ndUgpiu7KjtaQsEEczQTOlAHv9Hlq7ATDqVRCWI6bDE1JWy8XNTuWL8eDnWmPsdRhCZmNA2ovFWwcwXn1g05ccOHSpmtZoOv0yvRCWIUfKctmKFPHeUhRadvoL2UM2OtVwmdUjDZA5ad/37SzzK8R98RI3rskkQO/TpI+W/9PnnUiX6heLjXY5FCeIfn3hCNNG7PvlkIh1a2iAxghiEih0zBAwBQ8AQyBUCRhBzheyGl+8GTxC5ZUQsZVB3z5QpSXdQNRtBBBE/wFSCuZkSN/JGG4hWQbWVQQTR75905EsvSb10AOkliF5NlpbTGMLgbUNUgpiubCWI6drTVghifnm5g6Rzz3XwzxZfTHzQgiQqQUyHJRMOOoGxd9++TjXQBFr5PG4KGlR+2LEwgqjRK2mXarEhO9pm+sZvH31UTKAVg8YQxEzyfH3hQvfXZ59NwpznSANT0UYCXVEfgkwRddj/t/9zzzWAIlsE0UuqCTLDJBHBqzQIjtc3VgmiYsc9JWosBJd3RZAYQQxCxY4ZAoaAIWAI5AoBI4i5QnbDy7ddEETVEKF18EpTCCJmegwG8Tlk8K1yfDzgSBSCuNMTT0geLMGBtHWC6G+PEkSC86i85wmM4tcWqYYK7UtTxWtiqoFMyPOg+Hp13iA1WhZBgF778kt3cHyJFe4vkU2DJFsEkcillIMZ8Z/79BH/Q7TR+NQ1RsIIoq6HiQ+skl4lZ0SrVDJIVFElOXpM6xHl/mSaJ3njc4jpppJVcFB5Ju6jSQTRqKIEkXUbmyKY/SoWQVuN2kwZShAvfuMNtzQ/P2ntw7A6GEEMQ8aOGwKGgCFgCOQCAQji0Q+XyN/EubYOYi4w3lDyTCKIC76tddMX1bgFqzJfSqI1A/LmokWJgR6BZTREfVMIog6WMX3T5S8IsqHLcaQjiJisMehEe0mAEKQtE8Sg9pz8yivSRkgzwiLhGtWRtvsJogYuweSQpSKaIgR80UE9pAeSSCRJPaYEEbLkJ0I/lJYm0uFHFiTZIogakAaTy2yInyCyrAhLZ4Apbe/vWWdPiZRqwtB2EthFMfLjEuX+RM2TevmFKMWUjZmvysxvv5VjPCf+qMiaxr/VoENXv/WW/1RG+5iMUx8C0xAJWf/UNBxM18WXBFGC6NeqpyrQCGIqdOycIWAIGAKGgCFgCLQUAkkE8dqh5a5HzxJ34/CmDc5bqjGpyvUu3YDWj2UjNEhMY0xMvZqwv/Xr504aMUJC6WsExSCCiAnf6aNHi6+imhWicVDJBUEkyAaaPdXuMeAlWA77+FepZFK2mpimaw++bUo2IEKYMbJkgq5J6SeIBCtRXMj7rFdflbXYvKRG65tui+8p7aR8SPyfeveW3xr9Ugki5qXcM9qECS/H1b8Nc2E1x6Q8tFyQA/607xClUo81huj/O06iKQstGWRE66L5pWur97wSRIiatl/vAabMqj3kGtWmgg8+kEQuhfQomfQTxCj3J2qe+FnSB88YM0Z8eDGjVtNN9cnVdulEA30DbM4fN85xDE1jUN/QdQt5zrEeoB+x6HsmwlIg+iwz8eMVb8Af1qxEohJEluzQ51H7EM+DHsMP08QQMAQMAUPAEDAEDIGWRKDdEEQ0SJjzYWKnA2a2EIZzXnstcQ80SE06H0QuYNkMyCX5MKhmsEs4e/aDCKK3XLQkmLB5B+yZkLREhdP8UNNCb9n6G1KikknZShA1H7ZB7UFT613jD+0h5rga+t9PEKkLSxVAWLx5o61tjKA51kBEkAvuc5+ZMyVvJYhosraJByvylrnvs88mmQ5Tvn9tSm96fuv6fZlgqdptyDN9Ew20kmT61ETfmoDpcFCCSH24v/g1YrKqdfNez3IXSpxJj68ugZi4l+z7CSLXprs/UfNEm6vtpCz+eOYIEOSPnlpWXS3PE+3RtJo+aMkWSBiE0J/W2/Z0v1/98ku5/le9eiXWWvReo8FrwBaJShDRonvr5f/tjZzsLc9+GwKGgCFgCBgChoAh0FwItBuC6AUU80UCgHxXXJxE0Lxpov4urKhwDIrRRIWJEiq0eUvy8hqYVoZd11qPZ9qe1SUlspSBN2BLqrZBEMCJe8TC3k0R8sLcFLPRVEJfIN2c1avTpk2VTybnvi8tTUTcZOkVFUgrWmnIAwQ/l0IAFdZ3XLRuXSARCio73f2Jmic+n5SNvyVbNdUOKpNj5It/HybDkECvb2nQNfQd7id5e6PJBqW1Y4aAIWAIGAKGgCFgCBgCMQTaJUFs7puvhCoT/6TmrmMm5W1o7cmk7dlMO+XrrxPaJEiPSnFlpZhPQhDv9kXe1TS2NQQMAUPAEDAEDAFDwBAwBHKBgBHEXKDqy3NDI1QbWnt8t6vZdjHhZCkEiCDmk5i1YhqpJq97Pv10m9c2NxuYVpAhYAgYAoaAIWAIGAKGQFYQMIKYFRhTZ/Lg1Knu1FGj3CshSyakvrr1nd3Q2tOSCGMGef8HH0jgon369hXTUgIeET01ncllS9bbyjYEDAFDwBAwBAwBQ8AQ2DARMIK4Yd5Xa5UhYAgYAoaAIWAIGAKGgCGQQKC8ar27YXi5/H28tCZx3H4YAn4EjCD6EbF9Q8AQMAQMAUPAEDAEDAFDYANDoKRivSxnx5J24+dUb2Cts+ZkEwEjiNlE0/IyBAwBQ8AQMAQMAUPAEDAEWiECRhBb4U1ppVVKIojvzKt2L02rcpO+sFmFVnq/rFqGgCFgCBgChoAhYAgYAoZAxggYQcwYsnZ7QRJBbLcoWMMNAUPAEDAEDAFDwBAwBAyBDRgBI4gb8M3NctM2KIJYnmKx+izjZtm1AwSKIizG3g5gsCY2AwJVVVWurKysGUqyIjY0BNavX+8qKiraRLOoa9++fV1NTfrgGJ999pn74IMPWm27bLzR/Lemtff11157zS1fvjwtMCUlJW7AgAFp0+UigRHEXKC6YeaZVYJYXV3dYh+qUaNGuU022cSdcsopLXqn3n77bffjH//YjRkzpkn1aEksM6l4cXFxJslbJG0mWH733Xfu1FNPdVtvvbXbaKON5F5efPHFWal3e+sbUUHL5P5EzTMX6XLR14cNG+Z22203t/HGG0t/22abbdzSpUtzUf2c59lW7mPOgWhCAeeff768c/iG8Hf00UenzI0B81//+le35ZZbulmzZqVM2xpOXn/99W7TTTd1n3zyScrq8Axsv/327oQTTghN15L9rbWMN0LByeGJXLwHo1S3tff12tpaeZfvuuuu7ocffkjZpCFDhsj7vnfv3inTRT356quvus0228zNnj077SVGENNCZAniCGSNIE6cONH97Gc/c++9916LgHvPPffIA7fHHnu0SPla6IQJE6QefEAaKy2NZdR6H3744e6ggw6KmrxF0mWKZffu3d3mm2/uIIXjxo1zgwcPdmPHjs1K3dtq37jrrrukT0OYITJ//OMf3RFHHOFeeumlJuOS6f1pcoGNzCAXfX3SpEmC67777uuefvpp6WcMGNatW9fIWrbcZW3lPrYcQtFK/vjjj92IESPk7xe/+IWj36USNIeQQ57NkSNHpkra4ueeeeYZqWf//v1T1gUCsvPOOzu+5YWFhYFpW7q/tZbxRiA4OTyYi/dg1Oq2hb7+1Vdfuf/7v/9znTt3dnV1dSmbdskll7gf/ehH7s0330yZLspJNPK/+c1v3Lnnnps2uRHEtBBZgjgCWSOIL7/8srz8W4og8vJgML9kyZIWvbnZIAEtjWVUAHffffdWTxAzwZJZawZaaBBzIW21byhB7NWrl3v++efd7bff7g444ADBCg1Hug9hKiwzuT+p8sn1uVz09TPOOEMwXLRoUa6rn/P828p9zDkQWSzgt7/9bVqCSHFoDl955RWHhqW1CpYZW221VaR367XXXitaxvnz54c2p6X7W2sZb4QClKMTuXgPZlLVttDXmahhHPHUU0+lbBoax7322svtuOOOrrS0NGXaKCfvvfdet8UWW7i8vLyUyY0gpoTHTnoQaDJBxFRkxowZjs7JQ9GnTx/Z5xh/33//faK4xYsXu7lz58o+D8eHH34oWpqgGXNmRD799FPHhwAzrI8++ijwA8igXstiu2DBgkR5+oMPp7cu5DV06FDJX9M0ZVtQUCC+Eq+//rr4V4CDX4MYpT2ZYEl9V69e7SAdL774osxCrVmzJrQZ2LxPmzbNYdrADDV4YKITJCtXrhQTWcg+bfMKZSjev//9790+++yT2Od4azFzyhRL2sisNPcOPLMlbaFvpGurEkSv6SPPFOZi4AVp9Ir1dS8a4b+7dOnidtppp9AEDEJ5pvLz8yUNzzvvlTDzPO4JZJM0aCf9A4WFCxeKfwyDkdGjRyfy5Z35/vvvh9Yj1YnGPGeVlZXSLgZSQSTgm2++SbxHaDvp0Ob72+OtF+0mjb4LeYcFCdop/NrI84svvgj8pmTynQoqI+xYJu9g8khFEPFZ1fewbsk/laCNo+24P8ycOTPUDzDV+9+bfybtYTLkpz/9qfv222+9WTT4jQYGE9SrrrqqwTkOZNLfMn1+onxPo4w3qGdj+nBgg+MHtVzq6BfGSfQBxlS5kEy/+Zk8P1Ewj9rXsz3OY1wKrqtWrWoAq9YpTCFx8MEHi5uKf/zkz2jKlCnyDWXStanCfcLMlIncVGIEMRU6ds6LQBJBXJVf55asqXXfFaRWjXsz2HbbbaWDM1AM+uvXr18iOf6Be+65p5C4P/zhD4n02223nfv8888T6XjQf/7znyfOa76QkbVr1ybS8eOcc85JShdkksOAhDweeughd9xxxyWlv/TSS5Pyy2QHJ3U+ZOo/pPVk6yWIUduTCZaPPvpooh2YKVAmppFBs1YMBn/5y18m0mg9/YMzcDrvvPMkHbO95IsvHqRShQkAvT5ou8MOO2jSFt1mgqVWFIJPm7z3Ts9lum0rfSNKu4IIItcRVIVn989//nMiG+vrCSjS/thvv/3kfRiWkMEy/REyc+ONNyY9dxdeeGHSZQxiDj300KQ0mB7yvKqcdNJJYjqN+RPvLGaumSz7yU9+Is/61VdfrUkjbzN9zqZOnSq+ZZSPSwLtO/LII5Pe6zfddJO8r3gevd+BX/3qVw0IJWT32GOPlXzwQcdvjzxPPvnkBm0gKAQkhfP6161btwZBJaJ+pxoUkOJA1HewN4tUBHHFihWJNmhbpk+f7r088Zv3OpiCj6Zly7fTK1He/5o+k/YwSOa+/O9//9PLQ7dMOvHdCSP4mfS3TJ6fqN/TKOMNGpdJHw4Fw3OCgD08p3/5y18c5ESFbxX30v8+0PPZ2Gb6zY/6/ETFPGpfz/Y4D5LPe4r3i19Ug/3cc8/5T8n++PHj5b6kM6cmcRYdN28AACAASURBVMeOHd3vfve7wMmqwMxTHDzttNPEBSSVVQ/faEgif9W1rdfqIEUz7VQzIZBEEK8dWu569CxxNw4vj1w8L3IeYH2JDB8+XPY5xp93VpMXB0EY8K3gBcrsMB8aPgj/+te/EmXSge+77z43Z84cGYQy+NFBqj9gCB8fzvMHOUlFEFG//+1vfxOCyiwX/maUzcx6Y4S6kCcvCdpC1EtIKC9sL8mI2p5MsOQFRBmY7qAJnDx5suvQoYMMAsBChVljBorgotpc0qPJ9d4b0l9++eXSHrWJZ8aelyMzul9++aVkyTV6b//0pz/Jy0332aabIdZ6ebeQfjShYX/z5s3zJo/0OyqWaFUZKPMH2eHeMQjRY2zpo5lKW+gbUdukz55Xg6jXHnXUUTL40w+S9XVFJngLSdO+xWwvA2fdZ9u1a9fEhTrARcu4yy67iMUF/Rr/T/qpPhdg3qlTJ/F9wV+WyQk0akoY1eyfsunb//nPf+Ra8sDXi+fummuucZiPZSpRnzPy5d3A+/L4448XywTqDQlkYuv0009PFK1khrSPP/64Q+s3aNAgabPf/PvWW2+Vdzjt1j7Ie85vSaIDNq7/+uuvxaSLPCmD74BXon6nvNek+p3JO9ibTyqCiLZIv3uPPPKIYBNGEPF14l5fdtll4oIBnmjiiLjolSjvf9Jn2h60upSPhUY6oa+jWQ+TTPpb1OeHsqJ+T6OMN8gvkz4c1lb/cfyUwZHJIoTvJpMme++9d06DA2b6zY/6/ETFPGpfV4LIM52tcd4xxxwjYyrGWV5B0cC4ijFfkDDGYqx7yCGHBJ1OOobGj/vK5FlTBY0neWFNYWIINBWBJhNErYDOqOhgRI97t7w46LyQKK8w+CHyUyrh448TLrMtYRL2QdUXBwMgr8pfBx2NiTjKYJnZJX1Za50YbNFGL0HUc95tqvZEwdKbl/5GW0vZvHhVMIHgGB//VIKpB4NV/D+8wnGuv+6667yH5Xe2/BHeeustKYNygv78M90NKpLiQDosIbUMsPjDwZvyzzzzzMQxjvs1rSmKk1NtpW+ka4eeT0UQL7jgAsEMHMPE+no9MkygaX/jfUWkRt1n651x1gEu1hZe83HM4+mnmN4jkCP2/aa+mEihpUNDh0AQ0UIwyGZCi2s0SiSDFCbLlGTJBRn8S/eckRXvIMr3uxRcdNFFUjbEBWFwTd2effbZpBqAAz47XoFY8t7y4uM9r7+J9IlJvN8MT8tCk6rSlO+U5uHdRn0He6/hd9j3zJ+O+w5eQQQRosx36t///rf/sqT9TN7/mbaH+079wgbTWhEmJEkXRZMdpb9FfX60fP826HvqTZPq/mi/itKHvXmm+80zzP3EPQctOZY+jZ3gTldW0Pko3/ymPD/pME/V13MxztN364MPPpiAg/cX7xzMplMJE3QQ+HTCJDX9niBO2RCCngUpSrKRt+XRvhBodoLI7I5f8IcIml3ERwTzKh4cNJQMEDBRDZOwF7a+OK644oqkS3W2ZeDAgUnHo+wQFICHGs2dV1IRxKjtifLxo0zMq8CN+oOPzhR7ySmz9GglqCsvDQaXQQ7ROsOOiQ/Xe/+YKWPGzC9RPhb+a4L28VljwBr215T14aJiSb2yZWLaVvpG0L0IOpaKIKpJsp8gWl8PQjL5WFQTU/+EGpNcb7zxRsI0Ev8Vnu+g0OpoxyBGCINLykTIg2vUROrhhx+WfT+BksQR/kV5zpjcoy7edwu/zz77bClb/Zd1cM37wCtoTtGyeoXJJQbMaEZvueWWJFcFTYdJHuaVvB/9wvsbHF544YXEKQa4mXynEheG/Ij6DvZfHvY986dLNWjm20D7WGInlWTy/s+0PfQ7JirSCRpx6spESTqJ0t+UIKZ7frSsKN9TTcs21f3JpA9780z3G+0tUaSxwgIrnSRKd122zkf55mfy/GSKeaq+notxHu9DFBMsRaTCOp5g7x/76XndnnXWWTLxRb1SCRYN5Mf7KxvCM98Uy7hs1MHy2DAQaBUE0Q8lLw01o+IlzOwvsyL4j2STIDJrzIPZGIKopqR+U6Yggphpe6J8/CAhhFOGvBEOHHwwF6M9DLq8Qvm33XabmOByHhwxKcOJX0VfvHx40Gr4/7wmwHpNlI+Fpk21ZQCHWVjYn2oWUuURdi4KlnpttghiW+ob2vZU21QE8e9//7uYIGtfsr6eCsnkc40liMm51PthM9HiF55bHZwzUMf0CmkJgsi7nJl3/7tF95mwQ8IG12hC/QSR9AReOeyww4Qo8n7je8GsvAqTFxznHegXJjI45yUlYQNc/7WZ7Ed5B/vzS0VAvGn13R2kQdTJA/93yns9vzWPqO//TNqDmR7mdukEs1fuxRNPPJEuqQSvI20qi6UwghiUeSbfU70+1f3JtA9rnlG2usQGLh6NndCJUk5Qmijf/KjPT2Mw134a1NfDCGJTxnlgoM8QWluECXdMoZkoSSX4hdJH001wq5UWk/PZEL7FxJy48sork7JbX1npar9Z7taXtP61q5MqbjsthkDWCeK7774b2pioLw4NyOAnOvjZtBaCqKYQOqjRRhMhlJeCt+6ZtkdJTRiWDO7w2znwwAOT1okKIqdaL7aYjzHrpYF6br755sRpvdY/25pIEPCDj4XXZyogSaRDzWFiGoalt4LZIohtsW94cfD/DiOI6lPm9eGyvu5HL3w/WwTxjjvukHfO8uXLGxTGOxMfJaQ5CGKq54y6QBTSzag3dnDNQAufRQZHmN6pKSv+QGgQ0VT6Rd97LDStEvU7pekz2aZ6B/vzSUVAvGlTDZr1XZROg6g4ZPL+pw5R2qMDZUhlKtGBcpA7g/+6dN9I0kcliI39nqa6P43tw/52+vcJVsO3n4B9jDMopzklyjc/yvPTWMxT9fVcEUQmmLBSwFoGbR/aOUh6OiF+A0H+0glRaLmXjz32WLqkkc/fcMMNEuCr6O03XNFFp7r8Izq6vI47x/467+4qXmn6GsaRK2MJ2ywCWSOIzOTRyRmshEmUFwfXYruNlssrPKS8kFsLQdTwxN6Qwrw8mA33E8RM25MOS7VZxyzMK3feeWeDsr3n9TeaBvD12qkT8IDZ/VT46vW6Peigg0SD6Q92o+ejbvE9YWAZ9pdu9jtVOemw9F6bCUEksisfy6Dw1G2tb+AbRLAZTAAZVPkliCByjUaPxNxRpb339XRYKk5ss0UQCUjAO+f+++/3Zi+BWhjMYC2A5JIgRnnOgkzgkyoc32nq4BocwMOrZcDCAuLot0YgYA9WGJATlajfKU3fmG3QO9ifTyoC4k2batA8e/ZsweLEE0/0XtLgd2Pe/95MUrUH7Sz3w6vV9V6rv9HIQOx5h6STKP0tKkFs7Pc01f1pah8Oaj+kCs0V5WJ+reb9YTEUMEeFSNL3GxM8LqgOUb75UZ6fxmKeqq/niiCCA+v9MrnF+Bay6HepCMIKlyiiJKeTl/o/5wbuvL1beeLf3fraWle7YpmrK04OfhM03qj5Yo6r+uBdt766PqqtlsVYdPctf+LWdtrVldx2lSvr+5gr6/eEK7ntaiGJRefWB4XUa2xrCPgRyBpB5OXFA0QkSNZE5AWOCQGRSFWivDhIy9ITOuBBQ8esJnnzIfcTGJyI8VPkD/MYghjovkbezMWLg48ZwXWoFw8vdun41zDQpu5eDWIm7aH96bAkopaSOTSCRKPjBQY+/rKZnWLARGRSfD0hQYQbJ13Pnj311shWgwkQRQ4zHwYXmG8ROCNovR8lDmgkudf4Q3p9eZIyb6GddFh6q5UJQeTlD4b8+X2l2krf0LZrsCba4jdLIY3e57vvvtsRmhyNAIMU0jMp4ZX23tfTYenFKlsEkf52wAEHyDuBe8Qzy3OI7wzvY10/MJcEMcpzRnAcfPt4T59//vniF4ffIe9w71I6mQyuCR4xePBgx6QMJmC8q+ibkEFvUBT1sdt///0d2sJ33nknMcD2awOifqe89zLV70zewVG+Z5TF+1i/cwT54Vkk8jfH/FGXCeTDeb4R+KxhsdG7d295lr31jvr+z6Q95L9s2TIpP+jd4i2f3/htoe31L2flTxelv0UliJl8T6Pen0z6sL9tQfs840T/ZcJHtfRoZHErgVQHTewRqZz7zl+qifug8sKO6bcg1Tc/yvOTCeZR+3ouxnmKA2MscER7G2UCIxOt4OCDO7p1+//JVXw4xdUV5ru8Azs00PD5xxulD90pRK/g5MNdzfz6JeK0vmyvPPJQN/tvuyQOra+ocIXn/0eue++aB92jb1a4eSvTr51Zu3ypK7rw1EAimsg8/qNi1BBn5NOPStvdzxpBBAIIAtFI9aXETMtLL9WrsqO8OMgHXzQWGtV8GOiQDyZCfoKYal2kBx54QO5Mrl4caLa0vfgD8lFQh2MvQcykPdqV0mHJTBofBjAiXD4zxGpr7y2bwQDaQsWSLYMnPhh+nyX2GWTocg96DVpR/EP8gq07s5iUr2khza1N0mGp9c2EIKo2BDLNx9svbaFvaJ2JgsdEB5MOYOAXHRRwj+lL9HkiuAVpBNp7X0+HpRfbbBFE8oQEsgYWAxjuE9FC8dljLS+VXBJEyojynBGIhFl1+pq+M/hN5GCVTAbXhJFn0Kx58Zv8mdzyC8SQAB+alu8KE3t+ifqd8l8Xtp/JOzjK94xydFkpbYt3y733Cj7evNd//etfJ9qOX6pf4xz1/Z9Je7Qe3BO+OxC7VDJp0iSpo5+0B12Trr9FJYjkHfV7GvX+ZNKHg9rmP8ZYhnvMusteYdKXZahYizbIkocoxVwXhZx78w37HeWbH/X5iYp51L7uHedVvf+uq/16iTRDx0WNiTWhOPBs6PPjXx5G03i3RERnwt675Jj3vP5eOuczt3y/ndy8HrHgYbVLFwmBKx+U/F7yjjcqxr4iaUrvv1WzCdxiVr7VJhuLWxFaxuIrz5Xr8rrvLdtHzn3WjZ9THXgtmsz1NbFz5S/0lfR1BXmJtJyrnj3TlT//lCt98iFX9tRDruyZR1z+YftL2vWlJfVp6+pcyc2Xu+oZHySO2Y+2gUBWCaI2GTMeTBo0cIUez3SLiYTX9CfT65srPXWM4izemPakwpIyMXXwLpwb1GYIDGHgWfuR9EGExn8dA06Wa/Brx/zp2McPhRnBdLO+Qdc257FUWDamHmhE0t331t43tN0MLrJ5/9pzX882lnqPomz1neCf/IlybbbSRHnOqB+aJQZQ6d5f6eqFJoX3GuTcqzUMuw6SEhTxNSx9No4n3sFffRX5HZyNcv15cG9oO+/sVJLu/Z9oT8RvCkSGCWN/JPGgOhDUhsE179coEqW/RclHn52m9scoZTVXGjTsEETvRH02ys7WN78xmCtxSdWOgpMOccVXn58qSUbn+J4xwQERTzd+YjKdiSqsOdLJCz06xUjbwNgSF3XrfpD9ogtOaXCpjjcK/vl3SVP9af3SPA0SOyf1ZDL35H/+0xVfe0Ei36JvVrtFB3VxP3TazU2auDjp0tpvV7jCs/8paYtvvFTOQUTxX6xZElsvvHLCa8k+jerb6NkqOSeDqhkfyPWlD9yeVJbttH4EkgjiqJlV7tHxFW70xw1tmlt/U6yGhoAhYAgYAoZA60WAYBG1y5IHZc1RWwaeXg1Ac5TpL+Oyyy4T81HvOr3+NOwTbAkNJ1ZE2SZrNQsXtIsojlgyQQrReBK5mAmZujXfuaL//dthathWBc1U/pGdXMldDddl9rap4MQeDvPLxggEtHbF165m6SK3vrxMJoHRikL6cNVJJRDJDh06yLIY5eXlqZK6IY/2cqv+tpNb3XUPVxePLLq+rs7ldd7d5XXeza0vaxjUqebLL4Rs5XXaxRX850hXeMbxrvj6i1zFsIGBJqBP9u7tBuz6a7mm8NRjxLexpGK9u+XiUW5dx53dZ/f3Sapj0QX/lbT5Rx4g2/Lnervimy6T39UfTZV3F2UXnnOSyzs4poksvOhUV/XueFc5aYKkg0xWz/k0kW/Z4z3lOBpMk7aFQBJBbFtVz35tWaA3yh+RxEwMAUPAENgQEYjyDiRNa30P+gM8eO9R7bffuMp33nQMxJoijcGIAR+Dp8q3xjWl6EZdW3jaca7guG4y4I2SwfqaGkcQjFQiaeY2NOUNuwayh/8WrhEsL+IXyEtpr5hPM1FVMT3GVC9bsr6q0uUfsq9oNLKVZ1PywVQ3Sj/yLtIetTwiF0MO8QmHtCD0ffpf+ZDno2YTKR3LJtSuiqbtjZRhikQ1i7+SNhSceHCKVE40XPnHdk1KU/f9alf5/jtu0rE93OSju7u3TjjUXX3VVUn3YMBpJ7tVELS4Nuz7Tru4S/aMuU2l0wiiDcXkHZccrCNSCT7QQ+LErXxAMkkrOLarlM9khl+IPkrd8o/v7kp73uKK8Ck8sIMcK7rsrKTkkEm0qNKWg/dyRVec4wrP+ofLO7w+oun3B+2dIJYsgUFaiCckueyJ+2P5XnK6bKumvOOqZ05z+Ud1lndYLN+9He8BpK6wIFZWx50dJr4qEFPSFp5+vB6ybRtBwAii50axZEOUP/wkTAwBQ8AQ2BARiPIOJE1LvAcx7yJAQ9XU4HdwzaIvXf7f/xp4W8qHDnB5B/3FYaJVfMPFrmbB3AbpqufOdgX/PkJ8aSon1kfn9Sc86MAD3fEHdnaHHdgl5TfDi1Htd9/KQIl65FrQEnlN8UofvkvKJohEFJGBeOfdHCZnYVL59uuSJ35TUQXza4Jc+U2Bqa8MTk/skchq+PDhaReCZ1Ba+ui9EqExcWHIj5q5s6WMmoXzQ1I072F84qI8a/gWZypKCv3X5R/eUcjT+oqG2i2erbq8daI9456icUwndT+sEY1e3gG7yhp76dJHOU8d6opipNafXu8hWsRUktflz1Iv0jBhpJoxITVx8re0467u4AMPTLoHX3fcxf3QcWf3Xced3bddYsRrfte/pFxv01sP/PWjTJwtHztK+iIWBX5NYeFpx8q5oHdcyd03yDmvjyIEvfTBO2TyQ+tSOf5VSedtb+L3gR3c2o67uiH/vinJxLTyjdFyjZqCyjPZeTd5H3It70PeKUJS46QUTaKKlyBWvj5KAu5AfrXc/GMO1KS2bSMIGEFsIzfKqmkIGAKGQHtHAFLHgKPknhsDoaj+eLqcZyCpgsaw8KwT6wcqh3d0RZef7apnf6xJZEsgB2bYMYUioAIDVb9gckbY+Lxue8Ty67ybq3xthD+ZaCjr1q1N0thBTKh72dPJyxM1uDgLB9ASFV9xjtQNU7nqzz6RsjkWRWoWzJP0FWOGN0heGycOYER7MENrqpAneRXfcElGWRVf/T+Hpohw/+mEfkAZ2SaImO+yfEDJfTcnaU7S1ScX5ytefVnMDTVvNHvcH7RLsszBzZfH7qtvoqDsqV4uv8c+cg6M9C+dtlsDmNC/mio18+fKvZSyO+8u9YH41K5elcgaDRbn8w+NBXXhBAQwKShKeVksTZyQqA9c3oF/dhBZ+lj5S/0dGkW/oO3SCRztL4X/OdKfrMn7aMtpR/nApxvkVfDfo+UcJM8raOoKTugu57yRS3lPoUms/nyWmA8zyVX16QxX+uDtkrbwzBMd2j+eadLmX3q2HD/ptiVJQWpYCkPq5NFo5vXYxxX8o4cch0AiRXGNImmTCGJxkaTjeMFJhwrW/MZcVrdNtdzw4mG/c4+AEcTcY2wlGAKGgCFgCMQRYBaaQAcM9sI0BWFgcR2DjfLBzwYmIW/8Zwr/e3QimETJndeKv0zR5We5uvyGpK9q8kRXdOmZLv/Qv8nAUwYzaBpPPtwx2PJK5RtjXF73vVzpA7e5smcflzQMVr2BKyBn5CX5QHquv0gGsdWfzJBjDHpzLWj+pPxue0p0wfWVlbGBWrc9k+pKPai7t/4cY7Dpx5kBqgwOO+0iEQw14iKkuqlCKH3KI5x+VGGwyzVlvWPRytNdh/aU9P57mu66oPNo1yqGv+BK77vZlfV/0hWdd7LkLfXpk/sJgKA6cazgX4e5/KO7uMrJE13JHdfWD87jpK/smUdjOP/v34ksIErUG1+z0sd7uooRg13VpAlCRkruvSmRLuhHye3XxDCdPTPodEbH0PxTD9oA8YCYMRHkNbWseu8tSaNWAvRbWfKhawepO5M8OtmAHyICKdEJovIXn0tZJyagKA9zS55ryA0EM5uiZrK0FYLtl7zuf5E2Yu3gJd4J/8MjOrnqeZ+J3x/vmtKH73bF11wg7yIIsNzLeICe2pXLk55tNMfi49hxZ3fi7cuTCCKaeG+dIKFyP04+XLaV40Y61eCqmapXK8i7l/RyzYk9pF5MMGi+HGcyxaTtIGAEse3cK6upIWAIGAJtHgHVAshg4oBdJQR6kLYuqKGV8RDvlW/Wz64Tkt0rEDfyhuQgaBgYFKHlQYNWeOYJQtogJcyaozmpGDrAMZiqmjZZTLVEQ8h6ZD7tIH5cDIpVGFBTlmowIK74+jHbXvXhFCFnBHUoufVKVzX5bUmLBjLXomSIAa5qzCC8MkjzaU7QwkFiq2d9JINr6lY1/X1J69UOVoweJseqP/5Qqk9gDb2HfjO5TNvHoJy8Ck85KvBS/Ac1uAr3m3ta9eFkuQaChuaY+6mC9ha8ZSHxeMh9zPIooynh9iEkkk/XmPmhtL/TLjIIpk/St/CPY7BcRoCPq893mPcGmepiQggpI2okywVkQ7vCcgNSJwbqnXZxRKKE4ECWeAbok3L+wD+76i/nyX7VrJmiYWSSBG0T94AJFq7HBLLw3H+5kjuuiUW3POgvDq0S/mSQSPGBw/zQ8zzqPchkqxpryKGatjIBQb6QILY8Y/r8qwYRU0baIxqrg/eW563mq1ggF9X8ieaethzZKdGHUtUNAgfJJghLU/pKWBnl/Z+M3YO4+SYmpbyX9B2Sd0Bc44am9NhuDrNSoppCxvI67lJ/f+NkLHG/weHEg4Uw1639PrB4nfhZc8CfXY/7ipMIopqDYmKK1QF9gT6jJq+80/ARprziW65I1EODXzGBpHVh8kSF50CPp/Nr1mts2zoQMILYOu6D1cIQMAQMgQ0GAbQSEApMqDDZYjCpgxYG+JhlMuudf3hs3SxMuzTYQSoQKkbGgjRg/sWMNv6CDD7QIqioGarOzqs/DsSNAR8kD/IAKWLgibkXZooMOkWLcWAHOcYAnsE9g8WyPg/LTDzlQhBEy/LueFd8y5VSvpLUootOlZDyWhe2sv5Y590EDxlcXXWe93ROfqvPoZR3+dlShoa695qnVb41NjF400EcuCU0tQP6SLshhQy4ScNaZ/h0iaYmbmrLgLIpoves4PiDGmRDv9H7jPY2r0t9EBGpc9yEDZKBeH2kpL5HHuAgG5hRso8WqjECOVSNGflA7ohICyGBqKhg1im4Euwk7qtFcBwliWhx9F5I/eMYMknRVIGo5x/VJXZPj4AQlYsWjnLQXOMzKGX6yAX9H60dJoz09/xjYnnkH91ZgpagmeK+o9HDj9GfBxMtTRElTWyJIArR9pcBCVSTVuqCaIRM/OOYAILMqumzarYxKSUv8sZfr2beZzKBgzaV59q/XAT9nEAsXs1lU9rmv1ajgjIxwGQRpI6JHKwbRNPvuzcJHLr82eUdtn/snYmp7E2Xirk6k0EVI4dIHqkiJINr6eOxwDOf//0w16NnSRJBhLwlyorXgbUNC/4RW1YDjPU5ZCJMg8+oNQcm+3p9xeihiWZrYB3OEe3UpO0gkEQQy6vWu+Ly9Y6tSe4RwFHfb9oTVCrR38rKyoJO5fQYdWMh6ZZaU62mqtZVlcUiZKVraHlRVSQs0+XTkufBu7oiWntbsp7trWwCawwY0PTBW3vBTbRP8RDoOmDQLaZ4DB4YDGGeVzlhrCu65AwZWHgj34VhxTXkVfrIPS4P88nHewoB5VhiAB7XbJEWYUYck0+vQDqI0hckLOpM+HgZLB+2vwS1IX/142MwxwBVTNA67SKaFtU01q5YJmRY8+WZFmLVeXeHeRv5MDCEfEIyGaASsj9q8BjNN93WSxApE+KAWay0Y+Y0uZzBGloijjHLD8EqH9xP/CvRAHAc7QVaHX7L38F7u/KBzziIHOujseU4WuGmiA5OMQ+GwKEpwt8TgsO9SJQPfmccLxEcC8+IkQiiLqpWU8kBxAxtmpgPX3yakHT6Afk0VtvFPZN6dNtDIuH624sGk/IQ+qIsPVBRIb6JXMdgG0Grxz6TEaIZq60V4gVpzIZAgKSekNjrLpLf9EGw9AYSKXvyQUegIdrFswfeKgz6yQNc0ehJfpgvQma77SH4S1CTbnvKuXSmqJpv2BYtNmXwHKF1pR/QXyHNkG0lecU3xsgeBA7RyR80nUzOoGnTSQ/MlRN1R7vGZFK8vys+uvX6Wor28MAOWfGtDWov0Ucpt/i6C2NmwPHnj7SsNSh1Ougvrmr6B65m5QrRLOrkGRNunK94eVBS1kxQyTP5wG1Jx3WHe61tZfv9v45yS9bUynhf07AFT/yz+asY8aJotTF15RpM5HVygMm10sfui+XZaRchqnlxE2HSevHE/1HL5t2RStD4B/l0p7rGzuUOgSSCeO3QcplVuHF4wwhXuatC68r51ltvlfDaLKacC2FB+VNPPdVtvfXWsoAtobwvvvjiwKKGDRsm6+mwwDCL3W6zzTayeD2Jzz//fKkn1/N39NFHB+bRlIPXX3+923TTTR0Lv3rl1Vs/dhf/uL8rK6j0Hs7a75nDFrvbd3vZXbhxP3fBRv3cldu84H5YWtQg/8Lvytzzp05yV2w9UNJRp6EXT22Qri0cYCB5719Hucu2HOBWzFrbqqtcWelcUcPb0aDOVVXOraqPL9DgvPfA6tXORVmeK2q6bJY9ZMgQef569256MA7avO+++7p//KPpflte/FrTbx0Al9x1vWj2GAihodBBgmwZbGLyNmlCjJQwqBg3Mm0zyAfiRiRSBoII2XmAogAAIABJREFUzw6EwDsoYSAI0UOKrzpPBk+yA2G8/1YZKOu+d8uAWOsJCZWgNPHAHmhfIJaYD6rkn9BdTLHQLAWZCGIeR34MqLymVlqGbjFzRbuRLVFzSsmfvCsqnA4SIfCi/dOgJJ13d5hwImiZ0GhowAquZ9FxNK4SIOOgv4imF5IGvmq2yiCwKeIlNYqJ+HnGTXgh5XocE2JECAqTBfffmigaUz3SeQeZopE67Tgx8+Ucmm3RcIesHZfIzPdD/E/jGlRwwidMBZNWSAv+YF5h4M5gm3LLnn+qftkByPdtV0s9aQck3KsF9+bRmN+YiypebCFLmDDSf/U4UXXDROvMhAeiBE6u7bybtEnIwsGxwDbF1wePYcLy9x+nj5E3fZPJEiaSvKL9ueji0ySdaprpt6rV4nohlS8PkjRo7pVQSb277emKMJe9+waH9hDLBvoO57z1x8QUE1stw1uPbPxmIgFzV6kT5skP3ZmY3JZJG1/wFy0TLT3vTN59vGv4Y2IGH2s01OTHM+qXhN+gTvKwPXifRJn+9N59nh2pZ9x/UCdZ9Jh3i/ZW91WbT14anItzYcHFtEwmCynDpHUgYATRdx9uuukmGQzm5+f7zmRnt3v37m7zzTcXUjhu3Dg3ePBgN3ZsbKDjLYHw6JBCBpNPP/20pGGAum5dLMjCxx9/7EaMGCF/v/jFL9zhhzduUVhvmd7fzzzzjJTfv39/72H5PeammULISvOzTxC/nPSt5H3fvqPd5Ke/cHPGfu0m9Z7nStY1XNz34e7j3CWbPy+kcM645W764IWSvkGF28ABNIeQQwjxpyPrfWlaU9WLi527807nttrKuc02C6/Zt986d+ihzm2+uXMbbeTcr37l3LPBMUXck086t+OOsXSbburcIYc4911AdPWo6XJRNi295JJLIi2UHI5K/Rm08kz6sJj1hiho2jBL8kfAxIwQ0sVAmsECpp2133wdM2EiTH6KJRXASYOiEJSEWXhvpFK0eWhsECGM8QE4+5AzTBN1MWo0i0EaO/HPwoTRQzoIesHgC20P+XqlrjBf0kKEg0QHtZRN0As1ccw7ZD/RbskgdeDToiVheY5sCsEgZCHtM09I+GISPZW2QYLFzC0e0AKSB2FS/zU0m0WXx4gW/miqWaKdXC9mnqIlGBszP8yC6VhCOwuBGzZQJhSEfMbrqOSW8iGrCBMC7Ht9OrVvQdj8oqSJiQglkv4+6r/Guw8ZgVAkBvcH7CqmuKRRouHtk0wkSMRbBu4EPMKET6PfegfrnMf3LsRvzFuHKL/RwjUww+22h4MQJjRUDPjLy4XkojViIgMNJz5mEvQlTjjKhw905S8PEv83tLqc1/aztqZGyswkuFBQG2L+dTGCyHn/ZIver6KLYgQRAqeCPyqmp2pCS7RW+gVaRzWLJvCLauH0OrY6aYPG3StMkvCXK+GZQlMOtpRN+8S0Na6Fg6D7Rd5jEvn0GSckVjXrnXeTtqpVg/863pex5+RqeeeqjyOEDnN7FXyw8YXFr5oJIp6vijExv2Pqhohv7RP3JwhpXtcOMvlAH6r9frX4rKLJ9YpXi0vgpDBBe0g9mTAxaR0IGEH03YdcEsSlS5cK6UKDmE7OOOMMSbtoUXrfjt/+9rdZJYhoOVnMOKyeuSSIA854T0jSmkXB6yApbmgUIVNoEDcUQXP46StLGwxEW7p9aAwffdS5bbd1DhK3xRbhBPGHH2KEb4cdnBs40Ll33nHuhBNiBPAZn3XJQw/Fjp99tnMTJzrXt69z22zj3O9/n6xNjJouF2Ur9iyCvNdee7kdd9zRlZaW6uFGbbn+5z//ubvhhhsadf2GcBGh64UIxAeiDOrSiZqOMZDwC/6FuuA0gy8GGgwYEY18yGBGyN4RHZMCmkD8RNMQNz9jgAMxlGvjZoUMNv2ig231dfSeZyCF3xZkigAqiC48jQagJUTMciFg42KBPYQwxgkxeMkgMm6WqySgYuwriarKcg53XJswP2NgLma2HtKWSJzhD43wChHXpUrAiToRLAPCJ3hyX3veIhFENZCON9S+msN6oz9qVSA05Jcg7pCIiMt+aB66hdBTP0yX64gMiTa20y4J7TJYaQAXAsToQFx96yDomBdzD/BrxfcsW8LEgOB2+nHSz9FiKjnClJRzmFEmCL+PrHI+8dd5dyGM3GuvaFApTAalrFOP8Z7O+Df+yJLPaceJFpHymFRBoyYTPgQF6rSLaABJl4qQosUljQRSiT/TVVPfS6qTRjNW/zpvf9GJn1RlJGWW4Q5tQpNJHVP9MWmjwvtS01ZOmhCbpOHePHyXEH+J6It2MYDUqr+rRmPl3YTfJT6aBcd2lYBPtDlomZO8rrHlfPwm5NwTLB7oW17xE3vO0fd491L/MLN+3qXiA37Y/okJKW++9rtlEGh3BBF/omnTpjnMxtDAzZgxw1VX15sMKUHExJS/UaNGuddff11+e28RDwjX+rUA5M/xHxit+mTixIlC+l588UXfmYa7Xbp0cTvttFPDEwFHohDEyspKqdfIkSPd/PmpFwqGnP70pz9136KOCRAliJiYovlaPHW1+/z15Q1MTvO+KXFLZ6xxdbV1Sbms/rLArZjdcIaXRA92ec3dstOwpPRBO/MnrhSCOOPFhUGnk47lryxxs8csc1+9t6pBHUlIG6inakQLV5e5WaOWuq8/aXgPkzKOsAPRRbtJPee+ucJRF6/gZ0nZ3r+Kkvr+6E37zWdrk9J5r6muTH5Rc126dnvzTvUbZfpPf+rcSSc5t3Chc/vtF04Q77knRvrmzKnPsabGub33jhFHfiMlJc79/OfO+S0tR46MXf9cPBp51HTkme2yYzWt/z9lyhR5fm+/vekmMFdeeaX75S9/6Sqi2NXWV2GD+MWAWAbUDJC6dpA1yaI0jEGNDDKefFAGEWgcVbtFpD318WJgRboiTMye6x2bFX/sXld882UOX6nykS/J7Lj4XVVXJXzzuAYy4jX11EF0kPmrENGuHZxf+6cDu+qPpiYiiNI+JWhBZmBR2t/UNLQd7UHh6ceJrxYDN6K2EmGVdnp9QJWMqU+dt2zazT1Eu6j+Sf4BuDd9lN+JteoIxBEnrcVx017M/9D0cX/0D1NDoieK6Sna56+XSDGqaYSUiTbk/lsl4AnX46fG9V6TRHzdMhFZNqCiQkxydS276jmfJurFuACyoZpMBuGqzWTwTGRM6kCwpVxJ+SsvurLB/RKkkMAlupagkmPID/cQrTGTI2iNCbRDIBKINz6hmBdqZE1vXXk+JMjU1efHoqNCOOM+gd50mfyWpR8C/JbxVwZTMWm+58ZERE01Hw8qA59eMC5mvdJ4n/ESQAgS5F6Xjcg/vps8w0xMYPKuBJN7RzvTWTYE1SHsmEYBlX540qGiYeaeMImFmTWmvOrX6zUBxheTa9C0ija1+16SXstRn2OeZb8Ux02cvb63YKATZ2j/Ys/S7mJdQH+RYGBx/9LSR5LNpv35R9lXTTrvGb/w/kT7Sfv8vpX+tLbfvAi0K4I4evRoGZRhuomZJ1v+vIRJCSLEEJ8/TbP99tu7r776KnF3IFycu+KKKxLH+PHRRx/J8YGoT3wC0eQa8k4n++23n9tzzz3TJZPz6Qji1KlTHfXHrO1nP/uZ1OHII490a9c2JGmQYnwa//e//4WWrQRxyfQ17rrtXxSihjbv6l8OSiJVmk6Jl2bY+8g33U071ke50uNs79tvtLtrz/pZa+85728IKWVC5MIE0jT4vCmS7vKtBroLftRP/BU/GpKslV39VYGkwbRz9I0fJdpD/i9d2Lg1kCpLq91Tx06QvC7c5Dnx2SS/ficn++rkrShJKo804BokEGfOB/2tXVY/wxu13UFlhB1bubL+TCqCiPava9f6tPzCX7FHjxjxmzIldu6FF2L77/qUQePGxY4fHLfuiZqOXLNddnIrYnsdO3Z0v/vd75qs5V24cKGYrAa9J4LK3ZCO4QMmQSge7ylBJDJpm5iPxWej0ZTouloMeJUgss6dDAy9SxF020MCLKAxIEQ/BAMfsJovYmt9kR4thpqhap3U9K16/ucyeCEoDj4ykD3ICIujl/V/SsgIJqT4O2KiFSRoNHWQF3Q+m8cY/OIjVnzL5aKlQmMlazMScKLTLg5Cx6CcdOAFmcXHjkiPEggornmBVIETbcK/C5zzIHFxLU5CwzDroyZVX7WBct/QNPR9rN5/LE4cxIx4zHCZUFASr2axEoQFTVlJcdKahHJfr7swpuXzEEzxecOv8eC9I9c7EQDHm89x3VzN8mWx/oZ53ISxCXwoGzJKUCLM+ehLEiWXiYhTjpLgMPj4MaGABoVgMZg86tIUkSsWkhByL0sUdO0gfnck06VKCECCQPbpy5BphOcJDRfBisoGPO3QVhH1Vo7hQ3nluYnIrHKv9N4clhwESjLL8B9YYILIBAqkRgMPebOR5TfQIvt8Pb1pNEqoBFhiwXb66rn/kv4N4YPYs56nmijzPEhbPPfVv58uuIq3/FS/0eZRdpg5MaSNPkP/9Iq+OyRg00F/ccXXXiiTTxWvvuyKrjwvUX9/RFbyUHJJm9AYitY0/gzzHtbJA8ixmJmyLEXcP9k/YUZfICpppgIB5X3JO8cr7Cs5lGfY49/tTWe/WwaBdkMQCwsL3ZZbbimmmN9/H1sjBs3h3LlzHVo/FSWIEMgHH3zQFRcXOzR+ELvTTz9dk7moBBFtJWZp/G233XaSz7bbbps4xnGIK3LSSScljm+22WZC1PRatl39I+94bVIRRLSAW2yxhTv++ONFC8psHESV9nnbow3TtqLtDBMlfpAuAtbgH7jw/e8kmAwET0XTpSOIfU+aKIQR0njxZv2FTPFb/3p1jfloLp62OnHsmu0GC1G6ZtvBiWOknzU65lRPHYZfPs1dusXzorljvzSvQkjbRZs+59BiqihBhIDdtstwt+TD1aJ9e+KIN6WMb+fladLIW3CBkOJDWVcX810q+r7cfbcg2bcV7WrBqlL5m/jI51JeGEEkHYRS//DXvOQn/R2+mF4tbdR2o+meMGFC6F+YdiuMIGIl9aMfOXeVZ5k34hv96U8x01T8EQcPjkF4880xIlgYtyRGiX/ttbHrMWP94x8zS5eLsoNudq9eveQZZtKlqXLEEUeIj3FT82lv1+MLpiZu2naIjc6QowVhMMS6fqRV8zpN698y8PcGVfCel6A4Jx0q5oBinnro32JE88SDxWeIARRkU0wGn7hfolj6za40PwgW9YK45VrUD0sGuj32cQXHHSTaQ9XOyfGQATGDyPy4eacOFBPpCVAikWiHubqy0sTAtKlLAkDMZaB4wK4JLYKsjRgPToP5W3XAYuyisZs7O2ngKdqRyW9LRExdA1LNBrUd6nvmvRfcN8w+Ifn4gEGKvf2CQD8SpCOuWcFkFa2PVzNYfHP9+nBalkTYvOws0dKpBk/PBW7RDsWX7GhsP6FOkFHxna2oDzpIFFNMorWPqraMZ0aIVdznM7Be2l8w9bzgFNHqKuGkXzWHqNkuETTDhKBY1B+yKZFnPQGOOA5pRjtXfFUscmrKtsbb7PVzDSs36nHFPig9JI36QBS9ou+0VHVVE3vvdfzmGeGey6TIgR3kOSMYDH6ndUX1rjwsU8GzL2X02EdM9unz+NUy+cayFkzSMJmTDaGPy6QVGB+8tywZk418LY/sIdBuCOKSJUtkYHf55bHIdmEQKkHs06dPUpI//OEP4oOkB6MSxBUrVrhHHnlE/s4991ypw5lnnpk4xjnVYA4fPjxxHNKH1k+vZRsUMIb6pCKItPcnP/lJIriN1v+iiy4SDQYE2CukhwyzBEeYKPF7+2GPHaFzbtT1M4TgqP+gpktHED8evthBjvi74bdDRCup+2yn9o8FcIAY6fFB58Y0gwPPfC9xjHOr5scIGGaiRDUdee2MpGZwHA3cyOvqjytBvPkPQ13RmvpQ3zOHLpa0RFXNVAacPknK9+aXLo+pz3+ZkiB6r68qr3F37zXSXb/DS442qWTS7gMOOEDuNfc76I++GyRhBHHp0hjpe/DB2FWPPx4zRf3rX52bPDl2rmdcuXLGGbEgNqRctsy5jh1jJBJ/w3PPde4nP+HD5lzUdLkoO6jtTPiAFUGcmioEqSKv6dNjPmpNzc+ujyGAbyMmkkHmcZlihG8cg/psiJqYMsjKtWAyyECPgSACYaieN8cRAZTj5YOeFSLGchZoBDAvxMyTQEAMKFlqgHRojiAVmHDiR+cl2/iIyWCSgCd5seBpTWkXpph+cz58n8pf6p/QcjQ2f9oEmae+aKfQlPHbaxopET41iEy3PSWwDOZ+mFp6BQzUR1WPiybnkjNcbf46hyaMJSWCRAbrzz9VH+RDSRfBPs7/j/jlegftQXlEOcayJnJvWF8vIGCP5oHPmuBweEchBxABtIZo33SJjIJ/HiLaH0gGAYIg2yoa2RVT8eYQCaDDPbzy3NDiNIqyLhuCCSPkib5PICqeBQQSVnTFOdLX0dChNaW/E0yLCQuZiFr3Q2yiybMESGjBTTxB36C/qSWEPzueU/Gj7R5bdkLur5iddpF6+9OH7VfVrHcvTq2Sv8VrYq4pYr4fX7MTjTdY0AfUgoKyCJKj2sawvKMeh6BLFFz6f+fdxNQ96rWWrvkQaDcEkYcPDRwDMiJ+Dh06NDDYhBJEfxRTZvvR4qlEJYianm1LmJhiEvf73/9ezFoxbdW/s88+W7CYNWuWt4qixcQMNZWEET+IFOQL/0AkLF1zmJjOG79C6gJpxQzV+0e00D7HTUg0UQniWw99ljjGD3ws576xwq1bnkyikxKF7Hzx1jeyTAcazldvmelWfp5+AJUJQXzhrMkOTeiiD5JDfmbSbiYH6Odhf3XxD6m/iWEEccmSGAnERe+442K/L7vMObR7BOxEg6gEEWU8AWnwOcQXkUdLedJZZ8WIJQQxarpclO1vN/v4HPMOueWWW4JOZ3QMfP/4xz+60047LaPrLHFqBAiE4A1ckjp1853FH1IG40d2ynmhQna67iEL2qNtUTMuHVT6g074K6QmbWgZwgSzVMmva4cmm1yHlZHN42gWxW81Ho2WyKJo2LxCsBgmFhgv5FKoC2alaDgh2qm0So2thxJiloDheYAwYUaNBphgPrLeX1xjCFHSQDmQSzSpDNwxR0STi09ikOCbK30gA1PdoHyiHiMiJv6DqaJhat9F49WWhIkHMbOM0Pfwl2RSB/NwJbxR21pSsV6Ws+vRs8SNnxOLd6A+jPp+0C3BqrB88PpwRi0nVToxlRVyuLusvZgqrZ1rOQTaDUEEYqIH3nbbbW6HHXaQQR6BWK655pqkQBFhBBGfvbZIENEu4lOINjLoDzNDrxxzzDHie+k95v8dRvww74Qg4h+IhKVrDoKoZOuqXwwSjSS+kt6/Z/9Vb0IbRhD97c50H/L2+GFvJNZzZJ1DzGTDROscZmKq173fb4HgjMbUL5pHlHZDDDG3DvvLlCBCBCGB/EH+4pbTUkW6mdfE9Kab6tMS5dS7qswRR9SbmEZNl4uy/diyv3r1anl3sEZoNgSTVczJ16wJ9jvNRhntLQ9M5dSnqjW1XfwCD9nXEXa+OUSjvupgjyUWMD3FbAyCkkp0jUHWd/T7Del1al6IL2hbFO4HS0JsqIJWl/ungYS0H2DWB0nkGZF72OXPoi2CIKJ5xi8tkfboLrE1Gh/vGUhi8XMkLebXfq1qS+GKKTIazVxFIW2pdmWr3CCCyBqv4meMT/WTDzqWNMGaIBciEYnxhzyhe1KgnVyUZXk2DYEkgth/cqW7cXi5GzgleyGXm1a93FzNwHfy5MnuuOOOk8HezThExSVTgnjppZfqpbKFcKFhCAo+0RIaxE6dOgnhQ+MZRS688EKpf6pQ/mHEb+KjMR+6NQtj/n2armRt8hqG+PbhLxgk2QpSM2/CN0Ki/FrBoDJzRRC1LMw+33l8rgTxwW8zaE1H0iq5S0UQiayK3yF+m0GSSbuzbWJKff7f/3Nuu+1iGkNv/Z56KkYIdT6CJS0gjH6eRdRs8oAkIlHTkTbbZcdqkPz/008/lefjscceSz7RyD3WNcVH+N57721kDnaZHwFZX/Hb+vW9/Ofb074GR0EjlMrU0I8JGi1dHB2TzCANl/rToYmqXZNsyeDPz/ZbDgG0oZBFguEEmV1jPlx09f8SUV7xwRRzy+nvJ4JAhdWeIDdoKvGzxBS5tYiYiMaXqmktdWot9QgiiM1dN+mLviUymrsOVl56BJIIYvrkG1aKmpoaWc7Bu8h8VIIIEltvvbXr3LlzAhSC3mCK2poIIgt8U58okVNpCL6OpMfXKkyU+Hl9C/kIQe5u/uMwV1sTs/F/57EYYZz/dn0ITAKr4BuYa4JIQBfKiRIRNdcEUXEcf/9sIa1hBDAdQSTIzk2/H+pu2/VlV14UW6dN89ZtJu3+5JNP3Lvvvhv6l2mQGupAgJqNN3buiy+0Rs5VVTm3117O7bFH/THiROFn+J//1B/j18svx4jjK/FAtlHTcW22y06uWWxv0KBB8nxMmhS8/ia+u0cddZTDtNsb9TgoLz2Gb/JvfvMbx/vIpOkI+CORNj3H9ptDKizRMEjQm067iN9W+0Wp7bec5Qfwkc2V1qjtI7ThtKA1EMQNB80NuyXthiAy83///fe7N998082cOVP8AVnKATLUUx2jnHOZEETI4Kabburuuusu9/bbb7sePXqIv19zEMSxY8c61jTk7xe/+IUE0NH9L7+MBXWh665cuVI0FKQ5//zzpZ74HXI9a0H6ZdmyZYIJ67SFiRJECA9rIEIAnz7hrUTUTr2OdfowOb2/4xj32WtfixYN/z+NTqrpvNtsaRDJk2ielM/aiu8+MVfWXsTsk6A3PyypD8KTC4I44cHP3PTBC93CKaskKiplEoCHpUC85I56sLwGf0Mu+kDq++Z9s2TfG5GV9vQ+arycH3PzTIe/p/ePNqhEbbemj7Lt3ds5lFz8/frXzm2ySf3+mDH1OSxf7txWW8VMRIcNw+82pg0kvdfklCuIF4UWESU8fIugNltu6dz++zvn5UpR0+Wi7PqWxX4de+yx8ryFkTklkLwDUj1D3nx5HknP82tiCLQlBAhY4l2vrS3V3epqCLRHBIwgtse73rg2txuC+NZbb4m2kIGY/rFQ9R133JE0c58JQVywYIHbbbfdEvnhv4fWgPxzbWLKUhnaDv/2gQeSfVzmzZvnunXrJr6Imha/RKKpBglpwYY1EYMEM8Ze3caKhg4CxtIUPfcfI2TIn57lHlgHkHSQoxkvLRLilmsNIvVAkwnZ0iUxqAN/+CJiqqmSC4L42CGvC2HWMlnyAsxWzE5ee/K9Pl9InRLpPOscYkrqFfIISscxCLBK1HZr+ijb3/0uRuYgdP6/U05JzmHePOeIXIomkWUvdt/dubGxlUqSEhIDB+tu/BXJc+utnfvvf53zrDoj6aOmI3G2y/ZWmEjIPDdXedfx8CZwzrG+Ieunkg6T8qiCJUL37t2jJrd0hoAhYAgYAoZAxggYQcwYsnZ7QbshiNxhzCAJBrF48WJHCP9sRSpbtWqV++GHesLRWnsTWg80hNS3Cru/EEHDuvHGG7srrrgiJEXsMPjhT6cmpWGJqytq3A9Li1xtdcz0NCxdLo9jnkkdvGaxuSyPvCtLq2XNQnwyvVrDXJfrzb8l2q3ls4JKXoQlJAnatnKlcyFBUzU7WfYiSjouyHbZ5InPMhMn/gjHiQrGf7Cu6tq1yRMB/jT+faIqM3nDZI6JIWAIGAKGgCGQCwSMIOYC1Q0zz3ZFEDfMW5ibVl122WVuk002cePHj89NAZarIdCGEHjuueeEwA0YMCAntWbChojD6dZpzUnhlqkhYAgYAoZAu0DACGK7uM1ZaaQRxKzAuOFlwoD10EMPdVtttZX7whtxZMNraqQWsa7hiKunp/3D99Bkw0LgnXfeEV/jG2+8ccNqmLXGEDAEDAFDoF0hYASxXd3uJjXWCGKT4NuwL8ZUjmUviMzY3mXawK9cr65j0/49f1pwdMv2jl9bbj++h9ddd13WTNLbMhZWd0PAEDAEDIG2i0Bt3Xo3Y3GN/P1Q1HJuP20XwfZT8ySCeMuIcndUrxJ3+8jy9oOAtdQQMAQMAUPAEDAEDAFDwBAwBAwBQ0AQSCKI1w4tdz16lrgbhxtBtP5hCBgChoAhYAgYAoaAIWAIGAKGQHtDwAhie7vj1l5DwBAwBAwBQ8AQMAQMAUOglSNQ/elHrnbl8lZeyw2zekYQN8z7aq0yBAwBQ6BdIFD3/WpXs/irdtFWa6QhYAgYAu0FgYoxw11ex51dXufdXfXH0zNu9vqSYle7YpmrK8zP+Fq7wDkjiNYLDAFDwBAwBNosAsXXX+SKzjs5cv3Xl5a4unWZrVMZOXNPwpqFC1zhWf9wNUsXeY5uOD/XV1a60ofudCW3XeVqFs7fcBpmLTEEDIFmQaCuIPVCyUUXnOIKz/2XkMSii09LW6fKt8a64usvdgUn9nB53faIkcuOO7vC/x6d9lpL0BABI4gNMWm2I0QHZbH5DUE+++wz98EHH7RYU1iIPgqWNVW1rqqspsXqma2Cq8rbfhuyhUVbzqe8vPn9vYlOnKv1HFviXhScfHjkAcD6ujpX8J8jXV73vVzttytyWt2yPg/LAKV22eKU5TBIWl9bmzJNazxZ8cpLiQGY4Jmmna2xDVon+kLB8Qe5itdG6CHbGgItjkDt6lWu5qsvWrweWoH1NdWOiaFMhWsKTzs2abKsZslCl9dpF1c+qG9SdpRRNeUdt76i3FV/9omrev9dec/kH7JvIt36stLEb/1R8erL9e8jtI5xzSPlVk2aoMlsmwECRhAzACsbSb/77jt36qmnuq233loW3v7xj3/sLr744kZnXV1d7SoqKhp9vV5YWVnpqMs111yjhyJvly7ujQBxAAAgAElEQVRd6rbffnt3wgknJF1TXVnrLv5xf/fKNZmbBiRlFLJT+F2Ze/7USe6KrQe6CzbqJ2UNvXhqYOqZwxa723d72V24cT9Je+U2L7gflrbN5TtmjVrqLtzkOffcKe8GtrUlD956663SjwoKCrJejdbQ17PZqFGjRrlNNtnEnXLKKdnMNm1eQ4YMkXdP796906aNkmDfffd1//jHP6IkzUma/L//1RUc1y1S3mjzZODQbQ9Xt+a7SNc0NlHJLVdIWXVrvw/NonLcSElTdNGpoWla64myZx5xYF90yenSBmbuGyt1635wJffe5IqvPl8GhI3Np7HXVc/4QNqQf0RHxwDVxBBoDQiUD+4nExfZrEt17Xr31txq+fs2L/UyF3U/rHFlj/d0hWee4PIO3lueEd6fWA1kIjVffiHXVk19L3FZ1dRJcqy05y2JY/yofGOMHGeLVH04JVbugR1kv+LlQS7vwA6OPL0iz3CXP0vaihGDXV1RYSSlgTcP+52MgBHEZDxyvte9e3e3+eabCykcN26cGzx4sBs7dmyjyp04caL72c9+5t57r/6ha1RGPJSVlTJovOKKKzLKori42O28885ujz32cIWFhUnXQhAhbi9f8WHS8WztPNx9nLtk8+cdpHDOuOVu+uCFbs7Yrxtk/+Wkb6Ue9+072k1++gtJM6n3PFeyrunEukFhzXDgjXtmSXvu3OOVZigtsyJuuukm6Uf5+dm1+W8NfT0zJNKnvueeewQrnp3mlksuucT96Ec/cm+++WaTi+7bt6/beOON3ddfN3z2mpx5mgywGmDAkt9jnwYpq957yxWedpwr+OffXVnfxxzaQyUCXMMMdTakbFBfV3jev13Zc71d5TtvOsycGKDkH9NV6labwsQUE1QGOyV3XOvKBz/rqmd9lI0qNUseDPbQutFeId1dOySRKwaA3IN0Upe/zuUfc2AsjwN2lS3XNqesr6hweQf9JVb2h1OapWj6Y+mj9zr8rEwMgSAEquITF0ygRBWsEdA8hklJxXpZrYAVC8bPCZ8M4d3KuxP/v7we+7i8+LMpz3rHnV3t8qVhRTQ4XjX9fXm2eFeolNx8uRwr6/2AHpJt8ZXnyvGKUUNkv/L1UbKf121P2Sc9dagYOiDpOnbQFnKucvyrjvcu7gQmjUfACGLjscv4SjRtG220kWgQM7444IKXX35Z8mtJgnjttde6TTfd1M2f39AHJZcEEe0f5BMNYjoZcMZ7knbNomQCm+661nq+uqJGyPAPS1qfBjRXBLE19PVs9wc0/0wQLVmyJNtZp82vtrbW7bXXXm7HHXd0paUNzXXSZuBJwPU///nP3Q033OA5mv2fDHoYVDMAEu3VYfu70j69YoMHCF9d/Ww4gWswX9LBjAwa3hgtpkt6rDYLGsSyJ+5PKkPz9m6rP58VCgZmUwzAKt98Vfz5mKUvvvaC0PS5PAFJwm+yeu5sp9hArKtnfxxabO1334oZmLa39ptYtMHKsa/EcOmxjyt7vo+rnPCaDOjKnnywwaw+GgRwqJ7zqQSiIC+0ic0tpQ/fLXXGNLgpEmT+FpQf/Zm25h95gKsrzL7FRVCZbeHY+uoqxwRBFJeR1tYeiFyqCaFM6ytmmB13zsjHt/yFvqLxCysrKkFEA0f/rCuOjTN476q5pxzPWydFrC8vcxWjh7lU/b7y7dclr4qRL8k1POvkwV/5gD6JqspETbc95TjRSxHOkw5rBaT04bsaXCcnnJNJOs3Xu80/bH9Xs2CuJrNtRASSCOIjb1a4c54rc4+Nb5ualVRt/uabb9ysWbEPNQOzqVOnutdff935TeE4N2PGDKcakNWrVztMwT755JPQ7FeuXOnGjBkjmjx/ft6L0IJAEF988UXv4cDfqfKkLtTx3nvvlfz69Okj+xzj7/vvg02a0PDhJ0hdZ86c6Wpq6v3YVIN45ZVXSn04P2zYMDd79uzA+nHwq6++EnJ41VXB5gYJgnhlTIO4bnmxmzV6mftmzrqklz8fgqUz1ri1XxcnlVVRUi3Hi39oONM/f+JKIX0zXlyYdE3QzoNdXnO37DQs6FTSMepLPT4dudStmh+sAft+caH7dm7Msbquts4t+XC1aC+bqo2krYunrXYfDVnkPhmxROpRW10/4KWikGLqp3/fLQiu4zefrU2k0bS6pY1+idJu/zVh+0oQeQ5SPWd6Pc/XhAkT5JlAm7VmzRo9JdvW0teTKpViB/++adOmOcw4R4wYIc8jprFeYaJIn1W2CxYs8J5O/F67dm1SOu81YaQy1XsjkXH8x5QpU+T9cfvtt/tPZbzPe+OXv/xlVszdgwrXgRJ+byV33yADBBkAHNEx8buupP79waCFwAQQnsqJb0iasqcfTiaIK5qu8Sz45yGx8jvv7opvu8qV3HW9K73vZkdZ+EZSx+rZM4OaJMdkdv6AXROaNzSQXJPObzE0w0acIMpfxfAXkkzIqEPJfbeIdhMynkpq5n8ew4CB7FdfuOr5c2PauINiAz25Tx13Fm2EDhC9+eETivYU0cEfA87mFupGXYtvuKTRRdM/MceLIutragSn4psui5K8yWl0YqX4xktd1bTJTc6vqRlUz5zm/Noj8kTrLM/Ad982tYhmv77o0jNF06bkpqkVIPgTWDBpE1W4v36zTe+1UQki1+Qf+reEVUPt10sTWsSCEw5O+E1XvjZC6sikkArPcdXkt3VXfHtpR/nAZxzv5oJ/HyHXcAzfQRV8DzmGVQVkEcG6gmMFJx0i+wQli+X1tF6W2DJJiGkq5v0ELkMDipVD4dn/zEjjmciwnf9IIogbMhYMXJnlJpgK/nIQNf4Y1HjJH6SH4yNHjnQ33nhjIh3HLrzwwiSIIFXnnXeepNlqq63EZAvfQgaHQQIhJR8IZ5hEyXPbbbdNqhd5ev/69euXlD150n78nbzpzjnnnEQ6JYiXXXaZO/HEE5PSXX311Yl03h/XX3+9tJlBaZAoQRx1/QwxA0Xjp38vnD3Z1dXFAvRoOr8p6rKPvpf00wY2DGH/+evL5Rz+eOnkvv1Gu7v2rH95BaVfPHW1u277F8VH8fKfDZC8ex/5pitZmzxZgt8feUHObv7D0ER7rtlusFv5eWxGLSj/VMcgzVf/cpDkhcmsYuQnqYPOmZw4R5onDg82D4QMax7+7dplyVrHqO1OVX/vOSWI06dPT/mccc2jjz6a6GeYO9I3Mb9+6qmnElm2lr6eqFCKH6NHj5b3ibZDnzW/dp3nTs+xPfzwwwNzZSLJm877+8wzz0y6Jsp7I+mC+E7Hjh3d7373u6QJm6B06Y4tXLhQ3gUDBw5Ml7RR5yvfGieDAjU/Kn/+KSGAeUd3iQ0omOkO0Agy+114ylGShtlvghXIAAQS9nXTNbflA5+O5ddl9wbtEr8dBnefzGhwTg+IiWnHnV3R5Wc7BlhS1257yiBK0+RyW3z52Qk8FBdMyYrO/4+reHmwnINge6X64w+TNBo18z5L5MHgVPPJP7abK+11pwwOUxFeNKYMEgljz7UMHtdXZR4Iw1vHxvxWU9ko0RKD8meiM/+ozq6s3xNBp+UYGlcmERjsltx+jSu+4hyX1+XPCS1N6IWNOFEzd7YrH/K8K+//pEyMVODvGvfRQrseZM4MiQwTWU5mwbysrUnHxABYQxjoU2gN0aRqsBEmhfwig/+7rpcJGLRQuRSw4H6h0YwqRRf812EKyfMcJCy7gPljuuideq0+W0Fa/NpVK0UzD0ErPOckp5pvon6WPt5Ts2iwzYQgQvRK7rnR1eWtc/knHRp7trvsLlsmwRD1GSx7qpfsY0XAc6znOagBrUqfedQVX3VeknUHZrQqSga9+BWceLDkB/mGABb8OzbxBlGEcGLaj7kpWkomq7LlOqB1as/bdkUQGYRCEgmksW7dOvf++++7bbbZxu23336JPqAEcaeddnK77LKL+/DDDx0E6IgjjpDB2rx58xJpL7/8crfFFlsk/Hjy8vLcscceK1q1L7/8UtKhTcCMi7/ttttO8mDQq8fYMrBUiZIn9VmxYoVDc8igcfjw4bLPMf7QYngFfyPSQf7QPOA3CCl+7bXXEsmUIG622WZu7733dnPnznXLli1z+EyC26JFDUO1g1GXLl0Sefh/KPGDcN2x+wjRtqENHHTuFCEwkDxE06UjiGjYbtpxqPxByCA/12w7OHGMc5AtpO9JExPHL96svwSw0WvZ9upabwuf/22pu3SL512f499yZQWVMlimbpC1Aacnm7BCEAlwc9UvBrkxN810pfmVUuYFP+rnnv3XRCk7k3/lhVXusi0HCNkr+j6mKUVziJYSraJXqFvBqlL5u36Hl0IJImnyVpQk/vDBvOQn/R0+m2g9VaK2G00WWr6wP+8zoQSRCZNUzxl1GD9+vEyWELgJLdvkyZNdhw4dZCJj1aqYD0Vr6euKWdgW7fyWW24pZE81+LSJ58j/PKJdpX387bDDDqEEEdNNfaZ1e9JJJ4m/39tv18/OUqco742guvfq1UveDVhUNFV4RxKwJheCiaKQh5MPl8EXA8m8zrs5IYxx3xg/CcFEqvDME+U60jGAV6JJXv70jak3mhjyYsbbL4VES4Ugxk2lOA/xUZMt9gmak/C/i5tcVYxIb2HiL6ux+xAV6sgAE00ngzsdEKPF41zNF3MS2aPxIZALmlkV9S8SHLrv5YouOyvW7pnTNEnKLQSBAXP+sV2FwLRU1EY1g8tkyRRvwwjoAQZoY4MEQoZGhjSYy7Et+MffZVv5Rv0YIOjaTI7Rr4viSwPIPYn3q/yjuwhxYEDNcTTxCERIgqGc0F2Oq/aJ56Xq3fESOEjrq/mpf1gm9fKmrVn0pZRVdM0FDvM/zRd8ygc9K/t+P1TWstN0umUiI1eTCYWnHy9kjwi9ZXFig48bgZRqQ6wPsGKQ6MWdd3doiL0CeSmgj/fYx5XcFjzp7k3Pb4gzbSWap1dYOF6CxhzYQbbcM9JB5MTnOk7WvNfo70wIIgG2uNeq8ePZR5uJWTTWDywXpISeYDYIE2LUBVKoIoFl6O/ax3rdmbiXapYOXviSy7XxZ0iD23AMCw0pN96fORb412kXmYApvuMaV/rkg67q4+lyLzbU5YYU41xs2xVBhCQ9/HCyfwFaMI4rAVKC+Ic//CHJ3G3o0KGSDrNLBNM4on7ig+cVjpPfddddJ4cZ2D3yyCPyd+6558o5Zv/1GFvVMPx/9s4CXI7q/P+F0lKhFKj8aUt/lJYQSNAiIUoCwYIXDw7BIRASCBqCu1MsQCGQBHfX4k6CuxUPXHfL+T+fd/ede3buzO7s3t2r7/s8e2d35syR78g93/Na0jq1vSR+WZivEUBihx120NMit0oQBw4c6CC6Ktddd530GbNUXyjDOOO0i5RV4nfMcrOdbyYK0SG6qfoParlcBBHS88i5b8hHSea1uz8R7OOYat1envNRsP+oZW4U7aCey/aZGSkCTz/nHPqsEKiwmeiNBzztIH4N1e0riBBEiOlDZ8314XDHD5jjjl+h3VQi42CWH/gRUh99yEcYU5wG0a+HdBgnrXqrg1BWflvnH0o87oceekiuNdc76uNropUg5nrOMjri/UD7TRuQR1+68173+xH3nYUX+g1Ry0eWWWaZWIIYrkffQSeddFLGoXzfG/7JLGDR78suu8zfXdB3gm5RF9rjYkvTk4/IZAAiA4FCK8LkQHzd0tH10Jio4AuEiSllqvYfHwQrYCKuk4piTBgCjeTo1bTpYCu5uFhJv/x88S9Eo8gknaALvrR+/T9ZbScKKNolgjdQlsisTKjQMpXKR42VevBg8hqW+jSR0GAUaDEoWx8KDuFj2vTis5KHjHKtX/0vo0pC3UNC+YQnz8EEctsNxEQXM0+io6IB6yrRYBjcL2Hh3kLjhXZGhfG1lv0ok3j8pGpOnBLcb2gjfZNnSC9EA1zQtDD+httnyW/2oU0shkg7aR8uFkfwbSUIEH2tOfUYh4aXwB3cX2jUkdrTj5N+8GyVb7SOmBGinVJTPvqHmR7na2AR8l92RnTxgbppB3Ku91Ht2SkfMyL8hoUy4ic79xV5rjjff+7D5Qv9zSIJ14t7vWzEYOkj1xjyjyYr2yJCw+xrBU9IJM+tmJ2ff6oEUMEfGR9BSJwKBEu0qRN2kPca10tFTc5b3m1XTHCM4EZo1rgemORiritYvD1PFnB87Z3Wpdt8CaIEqhFf2SHBoppaTjS/+IxYCdA240ZYVOB3zYntc+Pai8+SfVgnEJRJF/wgmfouABvOk3OPS7k61Zx0ZLCvcvxmqfyG6TL1N10v7xLuY7SovGe5v/W9q3XJdt0VOqTTUDxsG49AvyOI6luokED4mNTgH4goQTzrrLO0iGxZ9b/vvvvc55+ntF5MYDkPgonJqP9Bk7DFFltknM+PXCam+daZZNKMyRf9DGscwp1TghiOYorfE+eHTcfQGrEfghsnccSP8pApfAORuHJdZWJ6+jp3uKOXneUwV/U/mMFC3r54rT2pNgQRbWNYPn3pe4dfZL7CKi3aTNqB8L006yPXWJupOYyqMylB/M8eT7oDFrnKffh0x5D+SceNryrPTdynrq6deCpBzPWc6ZjQkvHscX+hEVdtd9gMuzvvde1rti3XccSIEfJMYDIKmUsS/CUpQXz77bfdr3/9a7fJJpu4Ni8YC33K973hj4PIozzHxx6bGWrcL5P0O/1abrnl3C675E5onLROLceqNf/oMUkTf8SRg2UigPaAVW2OEaEUYTKsk4TqI/bLyNtFnjudOKDF6KzoZNCf8GmdZZ5/ZOVuW4rmk4kbZlvVU/ZPEdg0adA+yXbdFUTTQOqL2nNPlpV49cfRuouxVd9MtBlokZhoMvGHwKCJ0AA8mNkhmtcR4qhaRtl/5YUBplwPiSQofpUtQk4gKRA+TO90nEyyfRHfo1CURDQkTPq6StQMDrNbX/DRVHKHVlilbNN1U9EdYzQZ5WPXFIIAVqqBCZv+lW2U0p6xEFAM0YAeLI5gzphE8M9CuynP0kbryL0HGdRrFd6i8UOD1RmpSWu8ai88PahGtfv1N1wtbYcXB3jHQoBZIKq75CxXM32K+HAmHWfQUMIvaMLLN1rb+akZOJW20dD6IikV0lGRlfyqiS/4seigCwaQTMidCiaXBG/ChBdSDHnXgC/qFxsOfFOx7Vgh9phw89wGpqgEfxk+SIJ4af3hbVKCiDmnmsDzjkUzTfAqnm/xvSZVUEVZYMLPAhKEVrXNXB+Viu02lGtac/rxskt9jXkuVHTRT6OlNtx/Z4d7kGdK78dslgataW0+1hlNTz/WISWGtmnb7Aj0e4KIZowJEuQNiSOIYRivvvpqOW+ppZYSXyv8Gv3PdttlvkA4PxdBzLfOJJNmAlAwvrhAGDquOIL44osvyvlhgoiJKvVeeGG8v0Uc8aPNk1a51eEbiMSV6yqCCNlCo4kPYtSHAC8qcQRRjxeyhRDedfzLouWDKB7y62skdyTRSuMkCUF86sp3hXiiMY2SpONuamqSwEeYTkZ9MFlWiSOImFH7zxnlb7nlFrfkkkuKaSapHjBPJGUK5QohiKW613VsubYQwuOPP17MRhkDhI68ogTriZMkBBF8V1hhBbfssstmaPe1znzfG3oeW7V4YKGrGILJKmbq4WBDna0bs8yyEYNcxQ6biDaDSQKTIz5o5fjNyjWRIcWMixVsiE/IzAtTumCC8UHHyMv59pP2qQ8iGhYlFY2PprThEl1wr21TmqhdtnAQQDRHkEbxwXvpWUfC9nCfw/UW4zd+OvjL4YuGdoJJKeNg4lk2crCDJNWef6rsU02gmo5Kue02DKICar5HJpGIaKHIj5gO2KF4y6T2nOmBGRmaLOrGJ6vuigtc9TGHuqoJO8qktHK3rVIT0bSfGiZ0aopWjPFH1YE/k4xNguZcKbhQToMisTjBJB6RKLkQww3WlEkxwUTQSIvWeuhA0YhxLRH10RJzyJCPX80ZJ0ibtFsMLTH90lD/mOOFzTTpD9qmsBksuR/1OrIwgLaGPkE8II+QNUwI0fT6Gi4ZYAF/xFeP+tO57DChJJItCyeaiqbm7OkOcs7zwHObYYqd9qWsOmi3AlpPdgr59yBCvjmpXMthK7mmV56X+4Mk76TSkXt85GDpuy5C4VuHtkxw9JLdgzNaWxU09BArFd4pqhVlgYTzNaowZdTUlgUNTF9Z6FHtW/PbqYBR2QJL5SKILBahpW1I5yNkYUfeTy8/F5iPc/3oW93l56XGPnSga7j3tpSFRNpfPMOPMB2wSxd8AtN/LyCUvl+wpIAkVk7YIVU37/KTj3KYcKs5K5hg3honLGpRhneOSeEI9HuCCMFhMhc2MQ1rEMMQ44/FebnK+eflIoj51qkE8bHH4hOmq8leoRrEOIKoE0s1pfXHqd/jiB/HD/3NtW7Gzql+a7nZB2f6QUHMIEylDlJzxpA7xa+QfuSSUhBEbZOgPe8/+bW7dIsHZdx3HBMf/TAXQfzslfliNosvZpwkHXchJqZhDaIGpCGYCYJGnoA0w4cPz8ifqc9AHEHsjns9Dr+4/WjS8KfEioB3xDHHHBNX1CUhiPgdLrroohnBtPwKFbN83kV6/quvvip9PP/84mhq8O3GL5sIy8WWqv12CiYM/PPnw6RSCSFmS+p7hVmXpr2QyHaP3CeR7MhXqOcWI+y5BFzAbDAUFAJth6bZiAq2UWxs8q2PPI3ggEkW5AzcMCtEIK3lmwxpJ4hfpPy60cpi9opPJSZ2+FKhvdForUwaIRqKL5opfBYxt/TJneQbxORrZjtZ13OC7fBBohnGRxLtI35QpdIUKXYElwna5/4ikMvLzwU+cj7ZgtRSlqBCvqgfI4sVKpjaUTYuqAq+tHJ87isSaREtLpPhQoUJPlosriH1+j6tYoI6fJAc1/oxx4b8ynjSmsfgOTpxckl8/HRhBTIlCxHplDRM6INFnBGDU4sYmw4V/zPuAQglixsQVQiwf1/peIq5hdRjaYCfrFzH9deQZwZs8ZdEm8p9ruS65rjDgoidaPbU3LJ+5lXSVxaDwBnzacwiw8K7AsKJOTZCQnrK+36WkGn2oRkjgAyEX32h1WevMyamYlYboxWn3aiPLDaNWkUWFNA0UsZ/NmpOS5kxi/VBY2MqtyLvzcvOCyBQ7SDaVM5XrSPfWz5KBSqs1QWVUasE50V94Z6Wfkbkx40qb/uiEejXBJF/4gSoIVAMecGQpBpEAkzgg7jyyqnkndHwZu7NRRDzrZP8h0xAp02bltmQ94s0FZQhMmk2yVeDCHYEIhk7Nn6FRolf2LeQZPZC/K5pN/GauPi17syhdwZdJFDLhRvf3yUEcdZBz0g7SSKilpIg6uBbW9pEi5jNxzAbQawtaxCTWcx466va/Se1ft0mHTf+phCzuI+vnY7SIOpzhvmhplZR37ewr+KJJ54o92uYIHbnva545btlrGgR46KUUl8ugqjE+oorUmkAovqQ73vDr0N9jB9/PDMYk1+GqMwrrriiS5oOA1/rv/zlL8G19usq9DvEAzLIhAy/G0iN+FdVVwWBPtQvBgKC6SckSH11mCxItMx7bw0mOH7wmEL7pVH3IE70idDuaBnwVZMJCqv/aYLlt4FJGpOdKD8rv1ypvqvvIT5E+HeiRZQogB+8I1EGMYHThNQakIZJINophH2Mr/mN1wMiXHvmNNGAst/31fPHIL5dEPuxawnhY+Inpmtf/c81f/iu9KF+zn/EX4hItUQ27SpB60LfSVNCYB75ft4p7dcxTaDpj2oV0XT40XN516EZhjTwHeG+oy6/nD8mNb2F1IkGl4lzKHm4Xz7pdwnStOe/JJiT+pGipYHoq2C6KJN7rtmOmwgxhYzU33RdMG78D3m2IC88h75AZDAPRoOaj6hJZFk6IibPD+axoiVKa6BUy4a5sy/giLbaX3gB63Df/HMK/c4CAX5waIZ5Ziq2GSMEEO23Lyw26f2iWi60ghqMB2sCtGbcW2hjeVbkntBcggQKumGGXBsfS+4jzf+n7fGO4Vy0hjynWFeoplLMobl/suTybG5d4O56tVk+X/zQHrRO62ds9B1SC1njOkBEqyZNkHbFEuD8UyVab9XEVFJ7rh/vM4TrQv/Q+KoogYb8q7aTMiy0IBrgSc5TM9L0wolvvq8mr/oe0vrDWxZJdIFOFwrDZex3bgQyCOLZ9zW4Pa6odefdH28SlbvKnllCJ65nnHGG5EDEb4eIo5AngiuoJCWIlCcgBecTyRNNJGSMPIMzZsyITH6diyDmWydaGKKwEh2VFXsm0ZjtzZvXHnWOOnfddVfp57hx4yS3IRqhiy66SNIM6LjzJYict8cee0jESaJcRokSxNPWvsO9fsenkpvviUvflgig+L9pmgvOhQziK3fP9FfdOw9/6c4bc6+QnK7QIJZ/WSN+hUQmnbnvU9I+focQWfIS+lJsgvj5q/PdA6e/7t68/wuHHyPRU2dOeEoI6/2ntQfdoA/0hxyNfOgrwWf097fvtSdavmjTB+R8NJAvzf4o4/Pt++3l8hm3j0G27/qcnX766fKcobneaqutJBLu3Xe3R44lcqkusKBtI6Iu9yf+uzxTYYLY3fd6tjFzDE0cYyaXIzlEedYnTJggYznttMyQ4+BAGh0+mKiTsF5/a/Rj6oREL7LIImJeiq+0/wnjk++7SMfDO5A+KHHX/f6WgF1cEz5hzbBfTr+Tb5ayjKmYgh9e1D97JklMLEjLoP4v/NYPK9EEIGESyaq+7tcgHZ3pI+TGj8IodQ9bydVedEbQjppo+u3UnnG8RB8k1QZ+PJBZyCa+SuJ7eNaJgfmdf16xvquPGVpZfH8kCuKoVUSLKJNhTMguTWk7mMwjEjF22EqBGSWmvRocg3Gj4cEcke9ohVQkHP/9qYkgk0+OQ0pUIABoXlSTJhjq9RsyIMO8Ts8pxVYJMdoX1XfIJz4AACAASURBVNxgNqf94Voj3INKqjiGOa5P/oj8SHRSSBWivo2QStUMsWXxovX774KJLD6XLIJQJxNuvdcJ6sPEGPyZtNMWBDbsoyeNhf5AwKlP/Eox0x4yICOCptyHirW3RStfc8rRHX0sh64opIQIn0Jm05GEMe3OVyRKKPgdd1iGT6vm1dQoq369vAOq9t1JCDr4cf+KiTl+u1uM9IsW5btokMeuJRpzCSA1PdoUXxenMEtVH0R8KhFMy4n064uQSNLZ8E5rbJRnvnrSBPH9pRymwpgoy6KM5/fKMY0ajCk3gY6Q2nNOEhxYCJDrnU5BIQcL/CPvy2ceb893mH4WIPIqSvxoU82Fa89PLapwv6qoSSlaU1JSUF7OSSevVxcC3V91QEqjzW/12xXrBKK2pt0HtO64rfhKbrdh3GHbnwCBDII4eVa9G3NajZs6p2Ni8gR19egiOnHFP4bJC5PTtdZaS0zB/I7nQxCZVJ166qlB+gqdROGL6OdW1PqTEMR86yTAB/5J2jYRS2+4oT28MG3jQ0Y///znPwflFltsMZnQat8KIYhoHWj35JNP1moytqRUINrokX++QQgLZI9ommgUm+oy/evIK3jCwJuCcpds9qCDzHQFQaTTX71V5s4eebf4ItImH/wSiZLqS7EJ4tsP/U+0hdomW3Ii3j3tFYcm0RdSevjl/O8PntEeVZXIq/4x//tjF77pV5l43BknZfmBuePIkSPl+eLe4Hlbe+21HUnZw4LvHFpoLYeWW02awwSIc7vzXg/3PfybRRe0hfocsiXHKtr9MPnKltuRBSwVopX69fnfWRjyJd/3BucSeZX34OGHH+5X1eG7Bg5iIUy1Ih0KhXYMHTpUUuSEdpfkp/omEdGUSRWTLyblmDaGI5WiNdRJCOWKIfgEYfJG+gAmQKnJ3f+CdqLMBTUXGOaEQW46b4Ie9DEmZUJn+y1mhZiJaptEF0y3xcQKzYFq1DTEPuZ8aKDQpATannRyeeqhHGZvfMc3S0UiyqaTx0cRRDSFnEPd+AFC5rl24hc5dGBRfPO0L9m2SgqV/EHK0ERowCPV6kEC6K+YForf5A5BRFCtHz89CBZ+hTwzmCBDzljAkGihG68jBBLyTF36QTtEvfxW7Sn3FwREfDvTZSFwaGKiBEKPWa7edyxg4NuHlph60WiqYHIqUTbxL5x9bSrgCZrCtPZTFi9uuUEIjKRVoH3I2PYbif9vZ8y0aQNtEwGo0FryPPI9IIjHThTtNmaJ+KVpShbFSrdgSkAUJSg6ts5uxddvxCDxyVU/Q4KqRFkEcByTVzHvTT8TmLxHiWhAxw0LzNIxAUYz2ZiOKkswGxaJxJ9wyADR6Pv14JPJ+80fL1FQWWRRzXc2E1O/rmzfJWjQkAHB84c5KJir1o9zA5NPFojuujmV4zB9j/I+VlEtOfesPmfUxftAhWO8V1hkYkFDr68u5Kn5LPvrr8+0qOH5giSr4J+IdQY+khB2Dfqjx22bDIF+RxDRdhFogxxlxRTM8D755JNEK+xJ282nTgJZfPXVV1kDYtAu5ebPn98hEmLSPoXLbbbZZqL1IV9dNiHgCuktcgk5/PyUGLnKF/s4pIxk8vSjpSm3T2Ix2ucfZdV3de77jyold6H+cy5G3UnrKPa4GQP+aGFyFO4Ppt2kgmERI6l0172eq3+MmeAsH330kYypO64jfUz63sBHEhKbRCvI861m+Llw4Lim5PBzZCY5r5AyOnHGtDOXEHBCJx5ozkolEFNtx897qO1hTibH111BJuhMgtA0QqaYyGteMybjaBlLJZA+JuV+gBSIDWRBNYhRSbq1PxqNkLEwKdMJKtFXEZ4BiKaSEn4zJt9sDi0K/nlqIghZwKyNOosx0dW+5tqqeSftQgoh+giTVrlWac2FEkZMnPMR0aQ+cKdMsFXjphpX6lezXEgXCwjh+5lJLr5mqTQh2ecvaNeIZinBYIYMkMi5EBMZm5diIZ/+U5Y+FDuIEhpXyRm63qoOjWbFlql8jIp5xnb4SqKxJZAO+KAhbfOiaOc7nmzlMYeGvPFMIrSFVhNfOd+Ek/7TR/WnQ7PL74aH7pHIpdzP+E3y7oHscj4+nvi0QmrKNlxTrAkgThBdLB7wXeT+ox7eBbmEtjEXZlFB+nLFBblOyXmcRQV8LVUkErG3cMF+8TccMVjapF39MBbVgFOOxTP101bNsW9FoG3oFq281qUmxpoGhf2QVI0Kq1pytrzLyFPJO4YUGvLhufUiqmobts2NQL8jiEkmQ7lhsxKKAGk/0EaOHj06r0m+nm9bQ8AQ6HoErrrqKtFOXnPNNSVpHMKPf2W+eSEL6YyalfqBQeLqYdKiq9n41ZRKVFvDZCZqQs0kn/xqZWPXDIhRuC+qMWFi1JXCqj5kIglBFNPQ9MQQ8icaNyIWDh0o5n/4rzER9LUu7BNcmjouGEJU0bBxHGLpTzKpX7VixcZDTPPQFkHUdt86I0IifdCIrhznUzVhh8D0rjN9QeOtk1h8+YopmDaKxpLxpIPecM8XK6VGsfoaEI8Xn0lpjgkkhenqsRNFU4RvLKaMJFGHjBHlFKKG1kwjBUtgl4RpPTrbb+4H7nMViCQECJ9PRNM1VB+xr9wres9kbNEmj1ldNHAQRwgk1g0sxqgZcuBPeElmyjVtN2rLQhPtoJXvrGAmja+wCv6TLMbpONnP9SjfcpSru/RsVz3lANGa57qPCW4lz1BEvlFtyw9Qo8Gh/IUa/MxVg6qBgHhG1ZcVawY0uHKPQLI7mbdT+9XftkYQ+9sVL8F4MSvEVI3gFP1dMBm9edLzOT8PntluEtrfMeup4580aZJL8pk7t3ddy0cffVR8G6dOLR1B6sprKlExier3TKY5eFwfNPJorkAHcecn2R8E4RgyIGMy6Z8rK98jVxaNkO5n8onfk/j7pcmIH4xDy3XFFu0dEznIbJxo4nTMJyFwCCH50SwSIRWTUcwUfdFz1KfPPybjHjLAkf+NCSBaIszYJDjF6NUk2qa245/X2e8QejGtvfScDH84v14mvpjBoh0tZkRVtK5J712/P33hO9eS9DUQQvxi0dihQYV0qUCYyrcYJabY1RP3knsSLS7+h2hIlUyE/fz0/K7eqi9rw603is8oiyhE/aW/3PtoEXnG/TFG9ZHniOdPNW9RZcL7NIouiwOdlfLNRwjmEC003m2NDZJiQ+vlmQF7NU3X/dm2XEvGxIdrGfduk7b1/ffuW1IlGns9l/eKikYUxvwX01Rw1kUXyqOxVG2jnmPbZAj0G4JIFEASWWOeZlJ8BObMmSNBNIpfc++qkZQcJL7P9bl6l/iokb1rxH23t7wvknyyRQDtiejge0h6mlJMtLtjvOTtYyKgPlu5+qAJuSE1nU34HdcW5qI6mfGjEobLYzLI5IvJIGZzmtNRzh0yQAJcMInC5whTM3wdu0o03H22CaqYyKUnclGa0qi+qhlc8ysvZBwWsjBqlQA3xQ8CgU8WAYZM+h4C3F9iEpi+j+S64+e49ZhUPsbRqwUkkIBUSiowSUZ7JASRpO1Z8uJ1JWrql4c/bmeE5wntO+k1kgpRicFPtW5Jz4sqh1Y0eAa5NuSh3W5DCaSlmjm0/PkI2ke/zrAZtdZFwB4pRyqddC5btLa8k8SnML0YRfkgUqx3/2CyjFkp0Vj1fK3btskR6DcEMTkkVtIQMAQMAUOgtyCgPogEt0giEBGNvofWolRCsnH82nIJ5o1qJhtMnoYPkmihGjBF9xPptKsE8zF8B7OZ7mrAFrS4+QgBQKKEBOFoPzAnI3S/+CZGmKJGnWv7ei8CaJnRoqJBRMPGYkndJWfJwgiLOPiW8QzwrJMEHe02qS5k3/prSNJ2IrzWHD8pIJDdhUZAEJ98uNNdQCuKHyBmqElEAvyMHJxVw93atsC9/lmLfMpqMgPhhdtA08kiEPkgIeeYcGLOy3tToxuHz8n2GwsJfddJqpuG6IwJ+EajbSVYVS7BLF1NjfU9iUYaiwDxp771BvHPzFWPHe+IgBHEjpjYHkPAEDAEDIFeggCTFyYGRGRMKkxUIDdMNksl4stX2Z5WJq4dzWEWTG68lXCZAI9dS5LXFyNvY1wfCt0P2SbCJCZ0PUXQ3lYfsV9WszK0EXHai54yDutHJgJVUw5IaZXCWuahK4qmDdPOQkhLZiud/1V//RXSz2JpvDGP1ABOuXpHcCzMu7NJTcMCyVZAxoIH5mUPdpStnkKPYcFRuduWkvO00DrC5/EexywdP8mqA3dxmJtCZPnfwLuglAHJwn3pS7+NIPalq2ljMQQMAUOgnyHAhIPIgOGUFr0FBsyuqg7aTdIgYGpHYmgiCKIRYcILmTVJjoBGkYxLA0FNagKswVuS1947S4o55tOPSeAhgg9p6pLeMhqCuFTtv3OKIOoCSjoFBIFZ/KAx3T0mopaKj2EPfW67myB29/Wx9pMjkEEQj7ul3m12To2bdlvfy4OYHBIraQgYAoaAIdCdCKCZIkhJT5r4lRoPiCDj7mohzQSmeXVFCI3f1X2Pag8zXNEovz0v6rDsA2dM3fwAG6pVxNcTgo55LduaE46QKJUE5eiNQvoDjYjra6n9sff0cYmf2ZAB4qtIOpD6668sqfa/p+PRmf4ZQewMev3r3AyC2L+G3rdGyz+8yy+/PGfeub416uKPBhwbYuzii99aqsaamhpXqnQDpeqz1WsIlBIBEoMzmSVQgklpESDogxIHNGvFFggoQWYwveuMkO4DokOaDPyi4vwYyTHHeHLljyRiatVBuwZdIgef4hDekgSde7LYwv+bpscfkkAkVZMmuJozp0kgIoKMJDUrzNYnyGH5Bv8UzBrvvU00pwR5Kd9suCNSZG+SBfV1QQqI3tTvntZXI4g97Yr03P70G4J43HHHSSqGiorcPiH5Xq7m5uaikIrGxkbp4xFHHJFvF9yRRx4poetfeeWVjHNLOW4aKkZU2M6MO2OwnfzBP+s11ljD/epXv3KvvfZaJ2tLfvqNN94oOekuuqg4k+F//vOfbptttkneAStpCPQwBMgzJ5N0otjV1/Ww3vWt7kj4eBJLk4D6vjsyBocJJgFjCEpBwIea6Uc6wvYTHTCpQBA1bL1GoEx6rpZrfulZ6R9Bf9Agla2/hgSm0FxoWo5tBeH5IYjffePv7vCdkPhEZtQoh5KGYNpk0UyRaoE+kxNRk9p3qKCTO1o+eNeVbbR26j5Xs0lvS+TafIS0Kfh2+cGayClZudOmAZlmrJV7bpNqc+TKOUl0Pu1b2d6BgBHE3nGdekIv+w1BPProo2USXl4eHT2t0IvxyCOPSKL4J55IloMrWzsQpZ/85Cdu4sSJ2Yp1OHbZZZfJeTNmzOhwrFTjpqGNNtrIjRo1qkOb+e4odNz5tpOrPJpDyCHX4NZbb81VvKjHDzroILfQQgu5++9PPvGK6wCa5IUXXth99lmyyGdx9dh+Q6C7ENDcZkziNXF0d/WllO0ytoY7b3K1Z58okRkLJVD59JFokeQgI9k4qQEwrdRopHWXnx9URRJ3knmHtWn6uy0HAQsqck4IJedlS5nhlw9/rz5hUioVwtAVXeWe/5KUH7UXnu4abrsxXFRyt9GW5l8k+TiRMRtmXysh8fWE+htmpIjkF5/KLsHl8H0kP51EbdxiZECs9JxibSFqRKeknzXTjnCkRKCf5JxsfOS+VOL177/NaK6t/EcHqYwjrCQKJ5qj+lUSCZL6fd/c1m++lpyD7JfPqFUcPnwm/QcBI4j951p3dqRGEDuJ4E033SSEorsI4jfffON+85vfuPHjx0eOpJQEccUVV+xTBBEA0RzecsstXe4L1Nra6lZddVX3f//3f662tjbyWibdyfm//e1v3VFHHZX0FCtnCPQoBCq2WT81ef/kwx7Vr2J2BosFjcBKTkaZsK+7QlGj+4X7izaQEPD41mkoeMgKYfRpnyiAKk2PPyj7ME2EuKJVIyVH8wtPy35SUISFugi6g6kq6Qk0GfiCulohbpg2Mu58RHKnrbeqtNlw8/VZTyWBvZIfvkvOS8V29GpyrOrQPaUOJYgQM19qyctHXs0XnvZ3F/V7y5dfuLJ1B4qpZy48II1oBnVcbLlvWr/+MqNP1Yfv40gdoCLXdN0VHEFTEBYCyKnH+RW7bSmaYbnvhq7Y64LW6Bhtmz8CRhDzx6y/ntHvCCImpmiKnnnmGXfvvfe6KJPTlpYW9+qrrzrI3+zZs92LL77Y4Z8appwvvPCCO+WUU4QgXnrppfKbfXy+//77yHuqsrLSPf300+6OO+5wL730UobPoGrSDjvsMDmX47T/+uuvR9bFzt122839+te/dl99FR2uXQliknFT37fffusefPBBN3PmTNFmfffddxlt81vHuOyyy7rVV189+M3+ONPMYo87o1M5fnz44YfunnvuCcb05ZeZ/1ibmpoyxsA48AuMkrlz53Yoq3hw/cJCW1xrFhCi7jW//H//+1+5l0444QR/d0HfuYd+97vfFcX0uaAO2Em9FgH8/jA77E4hTLlM0t+Mf/cVu39o80rhgxfXT4KeYOLY+sWnEqkUM0o0dvjrFVtIaF099WBHvkLaQTTJPeaibd9/K3hXH3VQ0DTkDiKJFlcFkgExDF8bzkdLR640jgWfEYNc3WXnyemMi/35BnvB1JLz6q++RKLVQlhVJLz9zTMd/YY0NT54V9B2W01VSktHGoSxa0k6jqYnHxbiyvkE5aFekourSB45omNO2V93iQa72JFkMeWVMV1/ZdBO1JfWTz+Se4SykqR87+0kFyC/qw/dU7SFjIlrKL6XQwdmzFXII8c1k2t59CHSZsVW6zlyXELa0RhL3TtsHNW87euDCBhB7IMXtURD6ncE8fnnn3dLL720TMQxJWQS7fvtsZqH9oVj/gci9MMPPwSX4fe//33Gcb8s36+8MvPFD3mArP30pz/NOG+vvfYK6lSCeMghh7itt946o9ykSZOCcvoFwvGzn/3MTZgwQXd12CpBzDVuTjzvvPOCNjF3ZBy/+MUv3CWXXBLUCxEOj9X//ac//Skoy5dSjDujgSw/0KRtvvnm0l9wByv6uv32mTm7vvjiiw5jAq8o+fvf/96h7NS/LOUOWXoJ9+mnKVMlzmPc++yzj5RFwwueiy++uMPfMJuss8467q9//WvGP/ls5eOOffDBB9LmtddeG1fE9hcBASbZRNTrS1Kx4yYy0e/OMVXuvpVMXAnW0VVSe9aJouWKC37S2X7gS9n48L2u9rxTJAk8+c0a77o5o1r8+0oRmEfSaEAydt0iaI8gLkI0jj7EtZX9KN8rD9zVkW8REobZKUmoyS9Wtd/OriytgSNZOeeRvBziQdqEsuGDUmk6IIejVpHAKESexFySsnyHKPI9lxYw6GD6C+awpP/AlxGtICkPMMmUPvlkdJ3lXeVuqftG+vfOvNSYdtlciBT+k+ynvwhmvfxuevbJoEmC1rCv+uiDHf6OZSNXlt9oW6v2H18Unz3ur/IN15YPuTIRCCh4Nsy6RsxwGx9ILdDUz7pGsCXYjArJ0+lj2dg1U2a3isGwlWQ/GlcVsGp5a56rGL956pzRq4kPZ/m4YanfnDsk5X/aVl2lp9m2DyNgBLEPX9wiD63fEUQm6wRu+fHHH91TTz3lllhiCbfmmmsGsEIQTz31VDdv3jxXV1fnvv76azd9+nSZ6B944IFBOTRDEAslTHPmzJHf7OMT1kDhYwY5gfx9/PHHEtwFYnrXXXcFdSpB/PnPf+5WW2019+abbwrpWG+99WSyjybMF7R81IkfZJwoQcw1bs5/4IEH3G233eYwWyXwzpNPPukGDRokpBYcEMalY/zHP/7hIDT6m21Yk1mKcceNNbyf6wwxu/vuu11bOicRmt133303oyjmnYyPz7nnniuYxhFEyvjjffzxx92ffvVLt82oEY56VA499FD3y1/+MvApLCsrE7K6yCKLuPfee0+LddieffbZ0j4a7s7Kxhtv7AhYY1IaBILAGVuPLkkDTNi6I0CLRoEsFVFKApZqmxrv6TpfYMhB5Y6bJOleXmXQ1oh/WNqfj6iSTPCjAr2gzYJ0EcGymBN2ArYI2Rm1SkbfK7YYKdq11oqyFGEYtmI7cRDyMEBMUSGYECqScCu5gEDqvYJ2EpNSNKDkpFTzTO4hxop5atNjD8h3fB+TCjjgV1ex/UZyCqS6etK+qT4OXVGSYDfcdXNAPiv32T7oP7n+aLty53FBcxKQZ8gA8UfEJJPjmi+x+a25wbns5zpJsu1D93QEsIH4kqMSjVxnRM08STPB843Zb/mmQzPapn3INbhBjlXri39h2SbrpsoOXVGCBjF+8SEdnfIXrZ40IUUC0z6O1BX5GTE4RfoxOZ7YvlDdmbHZuT0fgaaWBW7GE43y+eCbzt3LPX+01sPOIJBBEK96otEdObveXf1kR1O5zjTSE85VonTOOanVQ+0T0T8hWWHypcfZQi7+8pe/CBny9/M9iQ8ihISgITvssEP49IzfShAHDhzoIBQq1113nfQRU0VfICH0vaoqfuWvM+OmLTShtAF5DEsuH8RSjTvcj7jfu+66q2gNw2ayceXZf/XVV8t44wiif259fb34DaI1xTRXhe9oKydPnqy7ZMt+sJwyZUrGfv/Hs88+K2UIPNRZwayW9pKMpbNt9cfzCfBRsfWYvMPFowHAJA4tApO/6qMP6TDpJEAIGhu0HrmE4BqE6K894wTxuepseHw0JUwoSdLeXSI+cpgVFpgaAVNBNDL5CNEx8ZErpogfX1oLRWRWNF9oi4gIWrnrlpFN1V15YWBWyGS/WKJ+bNwvSCr5ePpac79BJIYPEsybnnjI1c24SMgekTZ9PzkwoiykjS2J6VXUbJN7iKAq+M9RBtPdlrdTGj0/tYSeF7dtef9tOR8CipCagvqIqgrxVqk5/vDU/lOOlq20WTbflQ1NEV5MbNUXr2b6FDHX1NyABHNBNHIuxDHs38dxngfqbX452rpE+5Jti3aWOvCxbZ3/XSo/4boriHYSc08xF02bydJPBC0650Em9RriO9jy2cdBU5Bz6u3w0SBDvGemHCDXFzJI4Brqa/ngHUf0U//6BpXaF0PAEOjXCGQQxMmz6t2Y02rc1Dn1fQ4UJUrhKKb4+DGJDmvh3n77bYlkyUQdLeHf/vY3t/LKK3fAJQlBxMyPNh5++OEO5/s7lCCGo5ji48b5YXPBbbfdViKo+nWEv+c7bswywYK2GLdqANEshiUXQSzVuMP9iPv90EMPCTHHHPjYY491b7zxRlzRYH8+BHGPPfaQ1CL4lPoCmeZ6sfgAbv6HKKlbbNFu5uWfx3cij3Iu/e2ssLCx3HLLuV122aWzVdn5MQgw0VPtRrgIGgC0AyoE8CDptkziRq7sKrYaLRqZsmErubYf52sx2dbPuFgmt+I/tGCBRFPEDDAsomFgYphOU0DdmGeq6Vq4vP+b4Bx1l5wlhMDXVKqZXdOT8ZYJfj2l+K5alroLT8+7eiEVQwdmNWWELOA350+MRRuGKWPa2iDvhiNOaLjputT1HjJAyL6SEfwd1YwQs8L6mVcFZ3P9IJD1114m57bkSXSDikJfao6dKPURhIXFhLKRg1N9gzycOEW+l49rJ8jck2g0w4So2iNh9f+5PKMVFicCDd/wQaJRxC8RnGXRA3KUh78bGmTuab0PNHgRz5YvYg4KWU3nz+QcNLEaiIbf+CHWnnZsimRtPCRlEsv1TvuOl6+f0uz65px+G3wnwI0f6TV8PNtv7qvK8ZvJePAN5Bnn3aH3hJ4r6S/SPoa6j3eHkmPR4A5b0TW/9mKQooNyVQemzGNZYEBjLJrotjZH/kgIbxDl1DNB1fptawgYAoZAGIF+TxBvv/12mZATsAaBIGGaxyR9mWWWkbx4mOkRCKZQgkjQEeoLmzaGL0YcQSRIDueHCeJmm20mJrLhevzfcQQxPG7OIXrnkksuKakeBg8eLOaJyy+/vLRdCEEs1bj98eX6DnnbcMMNhSiCIXkO0dLFSVKCqJpVTFLDonUstdRS4u+Kz6v/2W677cKnBL9Vywi5LIZgsorJcj5a1GK021/qwGwPYhElhPRnQqgik9X110hNUEetIhoVJn6Y3oVFNCLDVnJVB+8upm1McP2JspZHA1B95IGpCfgXn7m6i86QcrQVFib8aC/xuULrI3UOHShb38RM/dUa7pgTrqLLfguBQVN0ytEZbaKVguCggY2byIMBydSjhMAcEpwlTY7wp4NUcB3UXBK/u2IKvn7i30ibQ1cUguGbKZJvz/c7hNzja6fRQvX+kH3zXhViK4sPM68S/0Cf5Gbrdy15/9JaQrb4muIDSSAUCc4CicqhQUUTqL6IqtULtwkRwm8RogjRp260YNx/0m4eCdolVyF+i7OukWa4fnp9CEjD/Y+oVhONs4wxTRA51nj3LSlNqKdhk4A9YLDdhnI+f9TvlWA7ukiAZq7m+Hb/f1JCVO5RWI5ZDaADQaZ+AtX4C0jakeaXn0s9k572WJ8Hxsazi3myjHPEIDFPlTyT6WfZv5e0Tu4dFgSKqZHWum1rCBgCfROBfk8QNTALQT2QqVOnRhKiIUOGZCWIjz0Wn0tIyUShGsQ4grj//vtLXyG1cRJHEMPjJuANAWmGDx/uiDiqQkRTiFUcQRwxYoQW7bAt1bg7NJRgB8TrggsukKBE+GPigxolSu6ymWXiO7rooos6NLhRopidddZZUYez7iN6Lniff357PrKsJ+Q4yDjxhSTarklxEWCiyiSt+oj9Iism6qIE10gneseUtOrQveQciFqcQAzKN04FAsGfi/MwZSVQRvg8JQ1oeTBfhNTRp6qJe2WYraLp4XyZVKYnypBPAn+oVpOIkIgSxPrrMrVDcf0txX5NNQD5VcEkksk1prkQLlIuRAnEBFO8sOC/RuJwMGCSDxGomTZZzO4Ih/zUcQAAIABJREFUXKIEMVeC9XC9SX9DkDRQipD6tKayfP01MkgI9UHaZNI/YpAQQnzMxCRZSc7QgUE0S8omkYabZ8rYqYfIoG0N9UI2uN4t33yVujc2XifwfYTEYkZNigtICxrPip3HBfcQJrkkkkfj7AdG0b4QnZX7DhKODx95CcGe9pOKRlrFXy8saOEg3GgmqyfvJ3XXnHF8qn9DBmQU594RUn3DDCcm2RefmSp/zKFBORYc5DmDOG4+wknaiE2GSGTUoBBmrlMPlsUdf1+S7/hCMv6oCMEsUGi+Qggt5dTElDyZ/OYj48SH8s6bhGBWH3mAmMYS+EieccmrmOnaQN+a0qlJak8/LklXrYwhYAgYAq5fE0RWXglQgxkeqS2QsWPHirbQvzcISoI2MUqDSPoCJvTTpk3zT8n4TpoKyhCZNJvkq0HUgCrZNGJRBDFq3Or7FvbRPPHEE2MJ4qhRo0TbGA7Io2Ms1bi1/kK2p59+uownjgDmIoj4hpLeY4UVVoj1/SSQDT6IUfdLrj6rvynBb+KESKiY9yZNh7H33nuLD63e43H12v78EFD/KHLHMQHFZIzVezXv1Imd5ouTQCAElkhPSiEsfHwRkzrC7E+a0G4CCOE7cBfRxFAWjYySAkwAA1NBJQ/pLX6JiAYHoQ7OU38ljRBKn5l8Nr/ygpTHjI7fdf/O9NeWg130RyNe0mcVIdxMsO+7Q7RfccFOIDyE8kd416F14pqAO+OqPffkQENEGchH1T7bS2APjoeJJ6Qcs79iCdow2kHLhuD7BhlUoT1NUM/9gLZJA9twHiabdRCc0atJGgf1rdPz47bco1xj9VFl4YE0G/SDgDjUTW6+MgLNpCOVyr7QfZWxb8zqrmKnca7hvtuFtGAKSvoFTQJPWQL/QDIDgrhB8sBZamIKEeReoM8QtyCdxpjVJQIrRJbnTe9lNPvZBP9P+oYJsC8smKDpg3BhAYAZZzEE8kl7XEdMP33hemAWChlG8JekLNpTMSFPRyelT9wL4MDCE6TPF9U4cy+HhUA21OkTxNZvvxYzZlls8aKkhs+134aAIdA/Eeh3BBGCQIRItHlbbbVVEOVSL//BBx8sBIJy+P6hBSLSKb5jURN+NG8c/8Mf/iBaGggjpppEQfWFgCmQxHHjxkluQ/zjLrroIkktoeXyJYikVaBOzZuo9fhbJYi5xk3kUiU1RC8luip9Zdy0EaVB1Oiu+NQxZnwX//OfzJXeUozbH1+272eeeaa7/vrrHfkFn3vuOTdjxgwh+qQ28QP7EFX21ltvlc8BBxwg4yWSLfswxfVl0003lePHHHOMXEd8WPXz/vvvB0U1gNCwYcPchRdeKLksMXelD7QXJ6TlwDQ1G5nDH5ZrwifsUxtVL7kpKct4TIqHAAElZNJ15YUyKeY7H7RUTPowR+M3k1yk8YlU4nFC8QdRC4cOFGKpvWICp/XotnKnTWUSzAQ4CFJBeP6nU1YLECAJyvL6y079zOpvuk6Sn1Mv51MXmhZEIh6us7yE/uc3k3qOa8JwDRCDFq+7RBK6i9/YmqLZA2sibjIx1kAf5L1DGm67UTRUlbtsnhqrpmPYZkxgnlt9/CQZI1rCsEmm+Nq98ZpErAQH1eRQt2pzCN4S9hXrDDZok9DgIpImgnbff1vSS6g/KRo/hP76hEvvC6J7KqmXgnn84TzqUXNEyKPUO3wlISqQaSJ2lo0YLNposKbPmKWWb7VeqmwW4gjBRFvKIoSaa2oqDTSmSYWxC+kbtUpGmxBoNHm6UKL1qQ9iNi0lZsqKoZqo6vml2uq7APzQxkI80chCzngXYN4KyUVY+KB/9XP+kwpiwztlz38FxF58OYcPkvvV7y/BZjgPH8WwNMy+Vo6RRgVMsQ6Q1CTpawheWCOYGAKGgCGgCPQbgojZ38iRI4UEMVnGL2vttdcW8qBgsCUNwujRo4MJONFLb7jhBrfnnntGEkTOgRihUdJJOxFLOccXkrFDOv785z8H5RZbbDEHcVPJlyByHmOC8MQlYU86bupCe4b5peKDxlPNW6MIYkNDg+T6A0sd+4ABmaY9pRq3YpZtu8EGG8gCgPaNlBfghWbTF01VouX8Laakvmh+SL+MfocIqkDwuN4sHOhxtvgi+nk3tTxbiCMk/fDDD/d3d/iugYMgn+HJbofC6R1Dhw51pEsxKR4CwaQLM8UhA0SzpQmw2UK4ZKKXjsSp5oVM9jFLJWgE+8Ir/kykISOY6EURRp3cknLAF7RkTL7RRqmPGxNoyvsml/X4aRGcI201waSRMrSHaETI2jPjrSL8dkvxXQO0oAkq32TdgJQQjEQC+KBJ23YDaRqiwPgg5pg8lm+0toynYpsxgq3kqiToD6T68Qdju6tRLZUgSvJ4Ismeeoyci1lmMQTTZIKtoAlD0BjSt5ozpwWBU8JmiJhFUoagJpX77iiBhfwonvn2S4O3sOigQrAkX/MmOQU1DyC+bpuPkH6rNrNq/53FHLn2zBNkkYH7Cm213kdar26VIGK+ma+gJUULScAeNOBx7z3VMrOYECdimgqRWn+NgLzGlS3Wfq65+klyHfUDMcQPVJ9X2tMgUWr6jZZX/GS99BryzI4YlHGe+nhiSi6+tmdOC9KNyDOPJcIhu4t1Au8gMTOurgoWH7BGMOn7CLQtWOC+Km+TT13jgr4/YBthwQj0G4KoCPGPBb+sbBoayuKHh99aPlJdXS15ACFO2YRy8+fPD3LzZSub69hLL70kAVjCkU/D5yUdN7n8MKmF2CUVomWigfzhh9QKaNx5xRx3XBvh/fhnMh58TH2tYbhcKX9jlvrJJ5/k1PahiYXsJ9EKkofTz7uYq/+zZs0SovrWW2/lKmrHEyKAyRcTPSZ+mnybUyGLmKfpij7fESU2vh9hzclTXfMr0WHzGx+6J/AbxGQSooLWkAAstBv2yyISJvuZTKo0/fdR2QcRRchLBxGCTKlIjjeCeqTfW0R6pB4CjHSXiIYDYjt+M/EHwxeTPuG7h+8b39FSRYlqdkkyrqIaSc0np/vZoj1DewIRpV4NfgOJZx+Ekf3+NYZgMUnnmlIn58eRFlI9gC2aH6LGQky4R7QvmBmTY09NeyG8aJgIbiP1trW5ptdS6SLEJPS+zHRH/liSfseMVs1YCdQi5rfkZ8S8efJ+qSTxaZ/VhofuDhYTWITwA6Zg5gphSyJKYOLSeySpI1cZNd+NC1LE+Sx8yL0U4zucq41Cj2PGW502c6457diMNB1+neReLN96tCtbL6U1hUSiQfSF9wDBmHwhEjHj4lN/XWoRSLXeRF6VYwTV2md7MfflXDU/55haGPh12ve+h0BNwwLJVkDGggfmNfe9AdqIioZAvyOIRUOuB1V0yCGHSDL7qFyFPaib1pUsCFx11VVC4K65pn1Sm6V43ocg/PjRYvpqUhwE0ADphEx91GSyzwTtqotEk8LxwBfwiYekPP6FTBbREjEZjBKNMkpQFp3kUU7zylFvy9dfupZ32wk/GkmIZNv872XySV/QuFAWEoAmTX23/BQWRDClDEm4EQgtv/0AMVF9LOU+1XhAWFL9HytbieqqGpghA1KmvE8/JpFNtT/4eGEyRwAPFfUXhXCSzw7ChqmhBF7ZfiPXRH66dFtt333jVMuEf15AOGdcrNVJ6oeGm68Xf0bxeYR077pl5CRbfAy9ROhoDzHz9QUfMfI2cp0Yb+Rn+EopDA7cRfzt0PhAZrkHWr/4zK8u0Xc0kBAqTBoxKybJPAsHLDyQs7Bywg7SHmRSfGyvvyKIBkoQJXzl9J5J0iDBf+S+mrRvkuIFldEIoGhDSe8QJfhI0g/8OBsfvjcw3YwqW8x9aEEx6+R5496LErTW3Lv1t9wgfdT7ALPqXMJ9r+V55/BdzVar0vkjxbqAe//H+annPJ0nkmBNJv0DASOI/eM6F2OURhCLgWI318Hkn+A6mIeSv7GvyqRJk1ySz9y53ZfguxDsH330UcmnSARdk96DAGQMwoW2SwPTNL/7lkzMCHQBsWOSpsFU0DCp1lEmciMHu+Y3XuswYPUJZBIejqip2hvaVd9CUj7gIybkb8SgYJIobfB7RDrfHcRjyIAMDSONQ4jUn5Hfat5Ztd/OHfrWVTvUl0zGgGncfju1j2voikE+Oczx0C6GtVhCMEetkhEESEi3ly+SutH+QirAQNviuqHlw18MM1yNpMk1LUQgm1wbyC11qGlvVF0SzfKDdyTSJsSUj/av5b23Ay2n9lXH4JuKRtVbyD7IE/UL9ppwfdQqElAlaWAcv10IKfUR1KlUIs/YOdNTQZ3ej/5fKAGg0iS8av/xGWaapeoX9QYE+bC9Y5tBYwzZx5xWrzHvjKSChpnz6tIm1WgOU8/zAFe2/hoOqwSJ3JsOfENZLAVYAIgS/Efrr79SFkI03UpUOdvXexAwgth7rlV39zSDIDY0L3C1jQscW5PehQCRREl70V1mlF2BFik1knyyRQDtin7m2wa+h1OmTIk1Ucu3PitfegQ06qNGJKVFfInEbysdcERyxkWYauIXSNTH5vfeFo0Tv9FA8gnC9A8dKNrC8EgqdtkimDjqBFK2IweLWSCECR9CInVCshpun+0a//uI5E2DqPjayHDd+huyQZ0EKekuqT37xGCcTOjV7JV+YVonPnnrriB9VO2t31eSn6OdRUvkC5NviCIRIsFDk6RD1AXHMauLFo3vEDME8z5+Ewiou4WFCDRjaHlZSCByZTbC2Zn+oiEVTNJRdQnYA4EpVPCzg8CwyNGdwrWG/JMOBZPrrhKJRjtumGDacPssFybZ4ENwK/nU1zm0ytXHTnR1112RuIvqv9g895WMyLcsoqA9l+vJwsjG64iJNAsQ2UT9rNF84ucb7nO2c+1Yz0TACGLPvC49sVcZBLEndtD6ZAgYAoZAT0NAI4E23nubkAz8miAS9elogUykMb9jQla5/85C1pgYo0USH8K0OaNO2DK26YA3UWMmCAtl0Rqi4cFckuAfxRSNuIiZXqnIR67+4v+nmGB6iR8gUUoJ3MEEGhHfwQXxi5kEBqm74oJECy/kyNP22PpaLqJ3si9bgJtc4+mNx/GhJZKtb8bc2XGwcKH5/TpbV288X7Wycq+Ry5IATHzwS40wLeY5iDNHjRq/mE2jpa6rlcUgNR/Xuolwiim03NNpbXq264uvbVzQoaj2bV/PR8AIYs+/Rj2lh0YQe8qVsH4YAoZAr0FAg2GQmB6/MQJ7ECSiKe1nCDHRvG86OZPtkAFCEKsOGC/J0YlESr5BklyjgWz5+H2H5jFOmp59UiZ3pTT30qAs9Fcjesb1pxT70VJoGhACt3SFkO5AongOW0m0jj4xRuPIxJqcdCadR6AzWsjOt979NRA4ShcdMt4NaPbWX0PMY/Fhxsy0sxLn00qAJ7Ty+DSb9C8EjCD2r+vdmdH2Y4LY6pzDvOQb59xX9uk0BkR8renMvWjnGgK9BgG0K0zmJFz8TdcFWip8oJgAQg4hOvj8VU3cK5Xc/puvAu1XTx4oGjoNyd8dZpWaHoTJM/5PXSVoX9qqKruqOWunnyPA/Sam5R++J36Hpbj3MDHHnBbzUqLjshCFRt5Pq9HPL0O/G74RxH53yQsecD8jiJgjfeScu8c5d7lz7lL7FB0DonA+7pybX/BNaScaAr0BAfEZymLiyBjQNBLtsbcJmkpyJRLNsqsFko2foYTjT0dW7eo+WHuGgCFgCPRFBIwg9sWrWpox9SOCiIbrZiOERSeE2Uj2I8RELM2da7UaAoZASREgFx9+fCaGgCFgCBgCfQMBI4h94zp2xSj6CUF81zl3mZHDNDmcO/do9/TTh0ficddd+7nPPz858lhhGtfrnXM/dMW9bG0YAoaAIWAIGAKGgCFgCMQgQJaCU+9qkM+8L3C1MjEEohHIIIi3vNjkzrmvwd3+clN06V659/0ikp1s2rLuOdbcfJFraLgg8Rg/+WS6W3rp37ittlqlwzmtrRe7gQP/6FZY4Y9u/vwzOhwvjCCCywznnGkieuXjY502BAwBQ8AQMAQMAUPAEOhXCGQQxMmz6t2Y02rc1Dl9xSwQP7i+qzl85JGD3WKL/dw98cTERGSuuvpct/zyv3eDBy/tKivPiTzn/fdPcEsu+Us3dOhyrq3t4sgyhRHFWc65ln71cBUyWAKEkGOvv0jTk49IXrdc4yVfV82xEy0PVy6g7LghYAgYAoaAIWAIGAKdRKCPE8Tbi0hwukdDmI2M3XTTXu4nP/lJYoI4efL6bpFFFnbvvHNcVlxuvXUfqfeSS7bPWi5b36KPvdrJ2zX/0wlN35XJkPPvYeYZteec1K0JyjN7k/+vth/nu/rr4xM7k8qh6enHgoo1GXfTMwQ2ipa2775xZaNWkTxhrZ99nFGISKEkhG/9jmjE0UJi7OZXX4w+aHsNAUPAEDAEDAFDwBAwBDIQ6MME8YsMcoMZ5gsvTHbl5WfL/m+/Pd3ddts+7pVXjswop8SmsTFVHrIURagWLLhE6vv++5Qp5mefneRuv32CmzfvaMcxrcff5qpTy9K3Bx88yM2cubu7//4D3XffnZ5RH31mLKecsrkQuUsv3V5+s4+P9knrY4tmEHJ4+OFjMuryy/jfR48e4BZf/BeuoiKFl3+s8O9XOee6zny57vLzU8mHR63iWr/8POPGz/Wj/upLXMMtN0iyYXJWtX7F/VR6qZqwg6vOkvut7ccfXPPcV3ps8JCGOf+R6JeazDyMWPWU/V3FFiPb00K0NLuKbdaXdBEkPo8SknWT8qBq//Gu8bH7JX8X5cg7SJ48jjXMvjbqVNlXte+OUqY7InLGdsoOGAKGgCFgCBgChoAh0EMR6MME8eEMIgRBQtsG4Zs6dUP5zm8+++8/PKPsM89MEj+9hRdeSEw4KbPJJiu5H344MygH2WP/uedu4w46aGRGfXvuOaSDeWaSOiFe5523TVDXQgul+veLXyzifG3e73//66CMjsHfXnnlTkE/lcwdeeQGjvq+/PKUDse0jL994IEDpY0ZM3ZOVN4/N/t3fEK7Rip22NjVHH+4a/38E8n7JKHz993RNT50d84OlI9d01XusY1ruOtmIRdoqbpCKnffylXutW2HphY0NgoxIvWAJFceOThDE9fhhBLvaH7xGdf2w/cZrZDHq/biM1Nk7J03Mo7pj/rrLpfj5LpDW0rCZhnPOsu7mulTOpiQMm7yDVKm7ftvJZk8uQUh/KRB0HMb77lVm+iwrdx1Synb4YDtMAQMAUPAEDAEDAFDwBDogEAfJojk42s3C1WC+Pe//84NGPAH99xzRwhZ2njjlYQIvfXWsVL+q69Odb/85c/clluuItoztIH33ru/g6TtuuvaQZ1KEH/721+4FVf8f1IfgV323ntdqY9ztP2kdVIeYoZm85tvTnMEoHnyyYlu0KCl3U9/upD7+utTpU5I3hdfnOzQHEIM58zZU36zj09NzXlB29oHxj1s2HId9uvx8Ja2l1jil26DDVZIfE64jujf7eaFHe7GTuxom/+dq7v4TIcGq+mxB9yCtjZXsfUYVz15P6m1+fWXXNmwlVzF9hulCMqLz8S2xrmS4Hy/nYWwlI8bJv5vsScU8UDlnv9yFZuPyKhxQXOTqzpgvKvYcpRrvPc21/zyc44+VWy3YUY5/weJ3DkOoYRw4usXp6Hzz0vyndQH1Ft7HosN7VJ/49WubORgwTeOUKP9hNRV7rqFq9xpU6kDbW1A9B6+t71C9M2PPyjHdKyYolK2eupBrmzEoIAkkpxeBSJZd8FpghmJoSH6YUy1rG0NAUPAEDAEDAFDwBAwBDIR6KMEsaEDqVGC+Le/LZVhsjlr1h5CsmbP3lPOOfTQ9dyiiy7ifvyxXVsI0TnggBGigSPQC7+VIC633O8yIn5ikvmzny3sxo9fK+hD0jqjCdWlDo0gRBDy6JdJ6oNYVnaWnD9pUjLzUm1j7NiB7o9/XCyjTT1W+DZe05N5a+b3q/WLz4QEKNGoPetEV3XQrq5i27FSUe25J4vGqvXbr4Vg1F15oeyHfDU99ZjDd06lraIsRUIO30d2Ve64iavaD01qcaTl/belvqr9dnLVR+zn6i483bV++pFUDhGEfJGEXQXiC9n1+0ifKv61vhbJ2GKGWj1p3xSx2nqMQytZvtlwIW/gEGf+mVFJlh/4dYJzzSlHZ5TS/RyLw2tBS4srG72ajLGt/Ec5H3JfvuHarnzsWg7sfak943hpq/bUY2R3yycfyu+yMau7ukvPcTWnHiO/m19+PnX8zddF41i+wT8D7WTlLpvL2P167bshYAgYAoaAIdAfEWhuXeD4tC1Y0B+Hb2NOiEAfJYikVGjXHvJdCeJZZ22VcQxCd999BwS5/9ZZZ1m37LJLihYPTZ5+MBuFpL322lFyvhLEiRPXy6iPtkgT4Wvrktapfa6tPc8RofTaa3cRLaGasNIXLcM2KUFEO0rfMYf1z8/1fY89hggpZqy5yiY/fkPCWzO/YgsWLHCQJiWIbKunHCC/22qqHb59jY/e71q/+Ur24bOG6SlmqJRFs0gdCNo39tVMmyy/K3fb0kEyiiUNt90o9UNiyoYOlO9oN5tfelYII21r0JUFtTWubPgg8TvU9tEgUqb27BNlF2aY+OMhaNjQzKn5JRo7lZb33nblG6/jGm6eqbsK2rb+7/NUn0et4iq23SD1gbCC5brp8ay7gqs5cbKr2ns7V7n3dq7u3+e4BTXV0l7tOdNT4994iCtbb9VUXessL4Qv3CG9pmhOEcxaGTsfSCFtyPf0OClPtNPWLz514IvWtXL85lLGJ93hduy3IWAIGAKGgCHQ1xGoaVgg2QrIWPDAvOa+PlwbXycQ6KMEkTQByQhiuNwyyywhGkByBUZ9CALDOdkI4qqr/tmtueZfgz4krZN6b7llb0kz8atf/UzSUfzzn3+V1BQQvEIJIkFtOP/CC/Fty8Ql2298Mzmvru78vM7LVqdzsztxu8afqqQJzRmaNaJeqhYN89KKrdYTktX0wtNCFjCBxM8Q8kXQFEgGxAdpuH22/K45icUA5yCIFVuPlu/F+ANRgjipRrDlnTdc+SbrijlszclTpe3mN1+XplTjiaYTafngXQnoUjZyZdf69Zeu7pKzZQxCIt94TfqOCaiOk3HjnwcxRsuKlq7uqos6NQwxwRViO0Daa37lBUd0UbSD6i8oJG7M6hKARiOQVp881TW/8HRK6zdkgIxTykHmJ+7VIdpsQES5Nl98Kn0W89Y0QWz58L12gvj6S3IcjSxYQkqpu/GR+1xFeuGAhQITQ8AQMAQMAUOgvyJgBLG/Xvn8x91HCSLmeQQVaSdDcRpEvwzfhwxZVnzvcmnNshFE8gj6JqZJ60Sbia/j8OF/z8hTSETTbATxsccOzRhreExEReX8KVM2yFoufN7mm68skUzD+zv3O3eAmPxvY+dqph8phIAUCk3PPOHKhq7oml58RvYRaKZ8kyFixgmBEC0XxHDzEQ6zSDSFkAnVtjXcPkt+sw9CJpoxiNVFZ7iaYw511YftLdo7NZEspL91l50rxE3PJUJn9dSDXd0lZ0nbaDtVxKeytVU0f+Ljx9gef1AidzJO+kRf8b/0RQLCqIYyTarQWLZ88I5frKDvaA6lzTtvCs5XzWvZeqmUFIwRQXNXP+taIdmcE3yGDMgwC4bkVh95oGt88C45j2upZcmDCEkOCOLwleR6qglq03P/TbXV1uZqTjhCzqs+6iDZJwsGBLmZ/538tj+GgCFgCBgChkB/RMAIYn+86oWNOYMgflvR5j79vtV9W9lWWG096iwmrvkTxDhzTr8uvscRxHvu2V/ImB/9M2mdzz47Sc4955ytM/p+4ombRhLEJ56YKPunTds0o3y4rwTa+c1vFnX4FIaPZfuNv+bIkf/I65xs9aWOFT8fHaah+LDh20ZePAKyYLIJEYJgkJcPDZqadhKABpKn/m5KsNC6IfgnKjGpnrx/8L188xEpn8HLznMEk2m8/86C7njVtDU+dI+cT/AYTC0b7pjjJNALfb7x6qBuzCqrDtkj6EfTs086yCl+eLWXnSe+lfSXoDGQq8a7b3GQ3IabrnN1l53nqg7dy1XsNM6RrqPqsL3FBJN0E1UH7hK0ke+XhvvvSPVn+CBXe/6prvHhe1J+jp5pb/WkCUG1Yua793au5qwTXe3Z013tWdNkvGhBMaetn3lVhokwEVEZB+Pi2qIRhJQ2PfGQ7FPzX4LRUAYSjUgQnCEDZEFA81+iRaVM5e5bS2AbgttwzcNRWIPO2hdDwBAwBAwBQ6APImAEsQ9e1BINKYMglqiNbqoWk7P8CSIRQoliutRSv3L77jvMPfzwweJ3ePfd+7kbb9wjqFMJIv6Fd965r3v++SPcxRdv54hqyr62touDsknrJHIpAW5WXvlPEr30rrv2c+PGDXKYm0ZpENE4Emn0D39YTHIiQhgxUSUXoz92vuNPSCRUP1VHuIz/+9VXj5I2zz//Xx3q8svl/734WhwNKkO0TsgKWjKiV2LCCTHA/01MUIcOFB88yqk/GuaSaooaEMSrLpLzOBeiVr7leq5sLEGH2oXE62r22b432beWt+ZK/fgEIvWzrnHlmw6VKKOND9wpxwgmg2BGKeQXP8m0v6SYVh5zqGjkIFaQYfpaNnwl0ZzKd19Tx3dMOkcOTo1/960duQUJ+NIZqRy/WYCTtkm/0X7yO8pvE19QLatbykFsIfr4hWJyi9Rff6WUhcwhkHzMRzlPyXndFRekfqfTXKCJleOP3Cfn8EeC4oABJq+YH49cOVVv2oQ4KGhfDAFDwBAwBAyBPoyAEcQ+fHGLPLQ+TBBrnXNXBOQmqYkphIegLmjOIGsQMz583333dYL6lCD+/Oc/DcoQ8fOww0ZHpplIUidtX331eNH20SZ1b731qu7FF6dIG2EfRMoTzIagONpPcjfecMOgo6jhAAAgAElEQVTuQT+VwD3++KFS5uSTN+twTMv4W9J1QEw1tYZ/rPDvtxf59k1VB7FQM0J84BrTqRKUINaceYIrH7O6EK26Ky4UMgYJIb0CQUyqJu4lhAESiaDRItALPoqQKHzYIGnFEgLkQGLwMyTiKH0juAxCfkGOVYzfzDXceZPkFcQUljGhOeQYmj8ineJbSV8rthkj+1vefSvoIgSYaKVoKyHBpRA0dBIddZ0BEi22+a150gzaPPqJ1i8sjAHs2777xuG7iNkufpGU16A7eg5mpYLFFiNd/bWXpcyIx6zuGu+7IwgopFjW3zBDCDuLA1wrDThEG0KOMRd+P0XIwUX8Jr/9WpuyrSFgCBgChoAh0OcRMILY5y9x0QbYhwkiGGHO2K5FzPd7S8tF7tNPpwtJamoiLUJ7XUoQiWJK6ovy8rMzjvtl/e/Z6tRyra0XSz7DcJt6PGpLH8i32NAQH3F0s80GC+lDoxlVh+4jqM1CC/3EnXTSuKzltHzy7bdFu3HDFWFyic8aPmoqGuClnGiZaW0gfokS3XTsWq5yly0k+In4JULY0oFOIBK1Z05zLR9/IFVRDhLXWSEiKVoxTFXpj5CfHTZ2jU88HFRdf9N1qWPDVhJfuuojD3BtaYIXpHgYMsDhV4nfnQS3SfvcNb+WMt+FbGJiimYSP0S0qZBh1boFjXXyS9PzT0lfw76PqtWrPY3cormlrboqCCoDRioQW9KVQIbBCk2jBu/RMqpxxXdTUmhgXjp2TVc/42KHD6JESU1rWFveThFYPde2hoAhYAgYAoZAf0LACGJ/utqdG2sfJ4gEqyHvXjuxK9Z3nyAWq85S1/P55ye7xRb7uRs9eoCLI5+Vlee4QYOWdgMH/tHV1xczeukLnbtTCzgbU0whYmqCmSZlkuphzn+CfIBCJCCI815NmaWOGCTkQoPQVB20m2ihfJPSBQ0NgZlq0q6RSkO0WWi5xq4p0UQlwiZJ5884QUhN2fprpPq88ZDIavHLa3r2CYe2EJNRtGxo4xgTZEmCuKRNKJWE+tti+t1JMCCI246bpBLeX3uZq7vygsCEs/aiM1zDzde7muMPF1JMOo4oQduJBpi0FFHaTlJ4+MTfr0OjtZLuAiEwjhDKYSuJv6gEzklHTG1+4zX/VPtuCBgChoAhYAj0KwSMIPary92pwfZxggg2mJqSe6+4JLE3EkQwICIq5rKYkIYxQXO5wQYrSHoPNKfh44X/RkPW9QlZyZMHOUIrR9RPCBSmhT7R4w4hbx7lWt583anWserQPQMCqJEy8TtEIEZos0gbkY9gzirmlEMGSJAUzsUUkvrw3UPL1/QCJqapFBBoxMKC5pBon/jTQYTQRpJEHm0hmkPIFOPD9JQ0EWjcNM0DwXeKKRC72tOPa8/lmCbgPiGV7+utKprTuLYb77s9dZ1mXhVXJHY/UWZpAy2wCoGKAh/TlmY5Tpk4goiZKqavJoaAIWAIGAKGQF9GwAhiX766xR1bPyCIANbgnCO9QvFIYnPzRW7EiL/nnVuwmH0otK45c/Z0s2fvGYnH9Onj3Ny5HYPcFNpWysy368khV12Jh5qOxj06GsVUTUoxL4VQoPnC/BEtFH6KDffe7qoO3l0iikLA8hUIW92l5zhyMGbLyYfPIe3jnxgWTF0hZRBLcgpW7r5VQICkz9OnBKegXSO9B3kINdF8cDD9hTYglpi+4udXiGAWikYTk9PGJx+RoEAN99wqbdKHBXUs0sQLJqmQ9CjtYfxZqSNiVkrOy6kHRxYl5yS48IkiiLogUHP8pMjzbachYAgYAoaAIdBXEKhvWuAOm1knnxc+6rgI3VfGaePoPAIZBPHRt5rd9c80ucfebu58zT2uBkgKURtnRhKjwglQ8Uhn3+oDuezmd+tdUH/NpUIMcpE5zYNIBEzSK0BU6i4/PxUddNhKTtIkjBwsZKzpqcdKPqbqQ/eUfkeRyIAQjhiUGbEUs9hJE4Ich5oOArKJhhJzW3wUGx+624ELhExzGUKe0JiijeuNQsoS8mBGCQQ4IIhvvt6hiPg/7riJpEDpcNB2GAKGgCFgCBgChoAh0A8RyCCIk2fVuzGn1bipc+r7MBQQxS+dc8875wiFf5tz7hb7dBoDIpQ+5JxjEl7eI+4fUl6UjxuWsy9ouSq2Wb+Dlo30GeRUJCpn6xef5qynWAXI24hPXpSQ7B1tH2ND84ZmENNYzCp9IXCMEqOoLdFHyYVIudZPP/JP7XXfMbsl32OUkGdRxw9OJoaAIWAIGAKGgCFgCBgC2RHohwQxOyB2tO8ggFapphfmuiMNAyk6OivNr7/sGohkeu2/hQiiPcTMUoPvdLb+nnI+5rZhv1LtW+tX/2sniB+8o7ttawgYAoaAIWAIGAKGgCEQg4ARxBhgbHfvR6D2nOmuee4rvX8gNoKCEfA1iK2ffVxwPXaiIWAIGAKGgCFgCBgC/QUBI4j95UrbOA2BfogAAXII0kPE17ayH/shAjZkQ8AQMAQMAUPAEDAE8kPACGJ+eFlpQ8AQ6GUIEJ225Z03elmvrbuGgCFgCBgChoAhYAh0DwJGELsHd2vVEDAEDAFDwBAwBAwBQ8AQMAQMgR6HgBHEHndJrEOGgCFgCBgChoAhYAgYAoZAcRGobVzg/nVhrXwe75Mp7YqLV3+uzQhif776NnZDwBAwBAwBQ8AQMAQMgX6BQE3DAklnR0q7B+b1xZzn/eIydskgjSB2CczWiCFgCBgChoAhYAgYAoaAIdB9CBhB7D7se1vLGQTx3a9b3Qsftbj3v2ntbeOw/hoChoAhYAgYAoaAIWAIGAKGQAwCRhBjgLHdHRDIIIgdjtoOQ8AQMAQMAUPAEDAEDAFDwBDo9QgYQez1l7DLBmAEscugtoYMAUPAEDAEDAFDwBAwBAyB7kHACGL34N4bWzWC2BuvmvXZEDAEDAFDwBAwBAwBQ8AQyAMBI4h5gNXPixpB7Oc3gA3fEDAEDAFDwBAwBAwBQ6DvI2AEse9f42KN0AhisZC0egwBQ8AQMAQMAUPAEDAEDIEeioARxB56YXpgt4wg9sCLYl0yBAwBQ8AQMAQMAUPAEDAEiolAbeMCt9OltfJ58t2WYlZtdfUxBDII4suftLgH32h2r35qN00fu842HEPAEDAEDAFDwBAwBAwBQ8AQMARyIpBBECfPqndjTqtxU+fU5zzRChgChoAhYAgYAoaAIWAIGAKGgCFgCPQtBIwg9q3raaMxBAwBQ8AQMAQMAUPAEDAEDAFDoGAEjCAWDJ2daAgYAoaAIWAIGAKGgCFgCBgChkDfQsAIYt+6njYaQ8AQMAQMAUPAEDAEDAFDwBAwBApGwAhiwdDZiYaAIWAIGAKGgCFgCBgChoAhYAj0LQSMIPat62mjMQQMAUPAEDAEDAFDwBAwBAwBQ6BgBIwgFgydnWgIGAKGgCFgCBgChoAhYAj0DgTqmxa4/a+pk89zH1pKu95x1bqnl0YQuwd3a9UQMAQMAUPAEDAEDAFDwBDoMgRqGhZIOjtS2j0wr7nL2rWGeh8CRhB73zWzHhsChoAhYAgYAoaAIWAIGAJ5IWAEMS+4+nXhDIL4wkct7p7Xmt2LH5vauV/fFX148E1NTa6urq4Pj9CG1pMQqKqqcgsWLOhJXbK+9BIE6uvre0lPnZs7d657+umnc/aXZ+Hyyy93LS02x8gJlhUoCIG77rrLff755znPrampcddcc03Ocn2tgBHEvnZFSzeeDILY2Waam5tdQ0NDZ6sp6PxStz1ixAi3xhprFNQ3/6Tq6mr/Z4/8XmosizXofLCcPXu2GzhwoFt44YXdT37yE7fEEku4Tz75pChdefjhh93PfvYzd8cdd3SqvnzG06mGuujk3jCeUtzr33zzjRs/frxbfPHF5V7j3jjwwAO7CPXiN9MbrmPxR128Gv/3v//J+4H7QD8vvPBC1gZuu+0299Of/tTttNNOWcv1hIO8R5deemm31VZb5ezOkUce6RZZZBH3yiuvxJbtzvutFO+D2IF2wYHuxLI72m5tbZX/8yussIKbP39+VoRvvPFGeT9fdNFFWcslPfjPf/7TbbPNNkmLd1s5I4jdBn2va7hoBPGRRx5xiy22mHviiSe6HISuaHvIkCFu5ZVX7tTYNtpoIzdq1KhO1VHqk7sCy2KMIR8sH3/8cflHwAv83//+t7v77rsd/xR+/PHHYnTFPfjgg1I/k7pCJZ/xFNpG+DyIsn4gM2uttZbbb7/93Ndffx0umvfv7hhPvp0s1b2+3nrruV/84hdCCu+55x53/fXXyz2Xb/96QvnecB17Ak7Z+oDFws033yyf/fffX565559/Ptsp7uSTT5ZygwcPzlquuw9CApZffnlHPysrK7N257LLLpMxzZgxI7Zcd95vpXofxA62xAe6E8vubPv99993Sy65pBs6dKhra2vLivJBBx3kFlpoIXf//fdnLZfkIJpxFqA/++yzJMW7rYwRxG6Dvtc1XDSCeNNNN8nLvzsIYle0XQyCuOKKK/Z4gtgVWBbjKckHy912203uzQ8//LAYTXeooxgEMZ/xdOhAgTsgh8OGDXPXXXedu+CCC9wBBxzgfvWrX4nmC61oZ6Q7xpNvf0txr6NNAVc0iH1BesN17E04X3311XJ/5CKIWOKwsPDxxx/36OFNnjxZNILvvPNO1n6iVf/Nb36T87nozvutFO+DrKCU+GB3YtmdbQPrrbfeKs/ZJZdckhVlNI6rrrqq+7//+z9XW1ubtWyug5z/29/+1h111FG5inbrcSOI3Qp/r2q80wQRUxHMZU455RR5IC+99FL5zT4+33//fQdAvvzySzHHg0xWVFR0OK47sBF/9tlnHaYArMBSHyYgKoW0refm2rLyxErUvffeK74Vq6++eqQG8dtvvxUN0syZM2UV6rvvvsuomt+KxbLLLuuoR3+zfe211zLK8yNXnf4JkB60FNo+2EZJY2OjtMuLM+qfeSmxjOpPIfsKwZJ2IEF///vfC2ky9hzuW3xuuD9YOYQUhDWIua5jIePJVWdshyMO0GfIsy+ffvqp+8Mf/uD+8Y9/OPw1fcnVdr7jwQ/p1VdfdUzMMAF+8cUXs/rr9YZ7HS0EuPI8xslHH33k3nzzTTnMBOW5556TZzhOo42WhnuNZ/ftt9/ugNEHH3wgPjdMUG6//XZXXl4udb/77rvuqaeeiutG7P58r6NWlOu9jqmlvu/oI+Ph3VVWVqZVZGxzvf+1MH5t3Bs8f1gLRNUH4eJ9q9hwL1M+m6mj1p9rm7Sf1JOLILLA4P9/4BrmEjR33B+YuL/00kuR/n253v9+G0mfM/4/Yi56+OGH+6dHfuc98+tf/9p99dVXHY6X6n7ThnJdn2L/7+O9yTWMurd0rEl85LT/+Wy1ftpPOt+g/lzPrvYhG5aFtq11R215JzKWKKsWxTluEWX06NGy2Jltjkmb//3vf+WdfcIJJ0R1Ia99hx12mPvd737Xba5WSTprBDEJSlYGBDpNEH//+9/Lw8WkKOpz5ZVXBkjzT2qfffaRcqwmotrHtA0CGBYmOjxo1Im5ltbtk5t82g7Xn+03pBSzGW1Tt2ET0/POOy8ow1i0r/6qFYRZz4/a/ulPf8roSpI6OYHJ4Oabby5146uCbwv1b7/99hn18eOZZ54RHxHMHzADptwmm2zifvjhh6BsqbAMGijCl3yx1CbXXHPNSHKvx/PZEjiCCZH6MvrX1CeISa5jvuNJUmc+Y6HvYYLI+WeffbbcIyzKqCRpO5/xMKlntdXHj+8soPj3Je33pnudBQPG4d8LiqFu8SvjXcLk/29/+1uAAcT8jTfe0GKyJYgCE2sfp5EjR2YEYdh2223FnBWTKu5LVsMh24suuqi8YydNmpRRZ64f+VxH6kr6Xj/66KPlnQ5G/rX/4x//2GHRKsn7n7aZOI4dOzYDH7TgjMEXyAwYQkqnTp2aUR6zz0IlaT+1/lwEca+99sroG6Z6cQLuYMr7378/qMOXJO9/yufznFEef0L+78UtSmofmKDz/2nChAm6K2NbqvuNRpJcn1L879txxx3lmrDwpcKCGHEMmPtAwksh3Yllvm0nGT+LSrzTmOuERTW+V111VfiQ/H7ggQfkGmQzadYT11lnHffXv/61w+KbHk+6ZbGOZ+Laa69NekqXlzOC2OWQ99oGO00Q+efwxRdfyD9k/knNmTNHfrOPDytOKoceeqj75S9/Gdh7s9LLg88q5HvvvafFxJeBf/L8c1QNJJpDVt39+vJpO6g8xxfsx+njmDFjZLWeVSoe+gEDBnQgGbyAmAhiPkP/nnzySTdo0CD5h60rXvRXsUArw4tIf7MNr6gmqZMhHHfccfIiwp9O7ezBKrziTP2MZ8sttxRtLRNzJmiQ7l133TVAoxRYMtHH/DLu89ZbbwXtJ/mSD5ZMmpko8/n5z38uExT9zZagQ4UIwUbAk39KaCOIUnnWWWfJPyKfFCS5jvmMh74mqTOfMfG8RhFEtBAcww9KJUnb+YyH+/DUU0918+bNk6iyPC/Tp0+XdsMBXXr6vc6Ckt5bkDywY9Kp+9gyUVWBIBIkaamllpIJPvcRx5lYbLfddlpMrjd1Ya7Ke4kJPObA3H/4Oapwr9Mek1KeKc5hgYvn7ogjjnCYe+Uj+VxH6k36XlcyQ/8xaUYzynh0jNpHNGJJ3v/cQ5j+42/Ee5DFGybeShh9dwcliFgS8C5Ha8s7b+ONN5b2830X0dek/dRxsc1FECFTPAt8WDzMRhDxnwK7Qw45RExRwROtFVEcVZK+/ymf9DnTusES64xcgjadfqJdj5JS3W9Jr08p/vfxf4H/99ybOhc46aSTBAfmSKWS7sQy37aTYrDZZpvJnIp5li9bbLGFvCfAOkqYk/Ge3WCDDaIOZ+zTRVEWUzorvFOId9BTBYK4wek18nnwjXaLvJ7aX+tX9yGQQRA/+q7VvfZZi2Obr+hqjv9P2a8Dkx5WEfFZ8IX9/POYMmVKsBuTAfYx8UgiudpOUoeW2XPPPUXLFtZiJPVBRGNK35lQh6VQu/yoOiF34IlZRzYBQzQJYfM1/M2YkDKp8KWYWD700EOCBXhEfcIr3X4/cn3PhSX/hM8991z5LLPMMqJB1d9sk6wqhvuA+RermWggfEnqgxh1HbWeXOPRcuFttjrDZcO/uSZRBJF/xBzLdX2ytV3IeFjo+Mtf/iKLKH5fe/q9zkKP3lt77723YLf77rsH+zjmWz5AEMGXhQVfIC5E31NB24CZGGaovkC0OB8tIQJB5BlnogvZ5JhGlGTiw3Oui0h+PUm+57qO+bzXtd9XXHFFRtNoUfEDUkn6/ocUMlZIly+867CUwEpCRQkibfnvzFmzZkkdvqZHz8m1TdpPv55cBNEvy3srjiCyEMi7aIcddvBP6fA9n/d/0ueMRljcBfsk2mn6QNm4ibzf6WLeb/len2L+72NMmM+zOLnppptKGhD+XxMErKukO7HM1XY+GOhzfuaZZwan8YyDZ9T/r6CQc7JYhIVCLmGRj3uUQEqdFczmqSuXn3Fn27HzDYFSI5BBECfPqndjTqtxU+fkn38p18sVwsRDg1kKmhb/w2oxq0EqrAyj4aE8/yD5J87qeZzkajvuvKj9aAD91XktE0cQ6Rcro5gUYGKhq7q+NknrSPrSTFIn5IsJApqDY489toNpmraJxpJJpo833yHC4Ks+QVq+mFhiUsOENe7TmXyESbFkXMUyMb3lllsEMzTFvsQRxCTXUetJOp586tS647Zc/6h/sBANjoUJYj5tJx0PPnWY/fGPmeeHCXzYlLs33OuKcVITU7RoYUFzq1oWLBcwHeR9EhbuP67Pf/7zHzkEQeQeR9BAcUzNrs455xz5HSaZ4Trjfue6jvm815Ug8j7whRV3tKwqSd//+Awx1qhw9rzDee+pKEEMk3Lwuu+++zJMdvWcXNuk/fTrKRZB5P8NY88VTCqf93/S54zxqKaaxY9cwv0JYU8ixbzf8r0+xfzfp2MlWjbXCWsBFkG6Mg1Yd2KZq23FJ8mWdxcLh6SpUlG///D/Yj2u2z322EMWyDDHziZYaHCdmEt1VliMW2655dwuu+zS2arsfEOgWxHoMoKo/xh5UZIzKfzxTatAhMno8ccfL2Y2PLj44WAuFfWCLeaLnYlb1KpsFEGEMGBCAsElzDdmBeq7WChBzKdOAhNsuOGGQhTBCI0DK2G+sArNSlsYb/2NA7gvxcSSSS5mr3GfsPbS70eu7/n8AyoWQVRT0rAZbxRBzOc6MtYk48m3zlwYcs9EEUTuK47x/Knk23au8fB8q3kf9yj3Ls8Pz3mYINKHnn6vK06dIYhaB1u0kuFroMch1RzTyTkTcFKUIF1NEPN5r8cRRDR9PkFkHEne/yxggENU0nX+n/ikJI4gKqaFbpP0069b8UqiXcimQVRyHH4X+W3xPd/3f5LnjHoxZQX7Cy+8MNxkh9+YCGLql0RyvTcUv54+j9CxYnap/raYU3eldCeWudrOFwe93zENR1AgYOLMIkA20bQyuRaj1ZINBUYxBMsNtMe+tUIx6rU6DIGuRKDoBPGxxx6L7L9OosMruJGFvZ2sxrBKhIaRf0jHHHOMdzT1VUlNXNsdTsiyA78PJq6+8BKCAPoTVyZi+PENHz48I/+TjjOOIGbzfSukTvrJyw2fHoL64ADvm5NCbPnnnGsFTcdbTCxLbWKaDUsdD9tiEUQ1qQyTaoK5cG/qNS/kOvIPNdt4CqnTxyDqO32OIohqEqars4W0nWs8GihEMdP+RS3E6DG2PfVe1z4WiyDiP4MGEU1/WPQdc+edd8qhUhPEbPel9iXJez0fgqhjzvb+nzZtmjx3UREhuY9WW201rUYiUnO/J+lncFIeX7L1069GCU5nCaK+i3JpEPN9/2tfsz1nlNEJte8aoueGtzpJh0znklzvjXzuN7+tJNenmP/7tG3MySEKK620kkSHJuhKV0l3Ypmr7XwxYMEMiymCHKLtw2ze95GPq48YFwRCzCWYA/N+OP/883MVTXScORjKBqL7mxgCvRWBohFEfA95wPinHSU4aqPJ8klWVLm4fawSo12I8snI1XZcnVH7CY2MVhDtl4pOZv2+q806Jly+nHjiiRlkwT82atQo0TayqhglhdTp13P66adL2/7kI5vJq3+ufi8mlvipQNrjPrlWv7VPUdtcWPrnJCWI+MjgL4JZFhqHsGg4bFYHVfhnhTaWe1/JTiHXMdd4CqmT6MD8o44L302fwwQRUs+EBgyYVCGFtJ1rPAQS4Xn2hUkAGg//OfOP+9+7+l7PdW9o34pFEKkPjSqLPmFNO8FosFpgko6UkiDmuo75vNcLIYiKa9T7X319uBd84b3CBBKLE5VSaRC1ft1G9VOPsS0WQXz99dflnbP11lv71Xf4nu/7P1xB1HNGGRZNWYzkOc4laLp51/AeySXFvN+i2sp2fYr5v4+2L774Yhk3/y8IdMczy3s1brH23//+t/wv8f+/RI0h6b7uxDJX20nH4JcbN26cLHYzx4Qs8v8il+CyQNTnXKLBskiTEyVJ3//+ufijYxrLPWdiCPRGBIpGENEyoKkiih+rJrxsMUsjSqGKaiaIfIZpCv/kMGkhYAgO5Sqs5vCP6f7775e8Tky6CJHNP5nTTjtNiwXbJG0HhXN8ueGGG6QdVv7wryHYBGPChMufuBLIQwkvmhYix/EC45+ATxb85jRKI9pQsMHfSP2IKJdPnThsk0gZ0oLZBRgyuWZC6QcDwJ+MlSxMcvbdd1/xWcHvEMfvqPQixcTSH3uxv+fC0m8vKUHUfxJcP/IZhYVJEYFEuM/BDj8I/Jwglf41z+c6ahu5xlNInfxzpF98wn5ftMt+JixMTIhgyP3LPsykydWnUkjbucZz8MEHS1s852hk0eyAK8+P/5zRh55wr+e6NxSrYhJE9e9be+21HdrCRx99NEgT5K+el5Ig5rqOjDvpez0pQUz6/ud5XHfddeU9TIRI/pfwPmVSxjvbz4dYCoKYtJ/8b8PPlg/BwXjGiODLbz+6LVjyXtay6remv/1I35QlqAx18dwSZIfFHXzeSEmjks/7P+lzpnXj34WWOxzQTY/rltyq9DPqnapldFvM+y3p9dG2i/m/j3ca8wN8YXWhTbW+cYFq/vWvfwlOnBdHIrWvSbbdiWWStpOMwS/DHIv7CMutJAsTXH/KJ9EKomnkeYsjc0nf/35/mWfRPs+viSHQGxEoGkFk8BAeovDxUPBhlQfCpcLDxz9GDQWv5dDA+Ell+UeHdkGPs4X4sHIU9wDnalv7kGR71FFHBTnEIBc45GPqFZ64shrMKir9Q+vCai6RBfmt2iS/PfwnMZGgrI4NwuFL0joJ3cwqudbDd1bKIN1hof8c4x+Pluc75DdKiollVP3F2JcES20nKUFklReSAjZM9KME7YTe42iamfSqg7t/zZNeR20jyXjyrVO1ByzIMJkOi94LPKeYVpNHj3+mhIcPS75t5xoPfqlo67UPTOp5V0Q9Zz3hXk9yb4BZMQki9UEMCXjg48TChC+lJIi5riP9SPpeT0oQ83n/QwIJBqG5conmik9j2JSvFAQxaT+z5Yejv75ky8l3xhln+EXFyoX/p3/+85+D+wO/y7BGNen7P5/njI6gbeG+9BcrMjro/eD/D//DIWHZpJj3W9Lr4/enGP/7CJrEYi2+h2EtF+mmwIz3aVhUK8zxXDiFz4363Z1YJmk7qs/Z9vGe0XvdT+USdw4aPBYcNc1IXDkWcPifT37jOEn6/g+fz//UqKCH4XL22xDoiQgUlSDqADGJIv8SL4k44R87aQOiNBucw4QWB180Gbxkoya4UXUnaTvqvPA+VvB8X77wcf1NhC3655uk6rG4LSuKaGXiVl6T1olPB23z8vK1hnHt8oJlNZcXZpL+FgvLuP4UY38uLPNtA/PfuOvi1xSUQboAACAASURBVIV5X67IkEmvo19vrvHkWycahFz99NvP9j3ftqkr13ggo2oqma3tnnCvJ703so2j0GNMGKOidRZaX77n5bqOWl+u97qWy7XN9/2v92bcAmKu9go9nm8/C20n13m8q7k/uE5xkuT9n+9zRgAaJuG8Z7IJEXpZiJo4cWK2YsGxYt1vhV6f7vjfxz3Mwjj5E4sp3Yll0raTjJf/FSwy4M+Zaz6IwoFFcywLcgkWXdQbNxfV8wt5/2saHRZpeoo0NC9w026rlw9p7UwMgTgESkIQ4xqz/YaAIWAIGAKGgCHQNxAgQBBaS6wBci06HnLIIWKSium0STsCRNjE8mjnnXcW7WIS7Vj72f3jG+QZtx9IH65H2QQiSboy0mLU12dP2UY6IDS211xzTbYqCz7GM4E2GTP8niI1DQsknR0p7R6Y19xTumX96IEI9HmCiJkJyXxzffwkrD3wOlmXDAFDwBAoGAF7D+aGzjDKjVFUCSKLYqKHSV82YbKM7xhuGaRq6U1SynsDM/JFFllEUv1g4trXJR8syVFKOhuCrUHkcmkEIZKYSuO2hLVUNsGnG9wJQtifxAhif7ranRtrBkFkNWHGk43uoTf6zqoCCYUJ057rY0lNO3cj2dmGgCHQcxGw92Dua2MY5cYorsScOXMkUE7ccd2PmR5pL5K4ROg5PWFbynsDLVcu7WtPwKBYfcgHy5kzZ0ogNaJtE/gwiRAgZ+7cuTmL4ntImpZc5qo5K+plBYwg9rIL1o3dzSCI3dgPa9oQMAQMAUPAEDAEDAFDwBAwBEqEgBHEEgHbB6s1gtgHL6oNyRAwBAwBQ8AQMAQMAUPAEPARMILoo2HfsyFgBDEbOnbMEDAEDAFDwBAwBAwBQ8AQ6AMIGEHsAxexi4ZgBLGLgLZmDAFDwBAwBAwBQ8AQMAQMge5CwAhidyHf+9o1gtj7rpn12BAwBAwBQ8AQMAQMAUPAEMgLASOIecHVrwsbQezGy08ktyQRtIhwRq6krhb6dvnll7uuTj6t42xpanVNdckSudZXNSXCUuvuiVvwbm5INt6e2P++2iciL5YqT1ZfxczGZQgYAoaAIdDzEDCC2POuSU/tUQZBnF/d5r4sa3M/VLf11P6WvF/HHXec5HSqqKgoSVvffPONGz9+vFt88cUlrw/5ow488MDItmbPni3JXhdeeGEpu8QSS7hPPvlEyu67777ST87nM27cuMg6OrPzyCOPlDxBr7zySkY1dx73sjvwZzNcXUXj/2/vPKCkqNI9TpIkQRF8mAgGdJXFXdnFJw8XPLjCKqjvqPveE1QWdUEkB0FXWZIIoggSlChpGLIEGXJGgkTJOQ3DwJAZmDx87/xve3uqurtCDz2hp//3nJqqunXjr6p76t/fvd81xYfqZPPUw/LZo9OkVZFR8s9Co6TDHT9IwtGrfsVfOXNDxr65QtqXG6/SoU1RH6zzSxcOERCHff84S9qWHicnt53P101OSRG56n87/NqcmioSF+cXHTAiPl4kOTngJVOk23ShrHvKlCnq8zd06FBTW7J78tRTT8l///d/Zzc785EACZAACZBAtgjgXQPv+NiS025mqwxmigwCJoHYJSpJnvs8UbpHJ0VG7wP0skePHupl8NKlSwGu3npU/fr1pWTJkkoUzp8/XyZOnCjz5s3zK3jFihWqHXiZHDFihEqDF9QLFy6otL/88otMnz5dbRUqVJAXXnjBr4xbiRg5cqSqf8yYMX7FzOmxWQmy65dCLxD3rzityu731GxZNWKP7Jx3XFYM3S2JF/zVw6D686VNybFKFO6cf0I2TDyo0vs1OAwiYDmEOIQg3jrT8yNAfmv2tWsi//63SNmyIsWLW7fu9GmR558XKVlSpFAhkbvvFvn++8Dpv/1WpEoVT7pixUQaNhQ5c8Y/rdt0OVE3WtOmTRspXLiwLFy40L9xQcbAKo8ffY4fPx5kTiYnARIgARIgARIggZwnQIHowzgnBSKsf4UKFVIWRJ9q/U6xMCzSHjp0yO+ab8T9998fUoEIK2fZsmUt25mTAnHcWyuVSDp76IpvN03nsChCTMGCWFACLIdbZxzNd0NlYTH8+muRihVFIOJKlbIWiAkJHsF3zz0i48eLLFsm8sorHgE4cqT5Tg0c6Ilv0UJk6VKR774TueMOkapVzdZEt+lyom7d4oyMDKlVq5ZUqVJFrl+/rqOztUf+8uXLy0cffZSt/MxEAiRAAiRAAiRAAjlJIOIEIuYTrV+/XjBsDBa4jRs3SlpampexFogYYopt1qxZsmDBAnXsTSSiXuKR19cKgPIRn4C3VZ+wdOlSJfomTZrkc8X/tG7duvLggw/6XwgQ40YgpqSkqHbNnDlT9u7dG6CUrCiI09tvv11OwxwTIGiBiCGmsHwdXhcvvy444Tfk9OKpRDm68axkZpiHLMfvvywntwceRjmg7lz55MGpAWo1R+1dGqsE4sZJB80XApxdik2U7XOOyYGVcX5tRHL0Ae3UFtEr8Tdk26yjcnyL/z0MULxtFIQurJto566FJwVtMQbMs0Tdxi05Met5NKY9teO8KZ0xT1pKhjGpOnbqt18GiwgY02+/XeS110QOHhSpXdtaIPbp4xF9O3dmFZaeLvLkkx7hiGOExESR8uVFfEdazpzpyT96dHDpkDrUdXtakPV39erV6vP72WefZUVm86hDhw5y1113SbKbcbXZrIPZSIAESIAESIAESCA7BCJKIM6ePVu9lMEyh2Ge2GMzCiYtECEMMedPp6lcubIcOHDAyxiCC9fat2/vjcPBpk2bVPx4mE98AoQm8qBsp1C7dm2pWbOmUzJ13Ukgrlu3TtB+DGsrU6aMakPjxo3l/Hl/kQZRjDmN7733nmXdWiAe2XBWulaepIQarHmd7ppgElU6nRZeusChjRdKjypR+tS071d7tvSqOcMUF+gEghR1QshZBYimie+uVunalR0v/yw8Ss1X3DTFbJWNP3BZpcHQztndN3n7g/Int1prVbxtfMr1NBnWZJEqq1XR0WrOJsob9cYyU76LJxNN9SENuAYKEM64Hmg7fyxrUqDbfgeqwyouNjbrip1AhPWvXr2stDjCfMXnnvMIv9WrPdd++MFzvny5Oe38+Z74Bg2CS4fUoa7b3DLPWZ06deSBBx64ZSvvwYMH1ZDVQN8TgeplHAmQAAmQAAmQAAnkFoGIEYhXrlyR0qVLq6GY586dU3xhOdy1a5fA6qeDFogQkAMGDJBr164JLH4Qds2bN9fJxK1AhLUSw9KwVapUSZVTsWJFbxziIVwRXnvtNW988eLFlVDTebGv5/vm/Vtr7AQirIClSpWSl19+WVlBMUEZQhX9M/ZHd0z3FdZOq6CFH0QXHNZgfuDBNWeUMxkIPB10OieB+N1rS5VghGj8oPgYJaZwrLcv63nmaB5eH++N61xpohJKnStO9MYh/bbZx3T1Et1uvXxYaqyy3CHy+sVkJdpaFxstsGLqoAUiBNinj0TLkZ/jlaVvSKOFqo7Tuy/qpK734AJBijmUmZmeieBXzyXJmX3mua2wrl6Ou662pV/9quqzEohIB0GpN8zXbFNijGAuptFK67bfsHQvWrTIcrOyblkJRAxFLVxYpGPHLEzwb/TQQ56hqZiPOHGi59rHH3uE4JXfRhLDiN+liyc/hrFWrx5cupyoO6sXWUdffvml+gzjR5dbDY0aNRLMMWYgARIgARIgARIggfxEIGIE4pEjR9SLXbt27Wz5a4E4fPhwU7pq1aqpOUg60q1APHnypHz11Vdqa9mypWrD22+/7Y3DNW3BjI6O9sZD9MHqp/NiH8hhDNpjJxDR3xIlSnid2+j2t27dWlkwIICNAekhhrEEh1XQwm/JIMM4QhGZ1W2jEjh6/qBO5yQQf4k+LBBH2D66f4qySupz7NeN2a+aAmGk4ye09FgGx7+90huHa3F7PQIMw0Th1XRml42mbiAeFriZXbPitUD8uFqUXD2btZzI5qjDKi28qgYbxjVfoeo3ludUxrqx+20FojF/alK69K41U7rdM1nQJx2C6fd//ud/qnuN+x1ow7MbKFgJRDjYhQgcMMCT65tvPENR//hHkVWrPNc+/9xz7a23PE5scHbsmEidOh4RifmGLVuKlCiBYdwibtPlRN2B+o4ffMAKTpxuNcBJFcrasGHDrRbF/CRAAiRAAiRAAiQQMgIRIxBhOYMFDi9k8PgZFRUV0NmEFoi+Xkzxaz+seDq4FYg6PfZ5McQUQ+KqVq2qhrViaKveWrRooVhs27bN2ERlxcQwVLtgJfwgpCC+MD8QwSpdbgwx3R1zUrUFohXDUI0bvIUOb7rI20UtEBcP3OGNwwHmWO766aRcOGEW0aZEFid7Fp9Sy3TAwvnjJ5sl9leP91mL5Co6GIH4wzurBJbQQ2vNLj+D6Td+HMBzbrVlZprnjuq2WwnEI0c8IhBT9Jo29Ry3bSsC6x4cdkI8aoEIYzwc0mDOIeYi4qOlddI773iEJQSi23Q5Ubfur3GPOcf4Dvnkk0+M0dk6Bt/q1atLs2bNspWfmUiABEiABEggGAIpaTdl+NIUte2L8/ddEExZTFuwCUSMQMRthPfATz/9VO655x71kgdHLJ07dzY5irASiJizF44CEdZFzCmENTLQhmGGxvDSSy+puZfGON9jK+GH4Z0QiJgfiGCVLjcEohZbHStMUBZJzJU0bt+/njWE1kog+vY72HOIt2/++pN3PUesc4hhslZBt9lqiKnOt2bUPsUZFlPfoMtw028IQwy3ttqCFYgQghCB2CD+fhs5rZqIxwzxeohpjx5ZaeHl1LiqTKNGWUNM3abLibp92eI8Pj5efXdgjdBQBAxZxXDys2cDzzsNRR0sgwRIgARIgARAIDH5plrODkvaxewM7BCPpEgABCJKIOpbjhffVatWSdOmTdXL3seYEPVbCFYgfvjhhzqr2kNwwcIQyPlEXlgQn376aSX4YPF0E1q1aqXab+fK30r4Lf3aM4fu7EHP/D6dLvG8eQ1DzO3DfMFAIVROanYvOqVElK9VMFCdOSUQdV0Y9rnsm13KiQ/mbQZa0xFptbizE4jwrIp5h5i3GSgE0+9QDzFFe/7jP0QqVfJYDI3tGzbMIwj17xFY0gKC0VdnZWR4yoBIRHCbDmlDXbenBea/W7duVZ+PwYMHmy9k8wzrmmKOcN++fbNZArORAAmQAAmQgDsCFIjuODGVj0CM3pAq/ecly7SNqRHBJj09XS3nYFxk3q1ABKBy5crJM88842UFpzcYipqfBCIW+EZ73HhORUcw1xHpMdfKKmjhZ5xbiCG8EHcfV58qGemeoYnLBnsE494lWS4w4VgFcwNzWiDCoQvqceMRNacFouYY03+7Eq1WAtBJIMLJTo+qUfJpjWmSdDXwZzSYfm/ZskWWL19uuQXrpAb9hIOaIkVE9uzRvRZJTRWpVUvkiSey4uAnCvMM/+d/suJwNG2aRzjO+M2Rrdt0yBvqus0t85xNmDBBfT5WrAi8/ibm7v7tb38TDO02ej0OVJaOw9zk++67T/B9xEACJEACJEACOUWAAjGnyBa8ck0WxILXvawe4Zf//v37y8KFC2Xz5s1qPiCWcoAY+lxPjBKRYAQixGCxYsWkV69esmTJEnnuuefUfL/cEIjz5s0TrGmIrUKFCsqBjj7fv9/j1AW9j42NVRYKpHn//fdVOzHvEPmxFqRvOHbsmGKCddqsghaIEDxYAxECcMQri71eO3U+rNOHIaf968yRHXOPKysa5v9p76Q6nXEfKgsiyoQ3T9SPtRWXD9ml1l7EsE84vUk4kuWEJycE4qIBO2TDxINycHWc8oqKOuGAB0uBGMUd2oHlNbBNab1WtXdhv23q3OiRFf0Z+rcYdX3Ox5sF8z2NG/qgg9t+6/Ru9kOHisDIhe3ee0WKFs06nzMnq4QTJ0TKlvUMEZ06FfNuRWANRHrjkFPkgL8oWBFhhIfeglOb0qVF/vxnEaNWcpsuJ+rO6pnnqEmTJurzZiXmtIDEd4DdZ8hYLj6PSI/PLwMJkAAJkAAJ5BQBCsScIlvwyo0Ygbh48WJlLcSLmN6wUHXPnj1Nv9wHIxD37dsnjz76qLc8zN+D1QDl5/QQUyyVofvhu//iiy9MT+ru3bvl2WefVXMRdVrMS4Q31UABacEGayIGChjG+OWz85SFDgIMS1N8/uc5Sgz5psdyD1gHEOkgjjZOPqSEW05bENEOWDIhtvSSGGgDNsxFxFBNHXJCIA5uuEAJZl0nlrwAs5PbzWtPrhy+R7XJm86wziGGkhoDygiUDnEQwDq47bdO72b/wAMeMQdB57v97/+aS9i9WwSeS2FJxLIXjz0mMs+zUokpIXzgYHQ35iuizHLlRP7v/0QMq86o9G7TIXGo6zY2GJ6Q8bnpaFzHw5hARLC+IdZPRToMKXcbMBKhfv36bpMzHQmQAAmQAAkETYACMWhkEZshYgQi7jCGQcIZxOHDhwUu/HEeihAXFycJCVmCIxRl5kQZsHrAQoj2pmLcn0WAhbVIkSLSvn17ixSeaPDDfDo9pNQqcVpyuiQcvSoZaYG9YlrlC2U8hmeiDcZhsaEsP1BZKdfT1JqFmJNptBoGSptTcXnRb90XrKBy0cUSkvgYxsaKWDhN1cWpZS/cpEOGUNeNMjFnGT+c+Ho49jbwtwOsq3r+vPmHAN80vufwqowfb/BjDgMJkAAJkAAJ5AQBCsScoFowy4wogVgwb2HO9Kpt27ZStGhRiYmJyZkKWCoJhBGB0aNHKwE3bty4HGk1frCBx2GndVpzpHIWSgIkQAIkEBEEKBAj4jaHpJMUiCHBWPAKwQvr888/L2XLlpU9Ro8jBa+rrnqEdQ2nd9rguGHuIUPBIrBs2TI117h79+4Fq2PsDQmQAAmQQEQRoECMqNt9S52lQLwlfAU7M4bKYdkLeGaM9LB+/AH5st48x21ss8DeLSOdXzj3H3MPu3btGrIh6eHMgm0nARIgARIIXwIUiOF773K75SaBmJ5xU1LTbwr2DCRAAiRAAiRAAiRAAiRAAgWDQEbmTdl3OkNtl6/zXb9g3NWc6YVJIHaJSpLnPk+U7tFJOVMbSyUBEiABEiABEiABEiABEiABEsi3BCgQ8+2tYcNIgARIgARIgARIgARIgARIIHcJUCDmLm/WRgIkQAIkQAIkQAIkQAIkQAL5lgAFYh7eGjh/CdVajHnYDVX1jh07ZO3atXnWDKwz6IZlemqGpN5Iz7N2hqri1KTw70OoWNiVk9fPpV3b8uoanE+5Xa5j7ty5cuLEibxqKuslARIgARIgARLIAwIUiLkM/cyZM/Lmm29KuXLl1Lpqt912m3zwwQfZbkVaWpokJydnO7/OmJKSImhL586ddZTr/dGjR6Vy5cryyiuvmPKkpWTIB7eNkRmdN5jiQ3Vy5cwNGfvmCmlfbrz8s9AoVVfUB+sCFr956mH57NFp0qrIKJW2wx0/SMLR8PTOum3WUWlVdLSM/t/lAfual5GnTl2R227rY9o2bozNkyZZPZfGxoTq82Ms0+1xXtU9ZcoU9d0zdOhQ26ZmZGTIo48+KjVq1JCEhATbtG4u/vjjj1K8eHHZvn27m+RMQwIkQAIkQAIkkEcEKBBzGXz9+vWlZMmSShTOnz9fJk6cKPPmzctWK5YuXSplypSRlStXZiu/MRMEYqFChaR9+/bGaMfja9euycMPPyxPPPGEXLlyxZQeAhHCbVr7n03xoToZVH++tCk5ViAKd84/IRsmHpSd8477Fb9/xWnVjn5PzZZVI/aoNCuG7pbEC7curP0qy4WIn/psU/359xMzcqG24Kq4cSNNpk/fo7ZWrRZIoUK9ZMOGU8EVEoLUds+lLj6Unx9dptt9XtaNNrZp00YKFy4sCxcutG3ygQMH5M4775RnnnlGMjMzbdM6XUxPT5f77rtPWrZs6ZSU10mABEiABEiABPKQAAViLsKHRQMiDBbEUIRp06ap8vJSIHbp0kUtIr53716/LuWkQIT1D+ITFkSnMO6tlSrt2UNmAeuUL79eT0tOV2I44Uj+toCOHbstzwSi3XOp72soPz+6TLf7vKwbbYR1sFatWlKlShW5fv26bbNnzpypvmeGDRtmm87Nxb59+0qpUqXk4sWLbpIzDQmQAAmQQAgJYCm7GZtS1XYsISOEJbOogkYgYgTiqVOnZNu2ber+YUjmunXrZMGCBXL58mXTPcW1jRs3yqVLl1R8fHy8zJo1S7Zs2WJKZzyJjY2VOXPmKEueb3nGdLAaQCBOmjTJGB3w2K5MtAVtxMsWyhs+fLg6Rxy2c+fOBSwTFj7ME0RbN2/eLPhFXwdtQezQoYOKwvWpU6faDgeDdaFYsWLSsWNHXYxp7xWIHTwWxAsnrsm22cfk1M4LpvmCmDt4dONZOX/8mil/cmKair+W4L/syt6lsUr0bZx00JQn0MmAunPlkwenBrpkikN70Y6tM49K3F7P/TclEJFzh6/I6V2el9vMjEw58nO8sl7eqjUSfT28Pl42TTkkW6YfUe3ISDNbbCCK0T69ndkXuI2ndpz3ptFp9R599A1u+u2bx825W4EYG3tV5szZJytXHpPLl81WXTwbGKJ67pxHxGzaFCtRUbtk69Y4yyY4PZfZ+fzg84HPFsRSoB9DdGMwv2/9+vWCYZzTp09XeTCUVIfs1K3zWu3xwxPahu8q37B161Z1DYLQN6xevVp9f3z22We+l/zOGzRooIbF232/+WUKEHH27Fk1zPTLL78McJVRJEACJEACOUkgMfmmWs4OS9rF7Mz635STdbLs8CQQMQKxR48eUr58eYHTCsyXg7DCdtddd5nEH14uEY8Xwe7du3vTIa5Vq1amu4yXxnfffVelKVu2rBqyhbmFeDkMFCBIUQ4Ep1VwU2bFihVN7UKZxm3UqFGm4lEm+l+0aFFTun/84x/edEiDMtq2bSuvvvqqKV2nTp286YwH3bp1U32GmA0UtECc1W2jGgYKi5/efmixSjIzPYu06nS+Q1GPbTqn0q8ff8Cv+F8XnFDXMB/PKfSrPVt61bQfjnl4Xbx0rTxJzVFsV2acKnto44WSeN4sWDDvD2VBnH1cLcrbn86VJkrsrxecmhLwOkRzp7smqLIwZFYz8hWpE/6xynsNaYa8EHh4IMSwLsN3f/6Y2erott8BG+4Q6SQQU1LS5d135ykrY9my/aVw4V5Srlx/mTLlV2/JSINhqgMHrpemTaeqY5xj+/DDwP13ei6D/fzgxyR8ZxQpUkQN6cbnpHHjxnL+/HlvO3Ewe/Zs9X2C6xhGrj+TRkEZbN2mCixO8J1WokQJ+f3vfy+pqaneVPieQRt8v7e8CUSkTp068sADD5h+sDFe18cxMTGqrDFjxuiobO+bNWsm1atXv+Uhq9luADOSAAmQQIQSoECM0BufjW6bBOLI5SnSYfIN+X5FSjaKyt9ZIJAw5wYi8V//+pdcuHBB1qxZI3fccYfUrl3b23gtEB988EF55JFH5OeffxYIoEaNGqkXpN27d3vTtmvXTg2X0vN4MGyqSZMmyqq2f/9+lQ7WBAzjwlapUiVVBl4SdRz2eLHUwU2ZaM/JkyeV5RAvgNHR0eoccdhgxTAGzDdCOoi/I0eOCOZnwZIBD4U6aIEIJxJPPvmk7Nq1S44dOyaYMwluhw4d0km9ezCqW7eu99z3QAs/CK6ej01X1jZYAye0XK0EDEQegk7nJBBhYetRJUptEGQQP50rTvTG4RrEFsJ3ry31xn9QfIxyYKPzYv9lvax5n5dOX5cPS42V4S8vlhuXU9TLMtoGsTauuXkIKwQiHNx0rDBB5vTYLNcvpag6/1l4lHz/+lJVdzB/kq6kStvS45TYu3rOYymF5RBWSlgVjQFtuxx3XW3d7plsKRCR5uLJRO+GOZhtSowRzNmE1VMHt/2GEFq0aJHlZvxM6LKxdxKI7drFSKlS/WThQs+zdfFikjRpEiXFivWW/fs94ksLRKT7059Gyb59CXL48EWpX/8HJSgPHvQX5U7PZTCfn9OnT6vP+Msvv6xGG8CiiR96IACbN2/u7S6s86VLl5YXXnjBa8GH5RCfI+PnMZi6vYW7OBgxYoT6jONHLQTcs7vvvlt9lu2cWMGSh+8GiGC7gL7gu7Jhw4Z2yVxdg7UTdWIONgMJkAAJkEDuEaBAzD3W4V6TSSCGe2fs2g+BiJeSQYMGmZLB2oB4LYC0QKxWrZpgOJQOUVFRKh2GXSJgOBe8fmKukzEgHuV17dpVRUOwffXVV2qDcwZce/vtt71xuKYtDG7L1PW5mce0b98+Zfn4+9//rrMF3GuBCK+FxvlBEyZMUG3GsFRjQBr0xcq6iLRa+H1cfaoYh4lC6MC7qZ4/qNM5CUSInqVf/ao2LTLHv73SG4dr2ur2S/Rhb/xH909R1kGdF/t1YzwCHu2MbrdeCSjfYaJTWq8VCL/ka1lWGQhECNPFA3cYccinj0TLpzWmmeLcnGAeIcpDG4IJ6JOVBdFYDpbD6F1rpkBQXom/Ybzkut+LFy9W9xr3O9BmtEQbK7ATiPHxicrTaZcuS4xZBPGwDnbt6onXAvGxx4aZhp9OmLBDpcPQVGNw81zq9G4+P/jBBtY5/KBkDK1bt1Y/nODHFgT88AI2SO8muKnbTTnGNK+99pr6rONHrTfeeEMwquHgQfsh2PgBC+0eOXKksaiAx88//7wSnQEvBhn51FNPKTEdZDYmJwESIAESuAUCFIi3AC/CskacQNRzC/V9huDDCxLmByJogThw4ECdRO0x9+ann37yrgmmh1xBYGIol3GDJaFp06am/DhxGmIabJluXjLHjx+v+rdkiflF3LdxWiD6ejHVv/ajHGOA1QjcIHCtgpXwQ3qIKcwNRLBKl1tDTPvXmSM9qkYJhqsaNwyDhXg7uS1rKCEEIqyNvuHY5nOCeZHBBlikMqb6VgAAIABJREFUYM1EPRB8m6MOS8p1s+UwUJluBeIP76yS1sVGy6G1Z/yKcdtvzFXF58Zqu3HDLDx1RXYCMSbmkBJ43botlVmz9pq20qX7qeGkKEcLxPbtY3Sxao95iRCS48ebl0xw81zqgtx8fjAEs2rVqqbPNz7rLVq0UM+/nteM+1ivXj0VBysiflCyc/7ipm7dTrd7WDExdLNChQqqHfrHLLv8x48fV2k/+eQTu2Tq2jvvvKNEMb4rbjXg+wQjE5wE7K3Ww/wkQAIkQAJZBCgQs1jwyJ5AxAtEWMYgdCDeEKwEoi/GsWPHqnx4GcP8JN/t9ddf983iKBCDLdPNSyYcUKB/sCTaBSuBuGnTJpXfVyBiiCrKHTJkiGWxVsIPGXr/fqZgbiCCVbrcEogQW7BoYg5ioA0OXnSwEoj6enb2EIRzP/1FWfkgFNvePk6tHQlvpVbBjUBcM2qfEp6wmAYKbvuNeW1wfGS1aSuabx12AlFfq1BhgFSuPMhve/316ao4K4EIZzWBBKKb51K3083n5/7771cjBXw/3/ocP6DoAEH46aefyj333KM+G7fffrtaVzTQEE83detyg9n36dNH1f3QQw8pT6VOefWIB/zQ5RQwlxGfeasfBJzyG6+DCeZ/a6dYxms8JgESIAESyBkCFIg5w7UglhrxAhECBy89vkNMfS2Ivjcfc7KQzymdMZ+TBTHYMvVL5vLl1gumw2EN2pldC6KVQNQvlnoorbGf+thK+OF6u7LjZcz/edqt00390DwPCsIMgimnndR88fSPal4h2uEUckIg6jrhtOfAqjgZ3nSR6vecjzfrS357J4F4fEuCGjaLuZhWwW2/c2KI6aJFh5XAg/MZuxCsQHTzXOr63Hx+nn76aTX3LhirGdYLXLVqlRpFgM/exx9/rKv07t3U7U3s8gDOajA38g9/+IP6zGNYvVOAl1O0cfDgwU5J1fxqOOEKVfjoo4/UnHA7S2uo6mI5JEACJEACIhSIfArcEohogYhhYXBQA0cx2g28WwtiXFycsizUrFnTLWtHC2KwZWL9Q7zc9ezZ07IN27dvV2ngmdQuBGtBBDvMccK8JKughZ/v3EIsZq+E37iseYDty42XAc/86C0KjlqGNFqYKwIxqs06VY8bj6g5KRB15zPSM5UV0W6OoZ1AvH4xWQ2ZxTDepKtZ8yd1+Xrvtt+Y14cfIaw2K+u0thJu2HBKV+ndx8VdU3MQa9Yc4Y0LdBCsQHTzXOp63Hx+tIMnDCsNNmBoLqyIGHLqG9zU7ZvH7hxD4OGcBxZPDAXW3pV95w77lqHnGK9YYXbG5JsO55iX/eyzzwa6pOLgvfmxxx4TN8tmIAOGt8Iz7HfffWdZJi+QAAmQAAmEjgAFYuhYFvSSIk4gfvHFF8pjH+b7weMoBJbRm55bgYgHAw4pkB+ePGGJhBjDOoNwBQ+nFb7ByYIYbJl4KYRnQXhHxZqIeOmcMWOG7Ny501Q1vC2inS+++KJa2xAWoaFDh8rXX3/tTResQERGzEnC0hm+7v51oVogfv7nObJ9zjG1Nt/K4XuUB1DMf9PLXCA9xCDmys3vtVX2LomVr59boEROblgQL8UmqnmF8Ew66f01qn7MO4SQxbqExhBqgXhia4LE9N8uuxaeFMxjhPfUSe+tUYJ14efm+XVoD9ZoxIa2wvmMPo/fn7We59C/xaj8sEBunnrYtMUfyEoXTL+NDOyOjxy5KDNn7lVb69ZY1qWX9Ou3Rp3Pnm0e5gwvprhet+5YGTJko2zffkbWrj0hY8ZsE5SDEKxARB6n51K3383nB15HsbA7hpK///77yhKPeYfz5s0zLWcDS1z//v0FHo2xhig+6++995763H3++ee6Su/eTd3exA4HEMXwsoo5fXo0AaxyDz/8sPoRB99pVgHfgeibcU3UQGndWBohIPE9g813rnegMhGHudrB/MhmVQ7jSYAESIAEnAlQIDozYgoPgYgTiFjGAS8w8ED6pz/9SQ0FMz4MwQhEvFT169fPu3yFfjnC/CTMhfINbgRisGXCuU6NGjW8L2b4RX7y5MmmqjGHDO289957venKlCmjXmh1wuwIRFgd0GfMewoUsKQCvI12u3eyEiwQe/CmCYti6g3z/DqsK/jZo9O86Ya9tEggZnJDIKLtp3dflC+fnafmIqJObJiXCC+pxhBqgbhn8SllLdR1Yo81Eef13CKwJBoDlvQwpjMeL/oiy6sqPK8arxmPlw/ZZSzSdb9NmWxOhg/frEQfhJ/vVqJEX1PO9PRMJR4rVcJSC1npMSdxy5Y4lTY7AtHpuTQ2ws3nB45vYDnDd4b+jOMY3oh1wI8usBbq69hjjh2s+1biy03duny7PX70Qn0dO3Y0JYNQLVasmPzud78zLbWhE+FHLPTDN5++btzDAzOcb2GUg1XQ1lb8YAbR6iZg6DvajiG5DCRAAiRAAjlLID3jpqzal662+Mvmd4ycrZmlhxsBk0D89+wkefWbROnzo3lx8HDrVKD26mUuYO2Csw2s6xXKgGF4R48edf3LuZu6gykTjkKwZlsghxjGupAuISEhZItUv/TSS+rFEZYWuwCHK1jewilgDT/jkhhO6UN9HaIMi8mjHempznMSQ1E/Xqavnr0h5w5fUWsXun25DkXduoy86LeuG3usgXj0KDyletaCNF7LzrHb51KX7ebzA6GHtUEhkowL0usycN+wNM7hw4fVeqRu76ObunUdodzDegcR62Ttw49dsE727t3bsXp8D+jh+o6JRZSQxA9cWKKDgQRIgARIgARIIH8QMAnELlFJ8tznidI9OjQvafmji55WaIHo9DKUn9ocDm05ceKEwBrZoEGDgC/N4dAHtrHgEeBzaX9PR48erSx348aNs02IpTMef/xxwfqoSUk583/h22+/dbRO2jaSF0mABEiABEiABEJKgAIxpDgjszB4X8VQNQxDi/SAIaPTO21w3BYNyBoSGunMcqr/4fxcdurUSdxs8FwabFi2bJkaetq9e3fbrLAENmzYUC3hA8spAwmQAAmQAAmQQGQQiBiB+P3336uFrDGciyH0BKKjo5UDnNCXHF4lYkkOLHzvtI1t5uw1Mrx6nj9bG67PZb169dT3ldPejfdR3zuDuYdYnsbNENhevXpJdkSob508JwESIAESIAESCB8CESMQw+eWsKUkQAIkQAIkQAIkQAIkQAIkkDcEKBDzhjtrJQESIAESIAESIAESIAESIIF8R4ACMd/dEjaIBEiABEiABEiABEiABEJLAMtcrN6XrjYucxFatgWtNArEgnZH2R8SIAESIAESIAESIAES8CGQmHxTrVaAFQtidoZ2uTefqnga5gQoEMP8BrL5JEACJEACJEACJEACJOBEgALRiRCvawImgdh/XrK8OeK6DFyQrK9zHyYE4JHwu+++EyzmnZ9DuLQzNxkmJiaK03p0uj1z584VrPHHQAIkQAIkQAIkQALBEKBADIZWZKc1CcSCjOJf//qXWqvv8uXLIe9mWlqaJCffuqhOSUlRbezcuXPQbezWrZta22zLli2mvKdOXZHbbutj2jZujDWlyc0Tq3bqNoSKpS4v2H1eLIMyZcoUtWj50KFDbZuLdemwYHmNGjUkISHBNq2biz/++KMUL15ctm/f7iY505AACZAACZAACYQxAQrEML55udz0iBGIPXr0UC/hly5dCinipUuXSpkyZWTlypW3XC4EYqFChaR9+/ZBlTVy5EiVb8yYMX75btxIk+nT96itVasFUqhQL9mw4ZRfutyIsGsn6g8ly+z054UXXpC//OUv2cl6y3natGkjhQsXloULF9qWdeDAAbnzzjvlmWeekczMTNu0Thdhbb7vvvukZcuWTkl5nQRIgARIgARIIMwJUCCG+Q3MxeZTIN4i7GnTpilxllcC8cyZM1K2bFl58803HXsyduy2PBOIbtoZSpaOMAIkeOyxx/JMIMI6WKtWLalSpYpcv349QOuyombOnKmeuWHDhmVFZvOob9++UqpUKbl48WI2S2A2EiABEiABEiCBcCBAgRgOdyl/tDHiBCKGmGI46Lp162TBggUSaMgpLCtbt24VCJapU6fKpk2bBHPnjAFDOTdu3Ch4wYbVb/jw4eoccdjOnTtnTO49vnLliqxdu1bmzJkjmzdvNs0Z1BbEDh06qPS4jvrthgC+9dZbcvvtt8vp06e9dVgduBWIsbFXZc6cfbJy5TG5fNl/6CyGrW7bdkZVc+lSksycuVfmzz8gFy8mWVUtdu3MDkuwAmeIpb1791rWiwuHDh2S+fPny6RJk5SFLjY2a4jt2bNnvfetatWq8oc//MF7jvK3bdtmW3agi0ePHlVlxMfH+13Gc4VyIQh9w+rVq9Wz9Nlnn/le8jtv0KCBlCtXLuDz65fYJgL9xzDTL7/80iYVL5EACZAACZAACYQ7AQrEcL+Dudf+iBOIGzZskMqVK6sXcQi7u+66S4zz9iAEy5cv772ONNggHM6fP++9MxUrVvRLo9NiP2rUKG9aHEDQYJhr0aJFTfn+8Y9/eNNpgdi2bVt59dVXTek6derkTacPIG5vu+02ee+993SU7d5JIKakpMu7785TVsayZftL4cK9pFy5/jJlyq+mcnv0WCZ33TVQFiw4KOXLf6HSY+jq3Xd/KXv3+s+Nc2pnsCwh7nEPixQpoob3gnfjxo1N9wcNhiWuSZMmiiO4gxXSvvHGG97+QNgb75vv8T333ONN6/Zgx44dUqJECfn9738vqamp3myzZs1SdbVq1cob53tQp04deeCBB/x+kPBNFxMTo8oKNKzYN63TebNmzaR69eq3PGTVqR5eJwESIAESIAESyDsCFIh5xz7cao44gYjhmHBYc+HCBVmzZo3ccccdUrt2be99g0Ds16+f7Ny5U27cuCFxcXHSq1cv9TL+wQcfeNPBCnXy5EllOYSoiI6OVueIwwbPlMaAOWZIB/F35MgRgTMUCFN4pdRBC0RYdJ588knZtWuXHDt2TOrXr6/mp8ESZgywiKFMzN1zE5wEYrt2MVKqVD9ZuNBTDyyCTZpESbFivWX//ixxDIFYtGhvlfabbzbKtWspMmHCDiUU33xzll9TnNoZDEtYSjEk8uWXX1bWM9wvWIJLliwpzZs3N9WN+4x5ffPmzfOKH1h29+3b502H+6Tv2UMPPSQQaPocezeWWW9hhoMRI0aoe9O9e3cVix8X7r77bnVf7RwawZKHewoRbBfgzAfPbsOGDe2SuboGiybqhJWVgQRIgARIgARIoGASoEAsmPc1J3oVcQJx0KBBJo7wqomXY1/xZUwEZyBw5gHx4BvczJuDIIG16+9//7tvdtO5FojwVGmcEzZhwgTVRgxLNYZ27dqp+KtXrxqjLY/tBGJ8fKLydNqlyxJTfsTDOti1a1Y8BCLivv/e7DG1WrVvpFatkab8OHHbTjcsURascxD4xtC6dWslBo1eSCEYYTXEMEo3IdRzEF977TV133/++WdltcSPEwcPHrRtyvr169U9hUMfp/D8888r0emUzs31p556SuCkh4EESIAESIAESKBgEkhNvykzNqWq7ViC/1SXgtlr9io7BEwC8ZtFydJq3A35dklKdsrK13msvJhijl8gK9yePXvU/Da8qGMYYrVq1aRmzZp+fXQjasaPH6/qWLIkS2T5FfTbMFS0xdeLqbbwoBxjgACBB1W3wU4gxsQcUqKvW7elMmvWXtNWunQ/adp0qrcaLRAx/9AYGjWaLFWqDDZGqWO37XTDEiIdcwUxXNO4tWjRQjE2zhlcvHixEmgYwvrJJ5/Ir7+ah8r6NjTUAhHzTTF0s0KFCqpteNacwvHjx1VatNcpvPPOO0oU44eFWw14tmBtdRKwt1oP85MACZAACZAACZAACeRvAiaB2CUqSZ77PFG6R5tf/PN3F9y1zkogzp49W72QY5giAuatNWrUSMXdf//98sc//lFgXYEjmOwKRDgdgfAzDm0M1GptQfQViHCSg/y+AvGll15SwwwDlRUozk4g6msVKgyQypUH+W2vvz7dW6SVQGzcOLBAdNtONwIR9wRWQcxBDLRBTBsDHAL99a9/VUIRDHE/YaULFEItEFFHnz591L3D8NVAjml82wHHNmgnLNtOAXMZkRZDoW81YNgr5uNqB0m3Wh7zkwAJkAAJkAAJkAAJhCeBiBeIX3/9tXrJ1pYTzBnDSzesU8bw9NNP2wrE5cuXG5ObjuGwBmVm14JoJRC1QHBaFkE3RovAQOsgLlp0WFkQBw4MLJ50GdgHKxDdtlMLRDuWuA+Yexes1QzC65tvvlEiCEM9fYeool8QiPXq1TN29ZaO4awGcyPh4Aj3Hz9SOAV4OUXawYP9LbG+eeGAB55MQxU++ugj5aDJ7fMUqnpZDgmQAAmQAAmQAAmQQP4hENECEQ5O4KAGwwCxtAUC5nXBWmgMcFYCy1UgCyLWP8QLfc+ePY1ZTMdYpgJp4JnULgRrQfzqq69UuVYWMd+67ARiXNw1NQexZs0Rvtn8zoMViG7b6YaldvbjK+D9GmkR0b9/f8UM3mx9w1/+8hcpXbq0n4Mh33RuzuG59cEHH1TPzaVLl+Tdd99V9frOI/UtS883XbFihe8lv3MMe3722Wf94nXElClTlOh1s2wG8mB4K+bKfvfdd7oI7kmABEiABEiABEiABCKMQMQJRAgEeIiENe+VV17xernU9/3DDz9UL/JIh+GKAwcOVBYrCIdAAhFCABatSpUqqTURIXJmzJihvKDqMrGHwxSIxBdffFGtbYj5cUOHDhVYMHUIViDCwynKtBsWeOTIRbVOIdYqbN16gbIS9uu3RsXNnp3lzRNtgBdTOJ+pW3esDBmyUbZvPyNr156QMWO2CcrRIViB6KadKNsNS3g8hRdTzOt7//331X3EvEN4KoUgMoYBAwbIxIkTBesLwlEMloSA0MdQykCOfbS32qZNm6p7CO+wP/zwg7FIV8f44QFeVjGnT1tDYZV7+OGHBdbLAwcOWJYDqyD6pn+wsEroxtIIAYnnAxtEqpuAvgd6zt3kZRoSIAESIAESIAESIIHwJxAxAnHRokXK2qLXwsNSEn/+85+VeDDeRiyDgEXI9Ys1vJdOnjxZ4ATF6sUZQqJGjRrePLDCII8xYD08LJ9x7733etPBwQyEqA7BCkTkgwUJggfiKlAYPnyzEn0Qfr5biRJ9TVnS0zMF4rFSJSy1kJUecxK3bInzpg1WICKjUzt14W5Y7t6923Qvca9wX99++21djNpjCQiINH0vcYx2wKIbKGAeHix9eDZ0nkceeSRQUtu4L77A2pCFpGPHjqZ0mzdvlmLFisnvfve7gFZKLH+CfvjmMxXy20nLli2VtRPLsFgFbW2tW7eu47qKugz8cIK2r1q1SkdxTwIkQAIkQAIkQAIkEEEEIkYg6nsK6w7mnzlZaOCBEvPWgglYYgHr5tmtc4fykC4hIcG7Nl8wdfimheiAIPV1bOObLthzrIF49Ogl8fVUGmw5On2w7XTDEvcQ1kmIJOOC9LpO7GG5wxBhzDENZDU0ptXHWNbkzJkzgrULczPAegex72Ttw/qZELu9e/d2bB4srm6c4+iC8PnAjx3wPMtAAiRAAiRAAiRQcAhkZN6Ufacz1Hb5+s2C0zH2JOQEIk4ghpxgPiiwbdu2UrRoUYmJickHrbFuQri007oHOXdl9OjRynI3btw420rww8Xjjz8uWCszKSlnvA1/++23jtZJ20byIgmQAAmQAAmQQL4jkJh8U61WgBULYnam5bv2sUH5hwAFYv65F9luCaxncK6D+W1YvzG/hnBpZyB+mDPaqVMnxw3zHoMNy5YtU0NP4UHXLsASiGGzWN4DllMGEiABEiABEiABEnBLgALRLSmmMwnEj6KTpGH/RPl4es5YJog75wgkJiYKlpNwO4wy51piX3K4tNO3F1iDEktgOG3NmjXzzep4jrmHXbt2dTVPEI50sHwGAwmQAAmQAAmQAAkEQ4ACMRhakZ3WJBAjGwV7TwIkQAIkQAIkQAIkQAIFkwAFYsG8rznRKwrEnKDKMkmABEiABEiABEiABEggHxGgQMxHNyOfN4UCMZ/fIDaPBEiABEiABEiABEiABG6VAAXirRKMnPwUiJFzr9lTEiABEiABEiABEiCBCCVAgRihNz4b3aZAzAY0ZiEBEiABEiABEiABEiCBcCJAgRhOdytv20qBmLf886R2eMFcu3ZtwLrnzp0rJ06cCHiNkSRAAiRAAiRAAiRAAuFJgAIxPO9bXrSaAjEvqIewzrS0NElOTnZd4tGjR9U6eq+88opfHqyzhwXYa9SoIQkJCX7XGUECJEACJEACJEACJBCeBFLSbsqwpSlq23c6Izw7wVbnCgGTQJywNlU+nZkkE9el5krlrOTWCCxdulTKlCkjK1eudFXQtWvX5OGHH5YnnnhCrly5EjDPgQMH5M4775RnnnlGMjMzA6ZhJAmQAAmQAAmQAAmQAAmQQMEkYBKIXaKS5LnPE6V7dFLB7G0B69W0adOkUKFCrgVily5dpFixYrJ3715bEjNnzlTlDhs2zDYdL5IACZAACZAACZAACZAACRQsAhEjEDEMc+PGjXLp0iV1B+Pj42XWrFmyZcuWgHc0JSVFpYdYCiSobt68qa6fO3dO5T9+/LjMnj1bdu7cKbgWKDiVqfOgbYsWLZJJkybJwoUL5ezZs/qS2qPN6Evfvn2VkBs+fLg6Rxw23SZjJlgGIQ47duxojLY8btCggZQrV04uX75smYYXSIAESIAESIAESIAESIAEChaBiBGIEEiwtkHwde/eXR3jHFurVq1Md3XdunVqnl6RIkXUEE6kady4sZw/f96bDmIP8V999ZW0adPGVF6LFi38hme6KROFf/31196yChcurI5LliwpRmtexYoVvWl0H4z7UaNGedupD7p16yYoLzY2VkfZ7mNiYlQdY8aMsU3HiyRAAiRAAiRAAiRAAiRAAgWHQMQJxAcffFAeeeQR+fnnn5VYatSokRJCu3fvVnf19OnTUqpUKXn55ZeV9QzWwAULFghEWvPmzb13XgvE8uXLy2OPPabKg2OXli1bqvKQRwe3ZSI9hBksm2fOnBE4oFm1apU8/vjjUrRoUYmLi1NFQuSdPHlSYDmEMIyOjlbniMOWmJioq/bu0e+6det6z50OUPcdd9whDRs2dErK6yRAAiRAAiRAAiRAAiRAAgWEQMQJxGrVqpmGbEZFRSmRNXXqVHVL27VrJyVKlJALFy6YbnHr1q2VBQ6OXhC0QKxevbrJ4yeGZN52223y5ptvevO7LdObwecAFkEIQYhHY3A7B/HixYsqf6dOnYzZHY+ff/55ufvuux3TMQEJkAAJkAAJkAAJkAAJkEDBIBBxAnHgwIGmOwdB99NPP3nX/qtTp45UrVpVWfFgydMbho1CpG3btk3l1wKxffv2pvJwgmUijNY6t2Xqgq5fvy7wUDp+/HhlJdRDWNEWY3ArEGEdRdsxHDaY8M477yhRjL4ykAAJkAAJkAAJkAAJhC8BjIq7kJipNix5wUACVgQiXiD6grn//vuVBbBy5cpqHqLvHk5gEOwEYq1ataR27dreot2WiQwzZsxQy0yULl1aLUfx1FNPqaUpIPCyKxDh1Ab5hwwZ4m2TmwPMzUS+GzduuEnONCRAAiRAAiRAAiRAAvmUQGLyTbVaAVYsiNmZlk9byWblBwIUiD534emnn1Zz75ysZnYCEesIGoeYui0T1kzMdfyv//ov0zqF8GhqJxCXL1/u0wvzKbyiIn/Xrl3NFxzOmjRpojyZOiTjZRIgARIgARIgARIggXxOgAIxn9+gfNQ8CkSfm2E1nNMnmaUFcf78+UqMGb1/ui1z/fr1Ku+gQYNM1f373/8OKBBXrlyp4nv27GlK73uCIQVly5YVzCkMJmC+5rPPPhtMFqYlARIgARIgARIgARLIhwQoEPPhTcmnTTIJxKtJN+ViYqZcSyp445L1Mhe+cxB97ws8hMKLaYUKFeT999+XJUuWqHmH8+bNkylTpniTawsi5hf++OOPsmHDBvn2228FXk0Rl5mZ6U3rtkx4LoWDm5o1ayrvpXPnzpUXX3xRMNw0kAURFkd4Gq1UqZJaExGCEUNUsRajb8B8QnhCNS7V4ZvGeL5161ZV5+DBg43RPCYBEiABEiABEiABEghDAhSIYXjT8qjJJoGYR23IlWrdCkQ0Bk5dYDmDWIMww4bjt99+29tWLRCLFy/uTQOPnx06dAi4zISbMlH42LFjlbUPdaLsV199VTZt2qTq8J2DiPRwZgOnOLqdWLtx8uTJ3nbqgxUrVqg0ffr00VG2eyzXAWGql9awTcyLJEACJEACJEACJEAC+ZoABWK+vj35qnERIxCzQz09PV2OHTumRFJqaqqpCC0Q4cUUS19cunTJdN3qxK5MnScjI0OtZ+hbp74eaI82YL3F5OTkQJdV3EsvvaREHyyadgFObQoXLiy9e/e2S8ZrJEACJEACJEACJEACYUKAAjFMblQ+aCYFYjZvglEgZrOIXM924sQJKVOmjDRo0ECsxOeVK1fk8ccfl0cffVSSkpJyvY2skARIgARIgARIgARIIPQEKBBDz7SglkiBmM07G44CEV2FR1QMl8UQUt8Ay2XDhg3V8h6wnDKQAAmQAAmQAAmQAAkUDAIUiAXjPuZGLygQs0k5LS1N6tWrF/TagtmsLqTZoqOjZerUqQHL7NWrl+zYsSPgNUaSAAmQAAmQAAmQAAmEJwEKxPC8b3nRagrEvKDOOkmABEiABEiABEiABEggFwkkpd6UnrOS1LbteHou1syqwo2ASSDO3Zom3y5Jkfnb0sKtH2wvCZAACZAACZAACZAACZAACZDALRIwCcQuUUny3OeJ0j2azklukSuzkwAJkAAJkAAJkAAJkAAJkEDYEaBADLtbxgYF6S4uAAAFf0lEQVSTAAmQAAmQAAmQAAmQAAmQQM4QoEDMGa4slQRIgARIgARIgARIgARIgATCjgAFYtjdMjaYBEiABEiABEiABEiABEiABHKGQECB2G1qkqRn3LTdbt68admizJv2eXXZdmXgmk5nt0dddsEur76WkZnzZaAOXZ/V3qkduVUG2mcX3LTDqYz88ozwOfO/06G4v6EoIxTPSCjKCMUzEooycKesvjuM8WBvF4xprY6dynBzf0NRBtpnF9y0w6mMUDwjoSiDz4j/nXZzfyPpOQvFMxKKMnCnrL47jPFO98aY1urYqQw+I+bPjdvvInMunpGAP4GAAhGOapy2LUet3eP+tCPNMT/KX7jD2lvqL0fTXZXxw5oU/179FnPifKarMgYuSLYsAy6BnVjgOhz82IWmXzszfeu763ZFyHtjbji2pdHARNsyPp7ucUTk1KcrN6xfzL5ZlOzYDpR/KD7Dsi1RP6e6KmP9QevnbNlud8/ZnC2plu349WSGq3Z8v9z6OYu/4u456z3H+jnDl7rTPcH1thNuWPYFF94Yet2xnDe+tX/O2vzg/JyhLXYBfXXTn7NXMi2LGbk8xVUZu09ZP2ezf3H3nC3fY/1dtPaAu++i6A3Wz9nBM+6esyGLrZ+zy9fdPSP/mmH/XfTCAOfvInzX2IXmI52fs1cG2z8jnae4+y5KTrP+Lhow391zdvKC9XM2frW75ywk//N2Wj9n/J/n/8S9y/95Jijh9j+vz4/543/e3/PR/7xzV62/i3Lzf97J89btMD10PIlYAiaBGHsxUwb95O4fbkj+WVIgml6AnQQi/1maP6fh9s+SAtFfmFAgmpnYCcRL1939EJEbArEZBaLpy8j1j6IUiPyf5/ADfFj8KHrZ3XcRBaL5+x0/muYXgdiwf6Is+tX6ByvTFxxPIpKASSCCwL7TGQKLi9Nm95AfT8h0zI/ykc4q4MXRqQ24vi/O2npwLemmqzK2HbO2UmHYg5t22H2po48Ltqc5lrN0l/2HdcUe5zLmOaxhueFQumM70N/UdOtf7XeecPeMwOJhFWBRccP1zGXrZ+TUBXfPyJGz1s/I+Wvuytgda13GjRR3zwgsBHbBDY81++3LiNnp/Iw4/VNYvc/dM2LXl81H3JUBC71VgGXQDZMLidbPyOGz7srAj2NWIe6Su2fkoI3FHJ8FN33BZ8sqpKS5K2PjYftnBOvdOrUF3zV2Ycku5zIgmuzCugPunhG7IWb47nbqC64nJls/Z/yfZ75L/J9n5oEz/s8zMwnF/zwMdXXz2eX/PP/38VD8zwN7jLJjIAErAn4C0Soh40mABEiABEiABEiABEiABEiABAo2AQrEgn1/2TsSIAESIAESIAESIAESIAEScE2AAtE1KiYkARIgARIgARIgARIgARIggYJNgAKxYN9f9o4ESIAESIAESIAESIAESIAEXBOgQHSNiglJgARIgARIgARIgARIgARIoGAToEAs2PeXvSMBEiABEiABEiABEiABEiAB1wQKrdx6QriRAZ8BPgN8BvgM8BngM8BngM8AnwE+A3wG+AxQIFIg8wcCPgN8BvgM8BngM8BngM8AnwE+A3wG+AyoZ4BDTF0bW5mQBEiABEiABEiABEiABEiABAo2AQrEgn1/2TsSIAESIAESIAESIAESIAEScE2AAtE1KiYkARIgARIgARIgARIgARIggYJNgAKxYN9f9o4ESIAESIAESIAESIAESIAEXBOgQHSNiglJgARIgARIgARIgARIgARIoGAToEAs2PeXvSMBEiABEiABEiABEiABEiAB1wQoEF2jYkISIAESIAESIAESIAESIAESKNgEKBAL9v1l70iABEiABEiABEiABEiABEjANYH/B2+HtuD2U78jAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "34bf39c7",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952ae31",
   "metadata": {},
   "source": [
    "### 학습 진행하기\n",
    "* 앞서 설정한 Flag 변수가 True이면, 학습 내용을 지속 저장하며 학습을 진행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2669beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = create_model_function(label_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa64a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      " [*] Success to read nn_softmax-468-1\n",
      " [*] Load SUCCESS\n"
     ]
    }
   ],
   "source": [
    "if train_flag :\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(dnn=network)\n",
    "\n",
    "    # create writer for tensorboard\n",
    "    summary_writer = tf.summary.create_file_writer(logdir=logs_dir)\n",
    "    start_time = time()\n",
    "\n",
    "    # restore check-point if it exits\n",
    "    could_load, checkpoint_counter = load(network, checkpoint_dir)    \n",
    "\n",
    "    if could_load:\n",
    "        start_epoch = (int)(checkpoint_counter / training_iterations)        \n",
    "        counter = checkpoint_counter        \n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        start_iteration = 0\n",
    "        counter = 0\n",
    "        print(\" [!] Load failed...\")\n",
    "    \n",
    "    # train phase\n",
    "    with summary_writer.as_default():  # for tensorboard\n",
    "        for epoch in range(start_epoch, training_epochs):\n",
    "            for idx, (train_input, train_label) in enumerate(train_dataset):            \n",
    "                grads = grad(network, train_input, train_label)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "\n",
    "                train_loss = loss_fn(network, train_input, train_label)\n",
    "                train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "                \n",
    "                for test_input, test_label in test_dataset:                \n",
    "                    test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "                tf.summary.scalar(name='train_loss', data=train_loss, step=counter)\n",
    "                tf.summary.scalar(name='train_accuracy', data=train_accuracy, step=counter)\n",
    "                tf.summary.scalar(name='test_accuracy', data=test_accuracy, step=counter)\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                    % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy,\n",
    "                       test_accuracy))\n",
    "                counter += 1                \n",
    "        checkpoint.save(file_prefix=checkpoint_prefix + '-{}'.format(counter))\n",
    "        \n",
    "# test phase      \n",
    "else :\n",
    "    _, _ = load(network, checkpoint_dir)\n",
    "    for test_input, test_label in test_dataset:    \n",
    "        test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "    print(\"test_Accuracy: %.4f\" % (test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a28a8",
   "metadata": {},
   "source": [
    "loss 값이 큰 상태에서 더이상 내려가지 않는 현상을 보입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89364d23",
   "metadata": {},
   "source": [
    "## Relu 함수를 통한 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84092f",
   "metadata": {},
   "source": [
    "### 분석환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85519b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "# 비용함수(cross entropy)를 구하기 위한 내장 유틸리티 함수 \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터셋 \n",
    "from tensorflow.keras.datasets import mnist \n",
    "\n",
    "# 데이터 중간 저장을 위한 모듈 \n",
    "from time import time \n",
    "# 경로 설정을 위한 모듈 \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aeb7b2",
   "metadata": {},
   "source": [
    "### checkpoint 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa582752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 진행되던 모형이 있으면 불러옵니다. \n",
    "def load(model, checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    # 만약 읽어온 체크포인트가 있다면 \n",
    "    if ckpt:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        # 그 안에 있는 모델에 대한 정보를 받아놓고\n",
    "        checkpoint = tf.train.Checkpoint(dnn = model)\n",
    "        # 해당 위치에 있는 정보에 대한 주소를 넘겨줌\n",
    "        checkpoint.restore(save_path=os.path.join(checkpoint_dir, ckpt_name))\n",
    "        counter = int(ckpt_name.split('-')[1])\n",
    "        print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c32137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리가 존재하지 않을때 새로 만드는 함수 \n",
    "def check_folder(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    return dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c641814",
   "metadata": {},
   "source": [
    "### 데이터 호출 및 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "580f241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    # 데이터를 받아 옵니다. \n",
    "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "    \n",
    "    # 데이터에 차원을 넓혀, 채널에 대한 정보를 보여줍니다. \n",
    "    train_data = np.expand_dims(train_data, axis=-1) # -1은 제일 마지막 인덱스를 의미 \n",
    "    test_data = np.expand_dims(test_data, axis=-1)\n",
    "    \n",
    "    # 각각의 데이터를 정규화 하여 0~255 사이의 값을 0~1의 범위로 축소시켜 줍니다. \n",
    "    train_data, test_data = normalize(train_data, test_data)\n",
    "    \n",
    "    # 레이블의 값은 0~ 10까지의 값을 갖습니다. \n",
    "    # 따라서 해당 변수 하나하나를 명목화 시켜줍니다. \n",
    "    train_labels = to_categorical(train_labels,10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def normalize(train_data, test_data):\n",
    "    train_data = train_data.astype(np.float32)/255.0\n",
    "    test_data = test_data.astype(np.float32)/255.0\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22615fb4",
   "metadata": {},
   "source": [
    "### 모델을 구성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a2f216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten() :\n",
    "    return tf.keras.layers.Flatten()\n",
    "\n",
    "def dense(label_dim, weight_init) :\n",
    "    return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init)\n",
    "\n",
    "def relu() :\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152959fd",
   "metadata": {},
   "source": [
    "### class ver. 모델 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9411ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클레스에 매게변수를 전달하는것을 상속이라 합니다. \n",
    "# tf.keras,Model 클레스를 상속받음으로써, 해당 모형에서 사용하는 메소드를 직접 사용 가능합니다. \n",
    "class create_model_class(tf.keras.Model):\n",
    "    def __init__(self, label_dim):\n",
    "        # create_model_class가 상속받은 부모클레스, 즉 tf.keras.model의 init부분을 실행해 해당 init메소드가 부모의 init메소드를 오버라이딩 하지 않고 별개로 실행되되록 만듭니다.\n",
    "        super(create_model_class, self).__init__()\n",
    "        \n",
    "        # 초기 가중치 설정 \n",
    "        # f.keras.initializers.RandomNormal()는 평균이 0 분산이 1인 가우시안 분포로 랜덤한 수를 생성\n",
    "        # 지역변수로 설정되며, 추후 각 층에 매게변수로 전달 \n",
    "        weight_init = tf.keras.initializers.RandomNormal()\n",
    "        \n",
    "        # 네트워크는 convolution이나 fullt connected를 층층히 쌓아나가는 과정인데 이는 리스트를 계속 더해주는 과정이라고 할 수 있다. # 따라서 tf.keras.Sequential()는 어떤 리스트 자료구조타입이므로 이를 이용한다. \n",
    "        self.model = tf.keras.Sequential() \n",
    "        # fully connected를 이용할 것이므로 이미지의 shape을 [N,28,28,1] -> [N, 784]로 변경 (convolution을 이용할 경우 flatten의 과정을 필요하지 않다.)\n",
    "        self.model.add(flatten())\n",
    "        \n",
    "        # 히든 레이어 설정 \n",
    "        for i in range(2) :\n",
    "            model.add(dense(256, weight_init))\n",
    "            model.add(relu())\n",
    "            \n",
    "        # 출력층 생성 \n",
    "        self.model.add(dense(label_dim, weight_init))\n",
    "         \n",
    "        # 생성된 모델을 호출하는 메소드 \n",
    "        # 인자로 모델의 전달사항을 지정 \n",
    "        def call(self, x, training=None, mask=None):\n",
    "            x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef146603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 막간을 이용한 상속과 오버라이딩 개념 복습 \n",
    "class parents():\n",
    "    def __init__(self,c1,c2):\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2 \n",
    "    \n",
    "    def add(self):\n",
    "        print(self.c1+self.c2)\n",
    "        \n",
    "pt = parents(1,2)\n",
    "pt.add()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb88d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 막간을 이용한 상속과 오버라이딩 개념 복습 \n",
    "# class parents():\n",
    "#     def __init__(self,c1,c2):\n",
    "#         self.c1 = c1\n",
    "#         self.c2 = c2 \n",
    "    \n",
    "#     def add(self):\n",
    "#         print(self.c1+self.c2)\n",
    "        \n",
    "# # 상속 ver1 \n",
    "# class child1(parents):\n",
    "#     def __init__(self,v1,v2):\n",
    "#         self.v1 = v1\n",
    "#         self.v2 = v2\n",
    "#     def sub(self):\n",
    "#         print(self.v1 - self.v2)\n",
    "        \n",
    "# child1 = child1(1,2)\n",
    "# child1.sub()\n",
    "# child1.add()\n",
    "\n",
    "# # 에러.. 부모가 가지고 있는 변수인 c1과 c2를 입력하지 않아 발생 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f8319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 막간을 이용한 상속과 오버라이딩 개념 복습 \n",
    "class parents():\n",
    "    def __init__(self,c1,c2):\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2 \n",
    "    \n",
    "    def add(self):\n",
    "        print(self.c1+self.c2)\n",
    "        \n",
    "# 상속 ver1 \n",
    "class child1(parents):\n",
    "    def __init__(self,v1,v2):\n",
    "        self.v1 = v1\n",
    "        self.v2 = v2\n",
    "        super().__init__(v1,v2)\n",
    "        super(child1, self).__init__(v1,v2)\n",
    "    def sub(self):\n",
    "        print(self.v1 - self.v2)\n",
    "        \n",
    "child1 = child1(1,2)\n",
    "child1.sub()\n",
    "child1.add()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f82fd",
   "metadata": {},
   "source": [
    "supper() 메소드는 부모 클레스의 특정 부분을 실행해야 하는 경우 사용됩니다.  \n",
    "해당 경우에서는 자식클레스의 인자를 더하기 위해서는, 해당 값들을 부모클레스에 전달하는 과정을 거쳐야 함으로, v1과 v2를 부모의 초기화 함수의 인자로 넣어 실행해 주어야 합니다. \n",
    "\n",
    "[참조1 || 딥러닝 class에서의 상속](https://velog.io/@gwkoo/%ED%81%B4%EB%9E%98%EC%8A%A4-%EC%83%81%EC%86%8D-%EB%B0%8F-super-%ED%95%A8%EC%88%98%EC%9D%98-%EC%97%AD%ED%95%A0)\n",
    "[참조2 || 코딩도장](https://dojang.io/mod/page/view.php?id=2386)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2b037",
   "metadata": {},
   "source": [
    "### Function ver. 모형생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a9fb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_function(label_dim) :\n",
    "    weight_init = tf.keras.initializers.RandomNormal()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(flatten())\n",
    "\n",
    "    for i in range(2) :\n",
    "        model.add(dense(256, weight_init))\n",
    "        model.add(relu())\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe08fcc7",
   "metadata": {},
   "source": [
    "모형의 형태는 다음과 같습니다.  \n",
    "||flatten|| --> ||dense + relu|| --> ||dense + relu|| --> ||dense||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe1bf8",
   "metadata": {},
   "source": [
    "### 데이터 불러오기 및 하이퍼 파라미터 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e9537b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "# 이전에 선언한 함수를 이용\n",
    "train_x, train_y, test_x, test_y = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c5b7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" parameters \"\"\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "training_epochs = 1\n",
    "training_iterations = len(train_x) // batch_size\n",
    "\n",
    "label_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91eccc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb5772de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Graph Input using Dataset API \"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639dcec",
   "metadata": {},
   "source": [
    "### 모형 생성 및 옵티마이저 생성, log writer 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee22d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "network = create_model_function(label_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0cdf4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "# 계산된 비용에 대한 정보를 모형에 반영하는 옵티마이져 \n",
    "# adam을 사용합니다. \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d3deb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Writer \"\"\"\n",
    "checkpoint_dir = 'checkpoints'\n",
    "logs_dir = 'logs'\n",
    "\n",
    "model_dir = 'nn_relu'\n",
    "\n",
    "checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "check_folder(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\n",
    "logs_dir = os.path.join(logs_dir, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c02c0a",
   "metadata": {},
   "source": [
    "### Restore checkpoint & start train or test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19312b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      " [*] Success to read nn_relu-468-1\n",
      " [*] Load SUCCESS\n"
     ]
    }
   ],
   "source": [
    "if train_flag :\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(dnn=network)\n",
    "\n",
    "    # create writer for tensorboard\n",
    "    summary_writer = tf.summary.create_file_writer(logdir=logs_dir)\n",
    "    start_time = time()\n",
    "\n",
    "    # restore check-point if it exits\n",
    "    could_load, checkpoint_counter = load(network, checkpoint_dir)    \n",
    "\n",
    "    if could_load:\n",
    "        start_epoch = (int)(checkpoint_counter / training_iterations)        \n",
    "        counter = checkpoint_counter        \n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        start_iteration = 0\n",
    "        counter = 0\n",
    "        print(\" [!] Load failed...\")\n",
    "    \n",
    "    # train phase\n",
    "    with summary_writer.as_default():  # for tensorboard\n",
    "        for epoch in range(start_epoch, training_epochs):\n",
    "            for idx, (train_input, train_label) in enumerate(train_dataset):                \n",
    "                grads = grad(network, train_input, train_label)\n",
    "                optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "\n",
    "                train_loss = loss_fn(network, train_input, train_label)\n",
    "                train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "\n",
    "                for test_input, test_label in test_dataset:                \n",
    "                    test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "                tf.summary.scalar(name='train_loss', data=train_loss, step=counter)\n",
    "                tf.summary.scalar(name='train_accuracy', data=train_accuracy, step=counter)\n",
    "                tf.summary.scalar(name='test_accuracy', data=test_accuracy, step=counter)\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\n",
    "                    % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy,\n",
    "                       test_accuracy))\n",
    "                counter += 1\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix + '-{}'.format(counter))\n",
    "        \n",
    "# test phase      \n",
    "else :\n",
    "    _, _ = load(network, checkpoint_dir)\n",
    "    for test_input, test_label in test_dataset:    \n",
    "        test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "\n",
    "    print(\"test_Accuracy: %.4f\" % (test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008fd10",
   "metadata": {},
   "source": [
    "정확도가 대폭 증가했음을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f56e41",
   "metadata": {},
   "source": [
    "## 초기 가중치 설정을 통한 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd49a77",
   "metadata": {},
   "source": [
    "### 학습 내용 \n",
    "* 초기 가중치 설정을 잘 해야 하는 이유는 크게 두가지가 있습니다. \n",
    "1. 우선, 초기의 가중치가 0이라고 가정한다면, 특정 위치에서 비용함수를 미분한 값이 0으로 수렴하기 때문에, 해당 계층 이전의 신경망에 대한 미분값은 chain rule을 적용한다면 모두 0으로 수렴하게 됩니다. 따라서, 모형이 학습되지 않는 문제를 발생시킵니다.  \n",
    "2. 또한, global min과 Local min이라는 개념입니다. 우리가 진행하는 학습의 목표는 모형의 loss를 최소화 시키는 가중치를 찾는 일입니다. ![cost1.png](https://miro.medium.com/max/600/1*hf_5r1yZr7JiOWc48TKv9g.jpeg) 위와 같은 포락선 형태의 비용함수가 정의된다면, 어느 위치 값을 초기 가중치로 지정한다 해도 정상적인 학습이 가능할 것입니다.  ![cost2.png](https://skill-lync-portal.nyc3.digitaloceanspaces.com/tinymce/05_20/15900802955804.jpg) 하지만, 대부분의 비용함수는 포락선의 구조보다는 다소 복잡한 특성을 지니기 때문에 초기 가중치의 부여에 따라, 최종적인 목적지가 Local min이 되기도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6da234",
   "metadata": {},
   "source": [
    "* Hinton의 \"Deep Belief Nets\"(DBN) - 가중치 초기화 방식으로 Restrict Boatman Machine(RBM) 사용 \n",
    "    - 2006년 제프리 힌튼은 새로운 가중치 초기화 방식을 고안합니다.![RBM.png](https://blog.kakaocdn.net/dn/bripjr/btqCqNShTAZ/gRIikkpz0sDp0kiRcooXlK/img.png)RBM 방식은 여러개의 층으로 이루어진 심층 신경망이 존재할 때, 신경망의 앞 부분 부터, 하나의 층과 그 다음층 사이의 관계를 통해 가중치를 설정합니다.  \n",
    "    - 예를들어, 위의 그림에서는 첫번째 layer와 두번째 layer만을 두고 아래의 과정을 진행합니다.  \n",
    "    - 1. 우선 처음 입력된 X에 임의의 가중치 W를 곱해 다음 계층으로 값을 전달합니다.  \n",
    "    - 2. 다음으로는 해당 값을 전달받은 계층에서 역으로 같은 가중치 W를 곱해 이전 계층으로 전달합니다. \n",
    "    - 3. 이때, 실제값 X와 역으로 계산된 X^ 의 값의 차이가 최소가 되는 방식으로 가중치를 설정하며, 모형의 끝 까지 가중치를 설정합니다. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80677a1a",
   "metadata": {},
   "source": [
    "* 하지만, 최근에는 이보다 단순하고 성능이 우월한 다양한 초기화 방식이 발전하고 있으며, 그 대표적인 방식으로는 Xavier initalization(= Glorot initalization) 방식과 Relu함수의 가중치 초기화에 특화된 He initalization 방식이 있습니다. \n",
    "##### Xavier initalization \n",
    "- 하나의 노드에 들어오는 입력의 채널 수와 출력 채널의 수를 고려한 가중치 설정 방식 \n",
    "- 평균 : 0 \n",
    "- 분산 : $$\\sigma^2 = \\frac{2}{Channel(in) + Channel(out)}$$\n",
    "##### He initalization\n",
    "- Relu함수와 쓰일때 가장 효율적인 방식 \n",
    "- 평균 : 0 \n",
    "- 분산 : $$\\sigma^2 = \\frac{4}{Channel(in) + Channel(out)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c6689",
   "metadata": {},
   "source": [
    "### 코드로 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b71a0",
   "metadata": {},
   "source": [
    "* 체크 포인트를 구성하는 방식을 제외하고 성능을 확인해봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80018282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d7c49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 로드 함수 \n",
    "def load_mnist(): \n",
    "    # 데이터를 불러옵니다. \n",
    "    (train_data, train_label), (test_data, test_label) = mnist.load_data()\n",
    "    \n",
    "    # 데이터에 채널 차원을 추가합니다. \n",
    "    train_data = np.expand_dims(train_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1] \n",
    "    test_data = np.expand_dims(test_data, axis = -1) # [N, 28, 28] -> [N, 28, 28, 1]\n",
    "    \n",
    "    # 밝기값이 0~255 --> 0~1로 축소 \n",
    "    train_data, test_data = normalize(train_data, test_data)\n",
    "    \n",
    "    # 0~10 까지의 레이블을 원핫인코딩 진행 \n",
    "    train_label = to_categorical(train_label,10) # [N,] -> [N, 10]\n",
    "    test_label = to_categorical(test_label, 10) # [N,] -> [N, 10]\n",
    "    \n",
    "    return train_data, train_label, test_data, test_label\n",
    "\n",
    "# 위에서 사용한 정규화 함수 구현 \n",
    "def normalize(train_data, test_data):\n",
    "    train_data = train_data.astype(np.float32) / 255.0\n",
    "    test_data = test_data.astype(np.float32) / 255.0\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38d41990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 함수 구현 \n",
    "# def loss_fn(model, images, labels):\n",
    "#     # 예측값 함수 \n",
    "#     logits = model(images, training = True)\n",
    "#     # 비용함수 생성 \n",
    "#     # cross entropy를 자동으로 계산해주는 내장함수 사용 \n",
    "#     loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits,\n",
    "#                                                                   y_true = labels,\n",
    "#                                                                   from_logits = True))\n",
    "#     # logit True는 예측 로짓값(확률 행렬)을 그대로 넘겨준다는것을 의미 \n",
    "#     return loss \n",
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)\n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits, y_true=labels, \n",
    "                                                                   from_logits=True))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d3446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 측정 함수 구현 \n",
    "# test data를 통해 정확도를 측정하는 함수를 구현합니다. \n",
    "# 실제값과 예측값을 비교해 참 1 거짓0 --> 평균 \n",
    "def accuracy_fn(model, images, labels):\n",
    "    # 예측 수행 \n",
    "    logits = model(images, training = False)\n",
    "    # 확률이 가장 높은 값을 기준으로 비교 \n",
    "    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
    "    # 평균 계산 \n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71b974ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 계산 함수 \n",
    "# tf제공 Gradient Tape을 활용한 계산 \n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ecd88de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten함수 구현 \n",
    "# fully connected model에서는 값을 선형으로 잡어 넣어 주어야 한다. \n",
    "def flatten() :\n",
    "    return tf.keras.layers.Flatten()\n",
    "# 추후 flatten layer로 입력층 정의시 사용 \n",
    "# [N,28,28] -> [N,784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9635bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer \n",
    "def dense(label_dim, weight_init) :\n",
    "    return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init)\n",
    "\n",
    "# units옵션은 출력 채널의 갯수를 의미합니다. \n",
    "# use_bias는 편향치를 사용할지를 물어봅니다. \n",
    "# kernel_initializer=weight_init 가중치 초기화 도구 입력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2c142f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu() :\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59259c",
   "metadata": {},
   "source": [
    "##### 변경부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c4b0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 생성 \n",
    "# 이 부분에서 다양한 initializer를 사용해봅시다. \n",
    "\n",
    "def create_model_function(label_dim) :\n",
    "    # weight_init = tf.keras.initializers.RandomNormal()\n",
    "    weight_init = tf.keras.initializers.glorot_uniform()\n",
    "    # weight_init = tf.keras.initializers.he_uniform()\n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(flatten())\n",
    "\n",
    "    for i in range(2) :\n",
    "        model.add(dense(256, weight_init))\n",
    "        model.add(relu())\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6ed1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "train_x, train_y, test_x, test_y = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2412aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" parameters \"\"\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "training_epochs = 1\n",
    "training_iterations = len(train_x) // batch_size\n",
    "\n",
    "label_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "72a230f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Graph Input using Dataset API \"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30f65166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "network = create_model_function(label_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9bd7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training \"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20801aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [    0/  468], training loss: [2.06317616], train_acc: [0.4375], test_acc: [0.2760]\n",
      "Epoch: [ 0], batch: [    1/  468], training loss: [1.91225195], train_acc: [0.4688], test_acc: [0.3525]\n",
      "Epoch: [ 0], batch: [    2/  468], training loss: [1.93908083], train_acc: [0.4375], test_acc: [0.4600]\n",
      "Epoch: [ 0], batch: [    3/  468], training loss: [1.83171368], train_acc: [0.5312], test_acc: [0.5871]\n",
      "Epoch: [ 0], batch: [    4/  468], training loss: [1.70389509], train_acc: [0.6719], test_acc: [0.6550]\n",
      "Epoch: [ 0], batch: [    5/  468], training loss: [1.53497541], train_acc: [0.7656], test_acc: [0.7012]\n",
      "Epoch: [ 0], batch: [    6/  468], training loss: [1.39252663], train_acc: [0.8125], test_acc: [0.7219]\n",
      "Epoch: [ 0], batch: [    7/  468], training loss: [1.34741354], train_acc: [0.7188], test_acc: [0.7426]\n",
      "Epoch: [ 0], batch: [    8/  468], training loss: [1.14287615], train_acc: [0.7578], test_acc: [0.7583]\n",
      "Epoch: [ 0], batch: [    9/  468], training loss: [1.11872625], train_acc: [0.7344], test_acc: [0.7865]\n",
      "Epoch: [ 0], batch: [   10/  468], training loss: [1.04204774], train_acc: [0.8047], test_acc: [0.8021]\n",
      "Epoch: [ 0], batch: [   11/  468], training loss: [0.77646697], train_acc: [0.8672], test_acc: [0.8066]\n",
      "Epoch: [ 0], batch: [   12/  468], training loss: [0.78121716], train_acc: [0.8359], test_acc: [0.8021]\n",
      "Epoch: [ 0], batch: [   13/  468], training loss: [0.65335333], train_acc: [0.8594], test_acc: [0.7957]\n",
      "Epoch: [ 0], batch: [   14/  468], training loss: [0.78570092], train_acc: [0.7734], test_acc: [0.8118]\n",
      "Epoch: [ 0], batch: [   15/  468], training loss: [0.68114233], train_acc: [0.7891], test_acc: [0.8217]\n",
      "Epoch: [ 0], batch: [   16/  468], training loss: [0.61224091], train_acc: [0.8594], test_acc: [0.8237]\n",
      "Epoch: [ 0], batch: [   17/  468], training loss: [0.63711369], train_acc: [0.8125], test_acc: [0.8163]\n",
      "Epoch: [ 0], batch: [   18/  468], training loss: [0.64550394], train_acc: [0.7969], test_acc: [0.8112]\n",
      "Epoch: [ 0], batch: [   19/  468], training loss: [0.63399148], train_acc: [0.7812], test_acc: [0.8210]\n",
      "Epoch: [ 0], batch: [   20/  468], training loss: [0.54378051], train_acc: [0.8438], test_acc: [0.8414]\n",
      "Epoch: [ 0], batch: [   21/  468], training loss: [0.44048822], train_acc: [0.8906], test_acc: [0.8609]\n",
      "Epoch: [ 0], batch: [   22/  468], training loss: [0.51283598], train_acc: [0.8516], test_acc: [0.8637]\n",
      "Epoch: [ 0], batch: [   23/  468], training loss: [0.43725669], train_acc: [0.8906], test_acc: [0.8527]\n",
      "Epoch: [ 0], batch: [   24/  468], training loss: [0.68361765], train_acc: [0.8047], test_acc: [0.8494]\n",
      "Epoch: [ 0], batch: [   25/  468], training loss: [0.45523643], train_acc: [0.8516], test_acc: [0.8605]\n",
      "Epoch: [ 0], batch: [   26/  468], training loss: [0.34680709], train_acc: [0.8984], test_acc: [0.8672]\n",
      "Epoch: [ 0], batch: [   27/  468], training loss: [0.44556212], train_acc: [0.8828], test_acc: [0.8743]\n",
      "Epoch: [ 0], batch: [   28/  468], training loss: [0.48993325], train_acc: [0.8516], test_acc: [0.8828]\n",
      "Epoch: [ 0], batch: [   29/  468], training loss: [0.40012515], train_acc: [0.8984], test_acc: [0.8838]\n",
      "Epoch: [ 0], batch: [   30/  468], training loss: [0.45086929], train_acc: [0.8984], test_acc: [0.8770]\n",
      "Epoch: [ 0], batch: [   31/  468], training loss: [0.37667575], train_acc: [0.8438], test_acc: [0.8778]\n",
      "Epoch: [ 0], batch: [   32/  468], training loss: [0.44528008], train_acc: [0.8906], test_acc: [0.8824]\n",
      "Epoch: [ 0], batch: [   33/  468], training loss: [0.25156018], train_acc: [0.9219], test_acc: [0.8879]\n",
      "Epoch: [ 0], batch: [   34/  468], training loss: [0.39312601], train_acc: [0.9062], test_acc: [0.8876]\n",
      "Epoch: [ 0], batch: [   35/  468], training loss: [0.36161509], train_acc: [0.8750], test_acc: [0.8877]\n",
      "Epoch: [ 0], batch: [   36/  468], training loss: [0.40570951], train_acc: [0.8672], test_acc: [0.8865]\n",
      "Epoch: [ 0], batch: [   37/  468], training loss: [0.34494293], train_acc: [0.9219], test_acc: [0.8858]\n",
      "Epoch: [ 0], batch: [   38/  468], training loss: [0.36204904], train_acc: [0.8906], test_acc: [0.8851]\n",
      "Epoch: [ 0], batch: [   39/  468], training loss: [0.27602583], train_acc: [0.8906], test_acc: [0.8878]\n",
      "Epoch: [ 0], batch: [   40/  468], training loss: [0.41319853], train_acc: [0.8984], test_acc: [0.8835]\n",
      "Epoch: [ 0], batch: [   41/  468], training loss: [0.41073710], train_acc: [0.8594], test_acc: [0.8817]\n",
      "Epoch: [ 0], batch: [   42/  468], training loss: [0.22986566], train_acc: [0.9219], test_acc: [0.8822]\n",
      "Epoch: [ 0], batch: [   43/  468], training loss: [0.43539304], train_acc: [0.8516], test_acc: [0.8899]\n",
      "Epoch: [ 0], batch: [   44/  468], training loss: [0.38782287], train_acc: [0.8828], test_acc: [0.8940]\n",
      "Epoch: [ 0], batch: [   45/  468], training loss: [0.27766651], train_acc: [0.9219], test_acc: [0.8932]\n",
      "Epoch: [ 0], batch: [   46/  468], training loss: [0.33271044], train_acc: [0.8906], test_acc: [0.8916]\n",
      "Epoch: [ 0], batch: [   47/  468], training loss: [0.41578105], train_acc: [0.8750], test_acc: [0.8919]\n",
      "Epoch: [ 0], batch: [   48/  468], training loss: [0.36779118], train_acc: [0.8672], test_acc: [0.8989]\n",
      "Epoch: [ 0], batch: [   49/  468], training loss: [0.47471893], train_acc: [0.8984], test_acc: [0.9009]\n",
      "Epoch: [ 0], batch: [   50/  468], training loss: [0.18726741], train_acc: [0.9453], test_acc: [0.8995]\n",
      "Epoch: [ 0], batch: [   51/  468], training loss: [0.29835850], train_acc: [0.9453], test_acc: [0.8991]\n",
      "Epoch: [ 0], batch: [   52/  468], training loss: [0.15779674], train_acc: [0.9609], test_acc: [0.8957]\n",
      "Epoch: [ 0], batch: [   53/  468], training loss: [0.29989165], train_acc: [0.9062], test_acc: [0.8942]\n",
      "Epoch: [ 0], batch: [   54/  468], training loss: [0.36108723], train_acc: [0.9219], test_acc: [0.8938]\n",
      "Epoch: [ 0], batch: [   55/  468], training loss: [0.27039009], train_acc: [0.9141], test_acc: [0.8936]\n",
      "Epoch: [ 0], batch: [   56/  468], training loss: [0.19087428], train_acc: [0.9453], test_acc: [0.8902]\n",
      "Epoch: [ 0], batch: [   57/  468], training loss: [0.29253703], train_acc: [0.8906], test_acc: [0.8907]\n",
      "Epoch: [ 0], batch: [   58/  468], training loss: [0.26334044], train_acc: [0.9062], test_acc: [0.8937]\n",
      "Epoch: [ 0], batch: [   59/  468], training loss: [0.28148067], train_acc: [0.8984], test_acc: [0.8961]\n",
      "Epoch: [ 0], batch: [   60/  468], training loss: [0.37134507], train_acc: [0.8828], test_acc: [0.9000]\n",
      "Epoch: [ 0], batch: [   61/  468], training loss: [0.36091772], train_acc: [0.9062], test_acc: [0.9033]\n",
      "Epoch: [ 0], batch: [   62/  468], training loss: [0.35875297], train_acc: [0.8516], test_acc: [0.9058]\n",
      "Epoch: [ 0], batch: [   63/  468], training loss: [0.27440935], train_acc: [0.9297], test_acc: [0.9047]\n",
      "Epoch: [ 0], batch: [   64/  468], training loss: [0.40719569], train_acc: [0.8984], test_acc: [0.9035]\n",
      "Epoch: [ 0], batch: [   65/  468], training loss: [0.31362489], train_acc: [0.9219], test_acc: [0.9043]\n",
      "Epoch: [ 0], batch: [   66/  468], training loss: [0.24011147], train_acc: [0.9297], test_acc: [0.9054]\n",
      "Epoch: [ 0], batch: [   67/  468], training loss: [0.20059629], train_acc: [0.9531], test_acc: [0.9077]\n",
      "Epoch: [ 0], batch: [   68/  468], training loss: [0.20970508], train_acc: [0.9219], test_acc: [0.9111]\n",
      "Epoch: [ 0], batch: [   69/  468], training loss: [0.33202708], train_acc: [0.9062], test_acc: [0.9105]\n",
      "Epoch: [ 0], batch: [   70/  468], training loss: [0.37819308], train_acc: [0.9062], test_acc: [0.9130]\n",
      "Epoch: [ 0], batch: [   71/  468], training loss: [0.32939601], train_acc: [0.8906], test_acc: [0.9151]\n",
      "Epoch: [ 0], batch: [   72/  468], training loss: [0.39699620], train_acc: [0.8828], test_acc: [0.9158]\n",
      "Epoch: [ 0], batch: [   73/  468], training loss: [0.30730698], train_acc: [0.9062], test_acc: [0.9162]\n",
      "Epoch: [ 0], batch: [   74/  468], training loss: [0.46340275], train_acc: [0.8828], test_acc: [0.9173]\n",
      "Epoch: [ 0], batch: [   75/  468], training loss: [0.18220168], train_acc: [0.9453], test_acc: [0.9139]\n",
      "Epoch: [ 0], batch: [   76/  468], training loss: [0.18103346], train_acc: [0.9531], test_acc: [0.9103]\n",
      "Epoch: [ 0], batch: [   77/  468], training loss: [0.37890610], train_acc: [0.8750], test_acc: [0.9101]\n",
      "Epoch: [ 0], batch: [   78/  468], training loss: [0.26572335], train_acc: [0.9219], test_acc: [0.9135]\n",
      "Epoch: [ 0], batch: [   79/  468], training loss: [0.31679592], train_acc: [0.9062], test_acc: [0.9161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [   80/  468], training loss: [0.30259189], train_acc: [0.9062], test_acc: [0.9185]\n",
      "Epoch: [ 0], batch: [   81/  468], training loss: [0.20902933], train_acc: [0.9375], test_acc: [0.9190]\n",
      "Epoch: [ 0], batch: [   82/  468], training loss: [0.26802915], train_acc: [0.9375], test_acc: [0.9185]\n",
      "Epoch: [ 0], batch: [   83/  468], training loss: [0.21413162], train_acc: [0.9297], test_acc: [0.9175]\n",
      "Epoch: [ 0], batch: [   84/  468], training loss: [0.24117382], train_acc: [0.9219], test_acc: [0.9172]\n",
      "Epoch: [ 0], batch: [   85/  468], training loss: [0.18385139], train_acc: [0.9453], test_acc: [0.9182]\n",
      "Epoch: [ 0], batch: [   86/  468], training loss: [0.35866028], train_acc: [0.9297], test_acc: [0.9189]\n",
      "Epoch: [ 0], batch: [   87/  468], training loss: [0.30564338], train_acc: [0.9219], test_acc: [0.9186]\n",
      "Epoch: [ 0], batch: [   88/  468], training loss: [0.40369505], train_acc: [0.8516], test_acc: [0.9168]\n",
      "Epoch: [ 0], batch: [   89/  468], training loss: [0.27077526], train_acc: [0.8984], test_acc: [0.9170]\n",
      "Epoch: [ 0], batch: [   90/  468], training loss: [0.26773673], train_acc: [0.9219], test_acc: [0.9131]\n",
      "Epoch: [ 0], batch: [   91/  468], training loss: [0.28887206], train_acc: [0.9141], test_acc: [0.9142]\n",
      "Epoch: [ 0], batch: [   92/  468], training loss: [0.14607096], train_acc: [0.9531], test_acc: [0.9150]\n",
      "Epoch: [ 0], batch: [   93/  468], training loss: [0.35936514], train_acc: [0.8984], test_acc: [0.9152]\n",
      "Epoch: [ 0], batch: [   94/  468], training loss: [0.23467031], train_acc: [0.9453], test_acc: [0.9185]\n",
      "Epoch: [ 0], batch: [   95/  468], training loss: [0.20623735], train_acc: [0.9375], test_acc: [0.9186]\n",
      "Epoch: [ 0], batch: [   96/  468], training loss: [0.27697462], train_acc: [0.9297], test_acc: [0.9161]\n",
      "Epoch: [ 0], batch: [   97/  468], training loss: [0.14467107], train_acc: [0.9609], test_acc: [0.9165]\n",
      "Epoch: [ 0], batch: [   98/  468], training loss: [0.30882251], train_acc: [0.9375], test_acc: [0.9208]\n",
      "Epoch: [ 0], batch: [   99/  468], training loss: [0.30381835], train_acc: [0.9219], test_acc: [0.9255]\n",
      "Epoch: [ 0], batch: [  100/  468], training loss: [0.25350058], train_acc: [0.9062], test_acc: [0.9282]\n",
      "Epoch: [ 0], batch: [  101/  468], training loss: [0.16686292], train_acc: [0.9453], test_acc: [0.9268]\n",
      "Epoch: [ 0], batch: [  102/  468], training loss: [0.18375136], train_acc: [0.9609], test_acc: [0.9218]\n",
      "Epoch: [ 0], batch: [  103/  468], training loss: [0.18267958], train_acc: [0.9531], test_acc: [0.9167]\n",
      "Epoch: [ 0], batch: [  104/  468], training loss: [0.26849213], train_acc: [0.9297], test_acc: [0.9147]\n",
      "Epoch: [ 0], batch: [  105/  468], training loss: [0.35293254], train_acc: [0.8672], test_acc: [0.9179]\n",
      "Epoch: [ 0], batch: [  106/  468], training loss: [0.21319932], train_acc: [0.9219], test_acc: [0.9209]\n",
      "Epoch: [ 0], batch: [  107/  468], training loss: [0.14062275], train_acc: [0.9609], test_acc: [0.9274]\n",
      "Epoch: [ 0], batch: [  108/  468], training loss: [0.17240348], train_acc: [0.9609], test_acc: [0.9292]\n",
      "Epoch: [ 0], batch: [  109/  468], training loss: [0.27937734], train_acc: [0.9141], test_acc: [0.9286]\n",
      "Epoch: [ 0], batch: [  110/  468], training loss: [0.15757774], train_acc: [0.9531], test_acc: [0.9283]\n",
      "Epoch: [ 0], batch: [  111/  468], training loss: [0.18909395], train_acc: [0.9609], test_acc: [0.9277]\n",
      "Epoch: [ 0], batch: [  112/  468], training loss: [0.26909399], train_acc: [0.9141], test_acc: [0.9297]\n",
      "Epoch: [ 0], batch: [  113/  468], training loss: [0.15754238], train_acc: [0.9531], test_acc: [0.9330]\n",
      "Epoch: [ 0], batch: [  114/  468], training loss: [0.19343120], train_acc: [0.9531], test_acc: [0.9326]\n",
      "Epoch: [ 0], batch: [  115/  468], training loss: [0.15395045], train_acc: [0.9766], test_acc: [0.9304]\n",
      "Epoch: [ 0], batch: [  116/  468], training loss: [0.16831370], train_acc: [0.9531], test_acc: [0.9286]\n",
      "Epoch: [ 0], batch: [  117/  468], training loss: [0.26232088], train_acc: [0.8984], test_acc: [0.9278]\n",
      "Epoch: [ 0], batch: [  118/  468], training loss: [0.26186773], train_acc: [0.9375], test_acc: [0.9291]\n",
      "Epoch: [ 0], batch: [  119/  468], training loss: [0.17452073], train_acc: [0.9531], test_acc: [0.9313]\n",
      "Epoch: [ 0], batch: [  120/  468], training loss: [0.23360521], train_acc: [0.9062], test_acc: [0.9324]\n",
      "Epoch: [ 0], batch: [  121/  468], training loss: [0.18593563], train_acc: [0.9531], test_acc: [0.9339]\n",
      "Epoch: [ 0], batch: [  122/  468], training loss: [0.16362697], train_acc: [0.9531], test_acc: [0.9322]\n",
      "Epoch: [ 0], batch: [  123/  468], training loss: [0.32729238], train_acc: [0.9141], test_acc: [0.9293]\n",
      "Epoch: [ 0], batch: [  124/  468], training loss: [0.22958899], train_acc: [0.9297], test_acc: [0.9245]\n",
      "Epoch: [ 0], batch: [  125/  468], training loss: [0.26079580], train_acc: [0.9297], test_acc: [0.9214]\n",
      "Epoch: [ 0], batch: [  126/  468], training loss: [0.15200414], train_acc: [0.9609], test_acc: [0.9237]\n",
      "Epoch: [ 0], batch: [  127/  468], training loss: [0.30605966], train_acc: [0.9062], test_acc: [0.9277]\n",
      "Epoch: [ 0], batch: [  128/  468], training loss: [0.14516893], train_acc: [0.9688], test_acc: [0.9293]\n",
      "Epoch: [ 0], batch: [  129/  468], training loss: [0.18631423], train_acc: [0.9219], test_acc: [0.9301]\n",
      "Epoch: [ 0], batch: [  130/  468], training loss: [0.33275822], train_acc: [0.8750], test_acc: [0.9289]\n",
      "Epoch: [ 0], batch: [  131/  468], training loss: [0.16024479], train_acc: [0.9688], test_acc: [0.9275]\n",
      "Epoch: [ 0], batch: [  132/  468], training loss: [0.16824827], train_acc: [0.9219], test_acc: [0.9265]\n",
      "Epoch: [ 0], batch: [  133/  468], training loss: [0.26220042], train_acc: [0.9297], test_acc: [0.9270]\n",
      "Epoch: [ 0], batch: [  134/  468], training loss: [0.28220254], train_acc: [0.9141], test_acc: [0.9302]\n",
      "Epoch: [ 0], batch: [  135/  468], training loss: [0.27299303], train_acc: [0.9062], test_acc: [0.9343]\n",
      "Epoch: [ 0], batch: [  136/  468], training loss: [0.21465543], train_acc: [0.9375], test_acc: [0.9367]\n",
      "Epoch: [ 0], batch: [  137/  468], training loss: [0.17060800], train_acc: [0.9531], test_acc: [0.9342]\n",
      "Epoch: [ 0], batch: [  138/  468], training loss: [0.20280398], train_acc: [0.9375], test_acc: [0.9318]\n",
      "Epoch: [ 0], batch: [  139/  468], training loss: [0.23600687], train_acc: [0.9375], test_acc: [0.9285]\n",
      "Epoch: [ 0], batch: [  140/  468], training loss: [0.21291408], train_acc: [0.9375], test_acc: [0.9271]\n",
      "Epoch: [ 0], batch: [  141/  468], training loss: [0.32838434], train_acc: [0.9062], test_acc: [0.9308]\n",
      "Epoch: [ 0], batch: [  142/  468], training loss: [0.30017868], train_acc: [0.9062], test_acc: [0.9355]\n",
      "Epoch: [ 0], batch: [  143/  468], training loss: [0.24005854], train_acc: [0.9531], test_acc: [0.9357]\n",
      "Epoch: [ 0], batch: [  144/  468], training loss: [0.14785913], train_acc: [0.9453], test_acc: [0.9307]\n",
      "Epoch: [ 0], batch: [  145/  468], training loss: [0.16033584], train_acc: [0.9375], test_acc: [0.9225]\n",
      "Epoch: [ 0], batch: [  146/  468], training loss: [0.35594496], train_acc: [0.8672], test_acc: [0.9201]\n",
      "Epoch: [ 0], batch: [  147/  468], training loss: [0.25252572], train_acc: [0.9531], test_acc: [0.9245]\n",
      "Epoch: [ 0], batch: [  148/  468], training loss: [0.43354321], train_acc: [0.8750], test_acc: [0.9318]\n",
      "Epoch: [ 0], batch: [  149/  468], training loss: [0.22984129], train_acc: [0.9375], test_acc: [0.9380]\n",
      "Epoch: [ 0], batch: [  150/  468], training loss: [0.17413947], train_acc: [0.9531], test_acc: [0.9400]\n",
      "Epoch: [ 0], batch: [  151/  468], training loss: [0.20222707], train_acc: [0.9531], test_acc: [0.9368]\n",
      "Epoch: [ 0], batch: [  152/  468], training loss: [0.24234718], train_acc: [0.9297], test_acc: [0.9336]\n",
      "Epoch: [ 0], batch: [  153/  468], training loss: [0.27554226], train_acc: [0.9062], test_acc: [0.9317]\n",
      "Epoch: [ 0], batch: [  154/  468], training loss: [0.35130757], train_acc: [0.8984], test_acc: [0.9330]\n",
      "Epoch: [ 0], batch: [  155/  468], training loss: [0.17446780], train_acc: [0.9297], test_acc: [0.9345]\n",
      "Epoch: [ 0], batch: [  156/  468], training loss: [0.20461330], train_acc: [0.9297], test_acc: [0.9379]\n",
      "Epoch: [ 0], batch: [  157/  468], training loss: [0.16079679], train_acc: [0.9609], test_acc: [0.9353]\n",
      "Epoch: [ 0], batch: [  158/  468], training loss: [0.19741568], train_acc: [0.9531], test_acc: [0.9324]\n",
      "Epoch: [ 0], batch: [  159/  468], training loss: [0.21273720], train_acc: [0.9297], test_acc: [0.9302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  160/  468], training loss: [0.15514831], train_acc: [0.9453], test_acc: [0.9280]\n",
      "Epoch: [ 0], batch: [  161/  468], training loss: [0.28705525], train_acc: [0.9141], test_acc: [0.9285]\n",
      "Epoch: [ 0], batch: [  162/  468], training loss: [0.28769395], train_acc: [0.9062], test_acc: [0.9323]\n",
      "Epoch: [ 0], batch: [  163/  468], training loss: [0.22128436], train_acc: [0.9375], test_acc: [0.9354]\n",
      "Epoch: [ 0], batch: [  164/  468], training loss: [0.17846763], train_acc: [0.9609], test_acc: [0.9371]\n",
      "Epoch: [ 0], batch: [  165/  468], training loss: [0.27893904], train_acc: [0.9219], test_acc: [0.9369]\n",
      "Epoch: [ 0], batch: [  166/  468], training loss: [0.14636195], train_acc: [0.9609], test_acc: [0.9342]\n",
      "Epoch: [ 0], batch: [  167/  468], training loss: [0.15362225], train_acc: [0.9609], test_acc: [0.9341]\n",
      "Epoch: [ 0], batch: [  168/  468], training loss: [0.18212496], train_acc: [0.9688], test_acc: [0.9363]\n",
      "Epoch: [ 0], batch: [  169/  468], training loss: [0.18730861], train_acc: [0.9531], test_acc: [0.9385]\n",
      "Epoch: [ 0], batch: [  170/  468], training loss: [0.23619244], train_acc: [0.9375], test_acc: [0.9386]\n",
      "Epoch: [ 0], batch: [  171/  468], training loss: [0.23224470], train_acc: [0.9453], test_acc: [0.9382]\n",
      "Epoch: [ 0], batch: [  172/  468], training loss: [0.14656174], train_acc: [0.9453], test_acc: [0.9388]\n",
      "Epoch: [ 0], batch: [  173/  468], training loss: [0.28254610], train_acc: [0.9297], test_acc: [0.9370]\n",
      "Epoch: [ 0], batch: [  174/  468], training loss: [0.09629511], train_acc: [0.9766], test_acc: [0.9356]\n",
      "Epoch: [ 0], batch: [  175/  468], training loss: [0.14672238], train_acc: [0.9609], test_acc: [0.9342]\n",
      "Epoch: [ 0], batch: [  176/  468], training loss: [0.21827373], train_acc: [0.9375], test_acc: [0.9357]\n",
      "Epoch: [ 0], batch: [  177/  468], training loss: [0.22153896], train_acc: [0.9375], test_acc: [0.9373]\n",
      "Epoch: [ 0], batch: [  178/  468], training loss: [0.20353559], train_acc: [0.9453], test_acc: [0.9402]\n",
      "Epoch: [ 0], batch: [  179/  468], training loss: [0.29868782], train_acc: [0.9297], test_acc: [0.9418]\n",
      "Epoch: [ 0], batch: [  180/  468], training loss: [0.22684589], train_acc: [0.9453], test_acc: [0.9415]\n",
      "Epoch: [ 0], batch: [  181/  468], training loss: [0.22867055], train_acc: [0.9609], test_acc: [0.9412]\n",
      "Epoch: [ 0], batch: [  182/  468], training loss: [0.13586214], train_acc: [0.9453], test_acc: [0.9390]\n",
      "Epoch: [ 0], batch: [  183/  468], training loss: [0.33148596], train_acc: [0.9375], test_acc: [0.9393]\n",
      "Epoch: [ 0], batch: [  184/  468], training loss: [0.13553569], train_acc: [0.9766], test_acc: [0.9383]\n",
      "Epoch: [ 0], batch: [  185/  468], training loss: [0.17672336], train_acc: [0.9453], test_acc: [0.9383]\n",
      "Epoch: [ 0], batch: [  186/  468], training loss: [0.17876475], train_acc: [0.9297], test_acc: [0.9388]\n",
      "Epoch: [ 0], batch: [  187/  468], training loss: [0.20161393], train_acc: [0.9219], test_acc: [0.9416]\n",
      "Epoch: [ 0], batch: [  188/  468], training loss: [0.23564000], train_acc: [0.9453], test_acc: [0.9459]\n",
      "Epoch: [ 0], batch: [  189/  468], training loss: [0.22669859], train_acc: [0.9141], test_acc: [0.9437]\n",
      "Epoch: [ 0], batch: [  190/  468], training loss: [0.08650144], train_acc: [0.9844], test_acc: [0.9431]\n",
      "Epoch: [ 0], batch: [  191/  468], training loss: [0.25973645], train_acc: [0.9062], test_acc: [0.9405]\n",
      "Epoch: [ 0], batch: [  192/  468], training loss: [0.27808467], train_acc: [0.9453], test_acc: [0.9387]\n",
      "Epoch: [ 0], batch: [  193/  468], training loss: [0.10448817], train_acc: [0.9609], test_acc: [0.9350]\n",
      "Epoch: [ 0], batch: [  194/  468], training loss: [0.25108251], train_acc: [0.9141], test_acc: [0.9359]\n",
      "Epoch: [ 0], batch: [  195/  468], training loss: [0.14159586], train_acc: [0.9453], test_acc: [0.9347]\n",
      "Epoch: [ 0], batch: [  196/  468], training loss: [0.19583108], train_acc: [0.9531], test_acc: [0.9339]\n",
      "Epoch: [ 0], batch: [  197/  468], training loss: [0.17371403], train_acc: [0.9453], test_acc: [0.9333]\n",
      "Epoch: [ 0], batch: [  198/  468], training loss: [0.11724833], train_acc: [0.9453], test_acc: [0.9341]\n",
      "Epoch: [ 0], batch: [  199/  468], training loss: [0.30824074], train_acc: [0.9453], test_acc: [0.9367]\n",
      "Epoch: [ 0], batch: [  200/  468], training loss: [0.16480130], train_acc: [0.9609], test_acc: [0.9390]\n",
      "Epoch: [ 0], batch: [  201/  468], training loss: [0.11243318], train_acc: [0.9688], test_acc: [0.9424]\n",
      "Epoch: [ 0], batch: [  202/  468], training loss: [0.14517444], train_acc: [0.9609], test_acc: [0.9451]\n",
      "Epoch: [ 0], batch: [  203/  468], training loss: [0.19226091], train_acc: [0.9531], test_acc: [0.9463]\n",
      "Epoch: [ 0], batch: [  204/  468], training loss: [0.35089931], train_acc: [0.8906], test_acc: [0.9462]\n",
      "Epoch: [ 0], batch: [  205/  468], training loss: [0.12640190], train_acc: [0.9688], test_acc: [0.9478]\n",
      "Epoch: [ 0], batch: [  206/  468], training loss: [0.16595334], train_acc: [0.9531], test_acc: [0.9469]\n",
      "Epoch: [ 0], batch: [  207/  468], training loss: [0.28657687], train_acc: [0.8984], test_acc: [0.9453]\n",
      "Epoch: [ 0], batch: [  208/  468], training loss: [0.15078306], train_acc: [0.9453], test_acc: [0.9444]\n",
      "Epoch: [ 0], batch: [  209/  468], training loss: [0.21692950], train_acc: [0.9453], test_acc: [0.9443]\n",
      "Epoch: [ 0], batch: [  210/  468], training loss: [0.19028156], train_acc: [0.9531], test_acc: [0.9456]\n",
      "Epoch: [ 0], batch: [  211/  468], training loss: [0.12718505], train_acc: [0.9688], test_acc: [0.9475]\n",
      "Epoch: [ 0], batch: [  212/  468], training loss: [0.13474295], train_acc: [0.9453], test_acc: [0.9478]\n",
      "Epoch: [ 0], batch: [  213/  468], training loss: [0.21631512], train_acc: [0.9453], test_acc: [0.9465]\n",
      "Epoch: [ 0], batch: [  214/  468], training loss: [0.08577981], train_acc: [0.9766], test_acc: [0.9461]\n",
      "Epoch: [ 0], batch: [  215/  468], training loss: [0.06307196], train_acc: [0.9922], test_acc: [0.9439]\n",
      "Epoch: [ 0], batch: [  216/  468], training loss: [0.18823773], train_acc: [0.9688], test_acc: [0.9421]\n",
      "Epoch: [ 0], batch: [  217/  468], training loss: [0.16511443], train_acc: [0.9688], test_acc: [0.9410]\n",
      "Epoch: [ 0], batch: [  218/  468], training loss: [0.13859707], train_acc: [0.9531], test_acc: [0.9408]\n",
      "Epoch: [ 0], batch: [  219/  468], training loss: [0.22834942], train_acc: [0.9297], test_acc: [0.9413]\n",
      "Epoch: [ 0], batch: [  220/  468], training loss: [0.16120112], train_acc: [0.9609], test_acc: [0.9417]\n",
      "Epoch: [ 0], batch: [  221/  468], training loss: [0.14426498], train_acc: [0.9453], test_acc: [0.9422]\n",
      "Epoch: [ 0], batch: [  222/  468], training loss: [0.17501745], train_acc: [0.9609], test_acc: [0.9429]\n",
      "Epoch: [ 0], batch: [  223/  468], training loss: [0.14358518], train_acc: [0.9609], test_acc: [0.9461]\n",
      "Epoch: [ 0], batch: [  224/  468], training loss: [0.23841864], train_acc: [0.9219], test_acc: [0.9467]\n",
      "Epoch: [ 0], batch: [  225/  468], training loss: [0.14299551], train_acc: [0.9609], test_acc: [0.9473]\n",
      "Epoch: [ 0], batch: [  226/  468], training loss: [0.24765232], train_acc: [0.9453], test_acc: [0.9482]\n",
      "Epoch: [ 0], batch: [  227/  468], training loss: [0.10221347], train_acc: [0.9688], test_acc: [0.9481]\n",
      "Epoch: [ 0], batch: [  228/  468], training loss: [0.11127976], train_acc: [0.9688], test_acc: [0.9470]\n",
      "Epoch: [ 0], batch: [  229/  468], training loss: [0.24534495], train_acc: [0.9375], test_acc: [0.9468]\n",
      "Epoch: [ 0], batch: [  230/  468], training loss: [0.14604698], train_acc: [0.9531], test_acc: [0.9467]\n",
      "Epoch: [ 0], batch: [  231/  468], training loss: [0.10437299], train_acc: [0.9922], test_acc: [0.9473]\n",
      "Epoch: [ 0], batch: [  232/  468], training loss: [0.16584261], train_acc: [0.9609], test_acc: [0.9469]\n",
      "Epoch: [ 0], batch: [  233/  468], training loss: [0.12230422], train_acc: [0.9766], test_acc: [0.9459]\n",
      "Epoch: [ 0], batch: [  234/  468], training loss: [0.19885163], train_acc: [0.9297], test_acc: [0.9468]\n",
      "Epoch: [ 0], batch: [  235/  468], training loss: [0.23163326], train_acc: [0.9375], test_acc: [0.9473]\n",
      "Epoch: [ 0], batch: [  236/  468], training loss: [0.07496587], train_acc: [0.9766], test_acc: [0.9482]\n",
      "Epoch: [ 0], batch: [  237/  468], training loss: [0.19722991], train_acc: [0.9297], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  238/  468], training loss: [0.11199819], train_acc: [0.9766], test_acc: [0.9485]\n",
      "Epoch: [ 0], batch: [  239/  468], training loss: [0.30134773], train_acc: [0.9141], test_acc: [0.9482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  240/  468], training loss: [0.15740809], train_acc: [0.9609], test_acc: [0.9496]\n",
      "Epoch: [ 0], batch: [  241/  468], training loss: [0.12527189], train_acc: [0.9531], test_acc: [0.9509]\n",
      "Epoch: [ 0], batch: [  242/  468], training loss: [0.13617249], train_acc: [0.9609], test_acc: [0.9511]\n",
      "Epoch: [ 0], batch: [  243/  468], training loss: [0.19819610], train_acc: [0.9531], test_acc: [0.9516]\n",
      "Epoch: [ 0], batch: [  244/  468], training loss: [0.07851814], train_acc: [0.9844], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  245/  468], training loss: [0.22366491], train_acc: [0.9453], test_acc: [0.9481]\n",
      "Epoch: [ 0], batch: [  246/  468], training loss: [0.23079696], train_acc: [0.9297], test_acc: [0.9465]\n",
      "Epoch: [ 0], batch: [  247/  468], training loss: [0.25784835], train_acc: [0.9219], test_acc: [0.9466]\n",
      "Epoch: [ 0], batch: [  248/  468], training loss: [0.18771413], train_acc: [0.9453], test_acc: [0.9464]\n",
      "Epoch: [ 0], batch: [  249/  468], training loss: [0.14308871], train_acc: [0.9688], test_acc: [0.9478]\n",
      "Epoch: [ 0], batch: [  250/  468], training loss: [0.21878295], train_acc: [0.9141], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  251/  468], training loss: [0.17172301], train_acc: [0.9531], test_acc: [0.9533]\n",
      "Epoch: [ 0], batch: [  252/  468], training loss: [0.20622674], train_acc: [0.9531], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  253/  468], training loss: [0.25820863], train_acc: [0.9297], test_acc: [0.9505]\n",
      "Epoch: [ 0], batch: [  254/  468], training loss: [0.18545943], train_acc: [0.9531], test_acc: [0.9495]\n",
      "Epoch: [ 0], batch: [  255/  468], training loss: [0.14124818], train_acc: [0.9531], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  256/  468], training loss: [0.09777968], train_acc: [0.9531], test_acc: [0.9497]\n",
      "Epoch: [ 0], batch: [  257/  468], training loss: [0.14009124], train_acc: [0.9609], test_acc: [0.9493]\n",
      "Epoch: [ 0], batch: [  258/  468], training loss: [0.18992528], train_acc: [0.9609], test_acc: [0.9496]\n",
      "Epoch: [ 0], batch: [  259/  468], training loss: [0.17011985], train_acc: [0.9609], test_acc: [0.9508]\n",
      "Epoch: [ 0], batch: [  260/  468], training loss: [0.23126952], train_acc: [0.9219], test_acc: [0.9512]\n",
      "Epoch: [ 0], batch: [  261/  468], training loss: [0.23974982], train_acc: [0.9375], test_acc: [0.9484]\n",
      "Epoch: [ 0], batch: [  262/  468], training loss: [0.20630890], train_acc: [0.9141], test_acc: [0.9465]\n",
      "Epoch: [ 0], batch: [  263/  468], training loss: [0.15870686], train_acc: [0.9609], test_acc: [0.9450]\n",
      "Epoch: [ 0], batch: [  264/  468], training loss: [0.12358148], train_acc: [0.9609], test_acc: [0.9437]\n",
      "Epoch: [ 0], batch: [  265/  468], training loss: [0.21259043], train_acc: [0.9297], test_acc: [0.9455]\n",
      "Epoch: [ 0], batch: [  266/  468], training loss: [0.21754950], train_acc: [0.9609], test_acc: [0.9476]\n",
      "Epoch: [ 0], batch: [  267/  468], training loss: [0.13115433], train_acc: [0.9688], test_acc: [0.9507]\n",
      "Epoch: [ 0], batch: [  268/  468], training loss: [0.06550513], train_acc: [0.9922], test_acc: [0.9528]\n",
      "Epoch: [ 0], batch: [  269/  468], training loss: [0.15104604], train_acc: [0.9609], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  270/  468], training loss: [0.17131746], train_acc: [0.9531], test_acc: [0.9502]\n",
      "Epoch: [ 0], batch: [  271/  468], training loss: [0.20255281], train_acc: [0.9609], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  272/  468], training loss: [0.08218886], train_acc: [0.9766], test_acc: [0.9484]\n",
      "Epoch: [ 0], batch: [  273/  468], training loss: [0.17614383], train_acc: [0.9453], test_acc: [0.9488]\n",
      "Epoch: [ 0], batch: [  274/  468], training loss: [0.14848739], train_acc: [0.9766], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  275/  468], training loss: [0.23878604], train_acc: [0.9141], test_acc: [0.9503]\n",
      "Epoch: [ 0], batch: [  276/  468], training loss: [0.11814874], train_acc: [0.9688], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  277/  468], training loss: [0.15525213], train_acc: [0.9609], test_acc: [0.9537]\n",
      "Epoch: [ 0], batch: [  278/  468], training loss: [0.18881354], train_acc: [0.9453], test_acc: [0.9533]\n",
      "Epoch: [ 0], batch: [  279/  468], training loss: [0.16645105], train_acc: [0.9531], test_acc: [0.9530]\n",
      "Epoch: [ 0], batch: [  280/  468], training loss: [0.16785210], train_acc: [0.9375], test_acc: [0.9526]\n",
      "Epoch: [ 0], batch: [  281/  468], training loss: [0.17703460], train_acc: [0.9453], test_acc: [0.9517]\n",
      "Epoch: [ 0], batch: [  282/  468], training loss: [0.12558685], train_acc: [0.9609], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  283/  468], training loss: [0.16613185], train_acc: [0.9453], test_acc: [0.9513]\n",
      "Epoch: [ 0], batch: [  284/  468], training loss: [0.21470645], train_acc: [0.9141], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  285/  468], training loss: [0.11730056], train_acc: [0.9688], test_acc: [0.9507]\n",
      "Epoch: [ 0], batch: [  286/  468], training loss: [0.19044316], train_acc: [0.9453], test_acc: [0.9499]\n",
      "Epoch: [ 0], batch: [  287/  468], training loss: [0.16075829], train_acc: [0.9609], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  288/  468], training loss: [0.19702320], train_acc: [0.9219], test_acc: [0.9502]\n",
      "Epoch: [ 0], batch: [  289/  468], training loss: [0.16665329], train_acc: [0.9609], test_acc: [0.9515]\n",
      "Epoch: [ 0], batch: [  290/  468], training loss: [0.15984008], train_acc: [0.9531], test_acc: [0.9520]\n",
      "Epoch: [ 0], batch: [  291/  468], training loss: [0.10839088], train_acc: [0.9609], test_acc: [0.9521]\n",
      "Epoch: [ 0], batch: [  292/  468], training loss: [0.15816295], train_acc: [0.9609], test_acc: [0.9517]\n",
      "Epoch: [ 0], batch: [  293/  468], training loss: [0.13775049], train_acc: [0.9531], test_acc: [0.9524]\n",
      "Epoch: [ 0], batch: [  294/  468], training loss: [0.17582688], train_acc: [0.9609], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  295/  468], training loss: [0.09155325], train_acc: [0.9688], test_acc: [0.9509]\n",
      "Epoch: [ 0], batch: [  296/  468], training loss: [0.16492665], train_acc: [0.9531], test_acc: [0.9512]\n",
      "Epoch: [ 0], batch: [  297/  468], training loss: [0.14766210], train_acc: [0.9688], test_acc: [0.9520]\n",
      "Epoch: [ 0], batch: [  298/  468], training loss: [0.20153002], train_acc: [0.9531], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  299/  468], training loss: [0.20787543], train_acc: [0.9453], test_acc: [0.9523]\n",
      "Epoch: [ 0], batch: [  300/  468], training loss: [0.18181342], train_acc: [0.9688], test_acc: [0.9526]\n",
      "Epoch: [ 0], batch: [  301/  468], training loss: [0.15923780], train_acc: [0.9375], test_acc: [0.9536]\n",
      "Epoch: [ 0], batch: [  302/  468], training loss: [0.10740934], train_acc: [0.9844], test_acc: [0.9525]\n",
      "Epoch: [ 0], batch: [  303/  468], training loss: [0.13493733], train_acc: [0.9688], test_acc: [0.9519]\n",
      "Epoch: [ 0], batch: [  304/  468], training loss: [0.12754145], train_acc: [0.9531], test_acc: [0.9504]\n",
      "Epoch: [ 0], batch: [  305/  468], training loss: [0.14519003], train_acc: [0.9453], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  306/  468], training loss: [0.15422425], train_acc: [0.9453], test_acc: [0.9507]\n",
      "Epoch: [ 0], batch: [  307/  468], training loss: [0.14776763], train_acc: [0.9453], test_acc: [0.9511]\n",
      "Epoch: [ 0], batch: [  308/  468], training loss: [0.17162031], train_acc: [0.9609], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  309/  468], training loss: [0.15635334], train_acc: [0.9609], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  310/  468], training loss: [0.09440958], train_acc: [0.9922], test_acc: [0.9492]\n",
      "Epoch: [ 0], batch: [  311/  468], training loss: [0.14894876], train_acc: [0.9609], test_acc: [0.9505]\n",
      "Epoch: [ 0], batch: [  312/  468], training loss: [0.10238995], train_acc: [0.9609], test_acc: [0.9527]\n",
      "Epoch: [ 0], batch: [  313/  468], training loss: [0.19296554], train_acc: [0.9375], test_acc: [0.9546]\n",
      "Epoch: [ 0], batch: [  314/  468], training loss: [0.17351612], train_acc: [0.9609], test_acc: [0.9567]\n",
      "Epoch: [ 0], batch: [  315/  468], training loss: [0.09637683], train_acc: [0.9766], test_acc: [0.9565]\n",
      "Epoch: [ 0], batch: [  316/  468], training loss: [0.08607146], train_acc: [0.9688], test_acc: [0.9561]\n",
      "Epoch: [ 0], batch: [  317/  468], training loss: [0.17198054], train_acc: [0.9453], test_acc: [0.9566]\n",
      "Epoch: [ 0], batch: [  318/  468], training loss: [0.20940536], train_acc: [0.9141], test_acc: [0.9561]\n",
      "Epoch: [ 0], batch: [  319/  468], training loss: [0.09142677], train_acc: [0.9766], test_acc: [0.9564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  320/  468], training loss: [0.14867315], train_acc: [0.9766], test_acc: [0.9552]\n",
      "Epoch: [ 0], batch: [  321/  468], training loss: [0.08820929], train_acc: [0.9766], test_acc: [0.9528]\n",
      "Epoch: [ 0], batch: [  322/  468], training loss: [0.16860336], train_acc: [0.9688], test_acc: [0.9501]\n",
      "Epoch: [ 0], batch: [  323/  468], training loss: [0.17229298], train_acc: [0.9375], test_acc: [0.9497]\n",
      "Epoch: [ 0], batch: [  324/  468], training loss: [0.13757636], train_acc: [0.9609], test_acc: [0.9509]\n",
      "Epoch: [ 0], batch: [  325/  468], training loss: [0.10418489], train_acc: [0.9609], test_acc: [0.9505]\n",
      "Epoch: [ 0], batch: [  326/  468], training loss: [0.20799762], train_acc: [0.9375], test_acc: [0.9527]\n",
      "Epoch: [ 0], batch: [  327/  468], training loss: [0.32851416], train_acc: [0.9219], test_acc: [0.9531]\n",
      "Epoch: [ 0], batch: [  328/  468], training loss: [0.20301177], train_acc: [0.9375], test_acc: [0.9537]\n",
      "Epoch: [ 0], batch: [  329/  468], training loss: [0.08824778], train_acc: [0.9609], test_acc: [0.9538]\n",
      "Epoch: [ 0], batch: [  330/  468], training loss: [0.10500370], train_acc: [0.9609], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  331/  468], training loss: [0.16307802], train_acc: [0.9531], test_acc: [0.9532]\n",
      "Epoch: [ 0], batch: [  332/  468], training loss: [0.15184158], train_acc: [0.9453], test_acc: [0.9545]\n",
      "Epoch: [ 0], batch: [  333/  468], training loss: [0.09338048], train_acc: [0.9609], test_acc: [0.9528]\n",
      "Epoch: [ 0], batch: [  334/  468], training loss: [0.18272012], train_acc: [0.9609], test_acc: [0.9541]\n",
      "Epoch: [ 0], batch: [  335/  468], training loss: [0.19316304], train_acc: [0.9375], test_acc: [0.9562]\n",
      "Epoch: [ 0], batch: [  336/  468], training loss: [0.09175015], train_acc: [0.9844], test_acc: [0.9574]\n",
      "Epoch: [ 0], batch: [  337/  468], training loss: [0.17935579], train_acc: [0.9453], test_acc: [0.9585]\n",
      "Epoch: [ 0], batch: [  338/  468], training loss: [0.16252159], train_acc: [0.9375], test_acc: [0.9577]\n",
      "Epoch: [ 0], batch: [  339/  468], training loss: [0.13507944], train_acc: [0.9609], test_acc: [0.9589]\n",
      "Epoch: [ 0], batch: [  340/  468], training loss: [0.10340554], train_acc: [0.9766], test_acc: [0.9608]\n",
      "Epoch: [ 0], batch: [  341/  468], training loss: [0.18017006], train_acc: [0.9531], test_acc: [0.9614]\n",
      "Epoch: [ 0], batch: [  342/  468], training loss: [0.11688604], train_acc: [0.9609], test_acc: [0.9605]\n",
      "Epoch: [ 0], batch: [  343/  468], training loss: [0.11997151], train_acc: [0.9531], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  344/  468], training loss: [0.15219000], train_acc: [0.9531], test_acc: [0.9602]\n",
      "Epoch: [ 0], batch: [  345/  468], training loss: [0.15445688], train_acc: [0.9297], test_acc: [0.9596]\n",
      "Epoch: [ 0], batch: [  346/  468], training loss: [0.23063934], train_acc: [0.9375], test_acc: [0.9607]\n",
      "Epoch: [ 0], batch: [  347/  468], training loss: [0.12735568], train_acc: [0.9688], test_acc: [0.9614]\n",
      "Epoch: [ 0], batch: [  348/  468], training loss: [0.15934125], train_acc: [0.9531], test_acc: [0.9617]\n",
      "Epoch: [ 0], batch: [  349/  468], training loss: [0.03971005], train_acc: [0.9922], test_acc: [0.9608]\n",
      "Epoch: [ 0], batch: [  350/  468], training loss: [0.07325833], train_acc: [0.9844], test_acc: [0.9606]\n",
      "Epoch: [ 0], batch: [  351/  468], training loss: [0.13268392], train_acc: [0.9688], test_acc: [0.9607]\n",
      "Epoch: [ 0], batch: [  352/  468], training loss: [0.10528830], train_acc: [0.9766], test_acc: [0.9606]\n",
      "Epoch: [ 0], batch: [  353/  468], training loss: [0.09198806], train_acc: [0.9688], test_acc: [0.9604]\n",
      "Epoch: [ 0], batch: [  354/  468], training loss: [0.10519119], train_acc: [0.9688], test_acc: [0.9599]\n",
      "Epoch: [ 0], batch: [  355/  468], training loss: [0.14944197], train_acc: [0.9688], test_acc: [0.9611]\n",
      "Epoch: [ 0], batch: [  356/  468], training loss: [0.12150541], train_acc: [0.9531], test_acc: [0.9616]\n",
      "Epoch: [ 0], batch: [  357/  468], training loss: [0.09220634], train_acc: [0.9844], test_acc: [0.9604]\n",
      "Epoch: [ 0], batch: [  358/  468], training loss: [0.06453342], train_acc: [0.9844], test_acc: [0.9595]\n",
      "Epoch: [ 0], batch: [  359/  468], training loss: [0.15852764], train_acc: [0.9453], test_acc: [0.9598]\n",
      "Epoch: [ 0], batch: [  360/  468], training loss: [0.16744547], train_acc: [0.9688], test_acc: [0.9604]\n",
      "Epoch: [ 0], batch: [  361/  468], training loss: [0.05541711], train_acc: [0.9922], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  362/  468], training loss: [0.08940996], train_acc: [0.9766], test_acc: [0.9591]\n",
      "Epoch: [ 0], batch: [  363/  468], training loss: [0.10382077], train_acc: [0.9688], test_acc: [0.9578]\n",
      "Epoch: [ 0], batch: [  364/  468], training loss: [0.16412300], train_acc: [0.9375], test_acc: [0.9573]\n",
      "Epoch: [ 0], batch: [  365/  468], training loss: [0.16392343], train_acc: [0.9531], test_acc: [0.9579]\n",
      "Epoch: [ 0], batch: [  366/  468], training loss: [0.10743403], train_acc: [0.9688], test_acc: [0.9587]\n",
      "Epoch: [ 0], batch: [  367/  468], training loss: [0.12264583], train_acc: [0.9609], test_acc: [0.9591]\n",
      "Epoch: [ 0], batch: [  368/  468], training loss: [0.17184162], train_acc: [0.9766], test_acc: [0.9588]\n",
      "Epoch: [ 0], batch: [  369/  468], training loss: [0.07582204], train_acc: [0.9766], test_acc: [0.9589]\n",
      "Epoch: [ 0], batch: [  370/  468], training loss: [0.11305918], train_acc: [0.9688], test_acc: [0.9587]\n",
      "Epoch: [ 0], batch: [  371/  468], training loss: [0.11253832], train_acc: [0.9688], test_acc: [0.9596]\n",
      "Epoch: [ 0], batch: [  372/  468], training loss: [0.10777031], train_acc: [0.9766], test_acc: [0.9601]\n",
      "Epoch: [ 0], batch: [  373/  468], training loss: [0.11190068], train_acc: [0.9688], test_acc: [0.9591]\n",
      "Epoch: [ 0], batch: [  374/  468], training loss: [0.25355998], train_acc: [0.9453], test_acc: [0.9591]\n",
      "Epoch: [ 0], batch: [  375/  468], training loss: [0.22323331], train_acc: [0.9375], test_acc: [0.9590]\n",
      "Epoch: [ 0], batch: [  376/  468], training loss: [0.14896877], train_acc: [0.9375], test_acc: [0.9595]\n",
      "Epoch: [ 0], batch: [  377/  468], training loss: [0.13750398], train_acc: [0.9531], test_acc: [0.9594]\n",
      "Epoch: [ 0], batch: [  378/  468], training loss: [0.12017205], train_acc: [0.9453], test_acc: [0.9591]\n",
      "Epoch: [ 0], batch: [  379/  468], training loss: [0.17328793], train_acc: [0.9375], test_acc: [0.9582]\n",
      "Epoch: [ 0], batch: [  380/  468], training loss: [0.11525019], train_acc: [0.9844], test_acc: [0.9578]\n",
      "Epoch: [ 0], batch: [  381/  468], training loss: [0.14106281], train_acc: [0.9688], test_acc: [0.9561]\n",
      "Epoch: [ 0], batch: [  382/  468], training loss: [0.11731471], train_acc: [0.9766], test_acc: [0.9552]\n",
      "Epoch: [ 0], batch: [  383/  468], training loss: [0.15229011], train_acc: [0.9531], test_acc: [0.9552]\n",
      "Epoch: [ 0], batch: [  384/  468], training loss: [0.10888306], train_acc: [0.9688], test_acc: [0.9553]\n",
      "Epoch: [ 0], batch: [  385/  468], training loss: [0.09843684], train_acc: [0.9609], test_acc: [0.9538]\n",
      "Epoch: [ 0], batch: [  386/  468], training loss: [0.10053033], train_acc: [0.9688], test_acc: [0.9531]\n",
      "Epoch: [ 0], batch: [  387/  468], training loss: [0.15979341], train_acc: [0.9609], test_acc: [0.9558]\n",
      "Epoch: [ 0], batch: [  388/  468], training loss: [0.21849829], train_acc: [0.9453], test_acc: [0.9594]\n",
      "Epoch: [ 0], batch: [  389/  468], training loss: [0.16313580], train_acc: [0.9531], test_acc: [0.9613]\n",
      "Epoch: [ 0], batch: [  390/  468], training loss: [0.09239776], train_acc: [0.9609], test_acc: [0.9612]\n",
      "Epoch: [ 0], batch: [  391/  468], training loss: [0.16278175], train_acc: [0.9297], test_acc: [0.9621]\n",
      "Epoch: [ 0], batch: [  392/  468], training loss: [0.07888329], train_acc: [0.9922], test_acc: [0.9619]\n",
      "Epoch: [ 0], batch: [  393/  468], training loss: [0.09451649], train_acc: [0.9688], test_acc: [0.9621]\n",
      "Epoch: [ 0], batch: [  394/  468], training loss: [0.11648588], train_acc: [0.9688], test_acc: [0.9615]\n",
      "Epoch: [ 0], batch: [  395/  468], training loss: [0.05076561], train_acc: [0.9922], test_acc: [0.9618]\n",
      "Epoch: [ 0], batch: [  396/  468], training loss: [0.07997103], train_acc: [0.9766], test_acc: [0.9617]\n",
      "Epoch: [ 0], batch: [  397/  468], training loss: [0.08560297], train_acc: [0.9844], test_acc: [0.9594]\n",
      "Epoch: [ 0], batch: [  398/  468], training loss: [0.08981841], train_acc: [0.9688], test_acc: [0.9604]\n",
      "Epoch: [ 0], batch: [  399/  468], training loss: [0.16916047], train_acc: [0.9453], test_acc: [0.9607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  400/  468], training loss: [0.10698613], train_acc: [0.9609], test_acc: [0.9609]\n",
      "Epoch: [ 0], batch: [  401/  468], training loss: [0.12795018], train_acc: [0.9688], test_acc: [0.9604]\n",
      "Epoch: [ 0], batch: [  402/  468], training loss: [0.21068734], train_acc: [0.9375], test_acc: [0.9621]\n",
      "Epoch: [ 0], batch: [  403/  468], training loss: [0.11180699], train_acc: [0.9844], test_acc: [0.9626]\n",
      "Epoch: [ 0], batch: [  404/  468], training loss: [0.12529993], train_acc: [0.9609], test_acc: [0.9638]\n",
      "Epoch: [ 0], batch: [  405/  468], training loss: [0.08565387], train_acc: [0.9688], test_acc: [0.9644]\n",
      "Epoch: [ 0], batch: [  406/  468], training loss: [0.12075074], train_acc: [0.9766], test_acc: [0.9638]\n",
      "Epoch: [ 0], batch: [  407/  468], training loss: [0.06087184], train_acc: [0.9922], test_acc: [0.9635]\n",
      "Epoch: [ 0], batch: [  408/  468], training loss: [0.08995792], train_acc: [0.9688], test_acc: [0.9630]\n",
      "Epoch: [ 0], batch: [  409/  468], training loss: [0.06866360], train_acc: [0.9688], test_acc: [0.9622]\n",
      "Epoch: [ 0], batch: [  410/  468], training loss: [0.05904418], train_acc: [0.9766], test_acc: [0.9612]\n",
      "Epoch: [ 0], batch: [  411/  468], training loss: [0.08844722], train_acc: [0.9766], test_acc: [0.9601]\n",
      "Epoch: [ 0], batch: [  412/  468], training loss: [0.14572637], train_acc: [0.9609], test_acc: [0.9600]\n",
      "Epoch: [ 0], batch: [  413/  468], training loss: [0.19195928], train_acc: [0.9375], test_acc: [0.9602]\n",
      "Epoch: [ 0], batch: [  414/  468], training loss: [0.15718162], train_acc: [0.9609], test_acc: [0.9596]\n",
      "Epoch: [ 0], batch: [  415/  468], training loss: [0.14836919], train_acc: [0.9375], test_acc: [0.9587]\n",
      "Epoch: [ 0], batch: [  416/  468], training loss: [0.25532159], train_acc: [0.9219], test_acc: [0.9586]\n",
      "Epoch: [ 0], batch: [  417/  468], training loss: [0.09207111], train_acc: [0.9766], test_acc: [0.9602]\n",
      "Epoch: [ 0], batch: [  418/  468], training loss: [0.16791570], train_acc: [0.9531], test_acc: [0.9606]\n",
      "Epoch: [ 0], batch: [  419/  468], training loss: [0.05898343], train_acc: [0.9844], test_acc: [0.9608]\n",
      "Epoch: [ 0], batch: [  420/  468], training loss: [0.22142601], train_acc: [0.9375], test_acc: [0.9616]\n",
      "Epoch: [ 0], batch: [  421/  468], training loss: [0.15512986], train_acc: [0.9531], test_acc: [0.9610]\n",
      "Epoch: [ 0], batch: [  422/  468], training loss: [0.05540237], train_acc: [0.9844], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  423/  468], training loss: [0.06845987], train_acc: [0.9844], test_acc: [0.9590]\n",
      "Epoch: [ 0], batch: [  424/  468], training loss: [0.11036398], train_acc: [0.9688], test_acc: [0.9596]\n",
      "Epoch: [ 0], batch: [  425/  468], training loss: [0.09045954], train_acc: [0.9922], test_acc: [0.9612]\n",
      "Epoch: [ 0], batch: [  426/  468], training loss: [0.17485970], train_acc: [0.9531], test_acc: [0.9615]\n",
      "Epoch: [ 0], batch: [  427/  468], training loss: [0.08687735], train_acc: [0.9609], test_acc: [0.9606]\n",
      "Epoch: [ 0], batch: [  428/  468], training loss: [0.17019963], train_acc: [0.9688], test_acc: [0.9614]\n",
      "Epoch: [ 0], batch: [  429/  468], training loss: [0.15230814], train_acc: [0.9531], test_acc: [0.9617]\n",
      "Epoch: [ 0], batch: [  430/  468], training loss: [0.12539004], train_acc: [0.9766], test_acc: [0.9609]\n",
      "Epoch: [ 0], batch: [  431/  468], training loss: [0.13092226], train_acc: [0.9688], test_acc: [0.9610]\n",
      "Epoch: [ 0], batch: [  432/  468], training loss: [0.12382855], train_acc: [0.9531], test_acc: [0.9605]\n",
      "Epoch: [ 0], batch: [  433/  468], training loss: [0.14265081], train_acc: [0.9609], test_acc: [0.9600]\n",
      "Epoch: [ 0], batch: [  434/  468], training loss: [0.10760643], train_acc: [0.9688], test_acc: [0.9585]\n",
      "Epoch: [ 0], batch: [  435/  468], training loss: [0.15828228], train_acc: [0.9531], test_acc: [0.9585]\n",
      "Epoch: [ 0], batch: [  436/  468], training loss: [0.13256964], train_acc: [0.9609], test_acc: [0.9599]\n",
      "Epoch: [ 0], batch: [  437/  468], training loss: [0.15137523], train_acc: [0.9688], test_acc: [0.9611]\n",
      "Epoch: [ 0], batch: [  438/  468], training loss: [0.08643716], train_acc: [0.9766], test_acc: [0.9615]\n",
      "Epoch: [ 0], batch: [  439/  468], training loss: [0.11083344], train_acc: [0.9844], test_acc: [0.9621]\n",
      "Epoch: [ 0], batch: [  440/  468], training loss: [0.13242745], train_acc: [0.9609], test_acc: [0.9612]\n",
      "Epoch: [ 0], batch: [  441/  468], training loss: [0.21050766], train_acc: [0.9609], test_acc: [0.9612]\n",
      "Epoch: [ 0], batch: [  442/  468], training loss: [0.11732259], train_acc: [0.9609], test_acc: [0.9608]\n",
      "Epoch: [ 0], batch: [  443/  468], training loss: [0.04893475], train_acc: [0.9766], test_acc: [0.9614]\n",
      "Epoch: [ 0], batch: [  444/  468], training loss: [0.14164364], train_acc: [0.9609], test_acc: [0.9616]\n",
      "Epoch: [ 0], batch: [  445/  468], training loss: [0.10203567], train_acc: [0.9766], test_acc: [0.9623]\n",
      "Epoch: [ 0], batch: [  446/  468], training loss: [0.11047081], train_acc: [0.9766], test_acc: [0.9619]\n",
      "Epoch: [ 0], batch: [  447/  468], training loss: [0.09451405], train_acc: [0.9688], test_acc: [0.9606]\n",
      "Epoch: [ 0], batch: [  448/  468], training loss: [0.14825819], train_acc: [0.9609], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  449/  468], training loss: [0.23528551], train_acc: [0.9531], test_acc: [0.9608]\n",
      "Epoch: [ 0], batch: [  450/  468], training loss: [0.25443667], train_acc: [0.9297], test_acc: [0.9634]\n",
      "Epoch: [ 0], batch: [  451/  468], training loss: [0.20533462], train_acc: [0.9609], test_acc: [0.9642]\n",
      "Epoch: [ 0], batch: [  452/  468], training loss: [0.09437019], train_acc: [0.9688], test_acc: [0.9650]\n",
      "Epoch: [ 0], batch: [  453/  468], training loss: [0.09917577], train_acc: [0.9688], test_acc: [0.9642]\n",
      "Epoch: [ 0], batch: [  454/  468], training loss: [0.11605173], train_acc: [0.9609], test_acc: [0.9635]\n",
      "Epoch: [ 0], batch: [  455/  468], training loss: [0.09660263], train_acc: [0.9531], test_acc: [0.9625]\n",
      "Epoch: [ 0], batch: [  456/  468], training loss: [0.06738299], train_acc: [0.9766], test_acc: [0.9604]\n",
      "Epoch: [ 0], batch: [  457/  468], training loss: [0.06945875], train_acc: [0.9922], test_acc: [0.9595]\n",
      "Epoch: [ 0], batch: [  458/  468], training loss: [0.05512560], train_acc: [0.9844], test_acc: [0.9603]\n",
      "Epoch: [ 0], batch: [  459/  468], training loss: [0.05057315], train_acc: [0.9922], test_acc: [0.9617]\n",
      "Epoch: [ 0], batch: [  460/  468], training loss: [0.12258491], train_acc: [0.9531], test_acc: [0.9623]\n",
      "Epoch: [ 0], batch: [  461/  468], training loss: [0.13284731], train_acc: [0.9688], test_acc: [0.9630]\n",
      "Epoch: [ 0], batch: [  462/  468], training loss: [0.13148858], train_acc: [0.9453], test_acc: [0.9640]\n",
      "Epoch: [ 0], batch: [  463/  468], training loss: [0.07434417], train_acc: [0.9766], test_acc: [0.9608]\n",
      "Epoch: [ 0], batch: [  464/  468], training loss: [0.13010348], train_acc: [0.9453], test_acc: [0.9606]\n",
      "Epoch: [ 0], batch: [  465/  468], training loss: [0.06303605], train_acc: [0.9844], test_acc: [0.9603]\n",
      "Epoch: [ 0], batch: [  466/  468], training loss: [0.14923479], train_acc: [0.9609], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  467/  468], training loss: [0.08283052], train_acc: [0.9766], test_acc: [0.9585]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    for idx, (train_input, train_label) in enumerate(train_dataset):\n",
    "        grads = grad(network, train_input, train_label)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "        \n",
    "        train_loss = loss_fn(network, train_input, train_label)\n",
    "        train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "        \n",
    "        for test_input, test_label in test_dataset:\n",
    "            test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "       \n",
    "        print(\"Epoch: [%2d], batch: [%5d/%5d], training loss: [%.8f], train_acc: [%.4f], test_acc: [%.4f]\"\\\n",
    "              %(epoch, idx, training_iterations,train_loss, train_accuracy,test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565789d3",
   "metadata": {},
   "source": [
    "가중치 초기화 방식만을 개선했음에도 test정확도가 크게 증가함을 볼 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a96e4",
   "metadata": {},
   "source": [
    "![initializer.png](https://t1.daumcdn.net/cfile/tistory/991D9B425B4305D34F)\n",
    "초기화 방식은 이밖에 다수 존재하며, 아직까지는 어떤 방식이 절대적으로 우위를 갖는다고는 말할 수 없습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8769878",
   "metadata": {},
   "source": [
    "## Dropout을 통한 성능 향상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5771f63",
   "metadata": {},
   "source": [
    "### 학습내용 \n",
    "* Dropout은 과적합을 방지하기 위해 하나의 네트워크를 구성하는 일정 비율의 노드를 비활성화 시키는 방법을 말합니다. \n",
    "\n",
    "##### 가중치의 갯수와 Overfitting \n",
    "![modelComplexity_overfitting.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQJo_BKmazScjXhO4xY-J-XdfEVA6YPIlo3zrTCUxZmDj8N5OBbSeL0QLoIKCbWKb5HvEg&usqp=CAU)\n",
    "* 일반적으로 모형이 복잡해질수록, 즉 모형의 layer가 깊어지거나 넓어져 가중치의 갯수가 증가할 수록, 과적합의 발생 가능성이 높아집니다. \n",
    "![exam_over.png](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99E3E23F5C3459982A) \n",
    "* 예를들어, 위의 그림과 같이 모형의 복잡도가 올라갈수록, 모형이 도출하는 Hyper_plane(초평면)이 train data에 과도하게 맞춰져 test데이터를 일반적으로 맞출수 없게 됩니다. 따라서, 우리는 이러한 현상을 막기 위해 DropOut 기법을 활용합니다. \n",
    "![Dropout.png](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-16841-4_21/MediaObjects/481859_1_En_21_Fig1_HTML.png)\n",
    "* Dropout은 학습과정에서, 매 단계마다 랜덤하게 일부의 노드를 비활성화 시켜 학습을 진행시킨 이후에, 모형을 취합시켜 예측을 수행하는 방식을 의미합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953799e",
   "metadata": {},
   "source": [
    "### 코드를 통한 구현 \n",
    "* 이번에도 마찬가지로 checkpoint를 남기지 않고 진행하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f35703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# 연산을 위한 라이브러리 \n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "# 레이블 원핫인코딩 \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터셋 \n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "675ec7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 호출 함수 \n",
    "# 상세사항은 위에서 정리한것을 참조합니다. \n",
    "def load_mnist():\n",
    "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "    \n",
    "    train_data = np.expand_dims(train_data, axis=-1)\n",
    "    test_data = np.expand_dims(test_data, axis =-1)\n",
    "    \n",
    "    train_data = train_data.astype(np.float32)/255.0\n",
    "    test_data = test_data.astype(np.float32)/255.0\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd48cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 정의 \n",
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training = True)\n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits,\n",
    "                                                                  y_true=labels,\n",
    "                                                                  from_logits = True))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13d417b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 함수 정의 \n",
    "def accuracy_fn(model, images, labels):\n",
    "    logits = model(images, training= False)\n",
    "    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
    "    acc = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2d005",
   "metadata": {},
   "source": [
    "##### Training 옵션을 통한 Dropout의 적용 \n",
    "* 텐서플로우를 통한 model에서 training = True 옵션 을 지정하면 Dropout층이 활성화 되어, Dropout을 실행한다. \n",
    "* 하지만, test과정에서는 모든 노드를 동원하여 예측하여야 하기 때문에, training옵션에 False를 지정하면 Dropout이 자동적으로 비활성화 된다. \n",
    "\n",
    "* 이를 low level에서 활용하는 방식은 다음 4.5에서 포함하여 다룰 예정이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a2f5a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 추적 함수 \n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d3d9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 구성 층 함수 \n",
    "\n",
    "def flatten() :\n",
    "    return tf.keras.layers.Flatten()\n",
    "\n",
    "def dense(label_dim, weight_init) :\n",
    "    return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init)\n",
    "\n",
    "def relu() :\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.relu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94c911",
   "metadata": {},
   "source": [
    "##### Dropout층 \n",
    "* rate: Float between 0 and 1. Fraction of the input units to drop.  \n",
    "  - rate옵션은 0~1 사이의 값으로 만약 0.2 입력시 매번 수행마다 자동적으로 20%의 값이 드롭아웃된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6522624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(rate) :\n",
    "    return tf.keras.layers.Dropout(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c6c32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b35a5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model (class version)\n",
    "# class create_model_class(tf.keras.Model):\n",
    "#     def __init__(self, label_dim):\n",
    "#         super(create_model_class, self).__init__()\n",
    "#         weight_init = tf.keras.initializers.glorot_uniform()\n",
    "\n",
    "#         self.model = tf.keras.Sequential()\n",
    "#         self.model.add(flatten())\n",
    "\n",
    "#         for i in range(4):\n",
    "#             self.model.add(dense(512, weight_init))\n",
    "#             self.model.add(relu())\n",
    "#             self.model.add(dropout(rate=0.5))\n",
    "\n",
    "#         self.model.add(dense(label_dim, weight_init))\n",
    "\n",
    "#     def call(self, x, training=None, mask=None):\n",
    "\n",
    "#         x = self.model(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352fb98",
   "metadata": {},
   "source": [
    "##### 모형 생성시  Dropout층 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1684de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_function(label_dim) :\n",
    "    weight_init = tf.keras.initializers.glorot_uniform()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(flatten())\n",
    "\n",
    "    for i in range(4) :\n",
    "        model.add(dense(512, weight_init))\n",
    "        model.add(relu())\n",
    "        model.add(dropout(rate=0.5))\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c14f7e",
   "metadata": {},
   "source": [
    "* 각 layer에서 Dropout층의 위치는 다양한 방식으로 사용되지만, 아래와 같은 방식이 가장 많이 사용된다.\n",
    "    - 1. layer >>> activation >>> Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "408b5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "train_x, train_y, test_x, test_y = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53d75b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" parameters \"\"\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "training_epochs = 1\n",
    "training_iterations = len(train_x) // batch_size\n",
    "\n",
    "label_dim = 10\n",
    "\n",
    "train_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "06b487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Graph Input using Dataset API \"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "378d4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "network = create_model_function(label_dim)\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5518a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [    0/  468], training loss: [2.36862159], train_acc: [0.1719], test_acc: [0.1038]\n",
      "Epoch: [ 0], batch: [    1/  468], training loss: [2.31953859], train_acc: [0.0625], test_acc: [0.1047]\n",
      "Epoch: [ 0], batch: [    2/  468], training loss: [2.32758689], train_acc: [0.1484], test_acc: [0.1425]\n",
      "Epoch: [ 0], batch: [    3/  468], training loss: [2.21729946], train_acc: [0.2734], test_acc: [0.2186]\n",
      "Epoch: [ 0], batch: [    4/  468], training loss: [2.21575451], train_acc: [0.2969], test_acc: [0.2907]\n",
      "Epoch: [ 0], batch: [    5/  468], training loss: [2.30938387], train_acc: [0.3359], test_acc: [0.3030]\n",
      "Epoch: [ 0], batch: [    6/  468], training loss: [2.33364058], train_acc: [0.3672], test_acc: [0.3406]\n",
      "Epoch: [ 0], batch: [    7/  468], training loss: [2.26559496], train_acc: [0.4141], test_acc: [0.3951]\n",
      "Epoch: [ 0], batch: [    8/  468], training loss: [2.23806000], train_acc: [0.4609], test_acc: [0.4547]\n",
      "Epoch: [ 0], batch: [    9/  468], training loss: [2.14988804], train_acc: [0.4844], test_acc: [0.4913]\n",
      "Epoch: [ 0], batch: [   10/  468], training loss: [2.19646907], train_acc: [0.3984], test_acc: [0.5347]\n",
      "Epoch: [ 0], batch: [   11/  468], training loss: [2.08794665], train_acc: [0.5938], test_acc: [0.5642]\n",
      "Epoch: [ 0], batch: [   12/  468], training loss: [2.01690388], train_acc: [0.6562], test_acc: [0.5941]\n",
      "Epoch: [ 0], batch: [   13/  468], training loss: [2.04234838], train_acc: [0.5312], test_acc: [0.6016]\n",
      "Epoch: [ 0], batch: [   14/  468], training loss: [2.05861044], train_acc: [0.5391], test_acc: [0.5988]\n",
      "Epoch: [ 0], batch: [   15/  468], training loss: [1.91704857], train_acc: [0.5938], test_acc: [0.5976]\n",
      "Epoch: [ 0], batch: [   16/  468], training loss: [1.85489464], train_acc: [0.6406], test_acc: [0.5829]\n",
      "Epoch: [ 0], batch: [   17/  468], training loss: [1.87143469], train_acc: [0.6172], test_acc: [0.5834]\n",
      "Epoch: [ 0], batch: [   18/  468], training loss: [1.92319334], train_acc: [0.5312], test_acc: [0.5983]\n",
      "Epoch: [ 0], batch: [   19/  468], training loss: [1.83770204], train_acc: [0.6406], test_acc: [0.6029]\n",
      "Epoch: [ 0], batch: [   20/  468], training loss: [1.81122756], train_acc: [0.5781], test_acc: [0.6146]\n",
      "Epoch: [ 0], batch: [   21/  468], training loss: [1.68999290], train_acc: [0.6250], test_acc: [0.6119]\n",
      "Epoch: [ 0], batch: [   22/  468], training loss: [1.70373726], train_acc: [0.6094], test_acc: [0.6221]\n",
      "Epoch: [ 0], batch: [   23/  468], training loss: [1.62134957], train_acc: [0.6016], test_acc: [0.6392]\n",
      "Epoch: [ 0], batch: [   24/  468], training loss: [1.45152199], train_acc: [0.6562], test_acc: [0.6573]\n",
      "Epoch: [ 0], batch: [   25/  468], training loss: [1.37952304], train_acc: [0.6641], test_acc: [0.6714]\n",
      "Epoch: [ 0], batch: [   26/  468], training loss: [1.46342349], train_acc: [0.6562], test_acc: [0.6824]\n",
      "Epoch: [ 0], batch: [   27/  468], training loss: [1.62249684], train_acc: [0.6172], test_acc: [0.6914]\n",
      "Epoch: [ 0], batch: [   28/  468], training loss: [1.42798090], train_acc: [0.6953], test_acc: [0.7055]\n",
      "Epoch: [ 0], batch: [   29/  468], training loss: [1.20172858], train_acc: [0.7578], test_acc: [0.7122]\n",
      "Epoch: [ 0], batch: [   30/  468], training loss: [1.47425389], train_acc: [0.6641], test_acc: [0.7179]\n",
      "Epoch: [ 0], batch: [   31/  468], training loss: [1.17103171], train_acc: [0.7656], test_acc: [0.7384]\n",
      "Epoch: [ 0], batch: [   32/  468], training loss: [1.23336339], train_acc: [0.7266], test_acc: [0.7603]\n",
      "Epoch: [ 0], batch: [   33/  468], training loss: [1.09809709], train_acc: [0.7656], test_acc: [0.7827]\n",
      "Epoch: [ 0], batch: [   34/  468], training loss: [1.13820314], train_acc: [0.7891], test_acc: [0.7973]\n",
      "Epoch: [ 0], batch: [   35/  468], training loss: [0.96080291], train_acc: [0.7734], test_acc: [0.7940]\n",
      "Epoch: [ 0], batch: [   36/  468], training loss: [1.21753407], train_acc: [0.7500], test_acc: [0.7872]\n",
      "Epoch: [ 0], batch: [   37/  468], training loss: [1.05022573], train_acc: [0.7891], test_acc: [0.7859]\n",
      "Epoch: [ 0], batch: [   38/  468], training loss: [0.87813520], train_acc: [0.8203], test_acc: [0.7910]\n",
      "Epoch: [ 0], batch: [   39/  468], training loss: [1.00711393], train_acc: [0.8281], test_acc: [0.7950]\n",
      "Epoch: [ 0], batch: [   40/  468], training loss: [1.15490365], train_acc: [0.7734], test_acc: [0.7978]\n",
      "Epoch: [ 0], batch: [   41/  468], training loss: [1.00416279], train_acc: [0.7344], test_acc: [0.7963]\n",
      "Epoch: [ 0], batch: [   42/  468], training loss: [0.87888384], train_acc: [0.8438], test_acc: [0.7950]\n",
      "Epoch: [ 0], batch: [   43/  468], training loss: [0.91032922], train_acc: [0.7734], test_acc: [0.7947]\n",
      "Epoch: [ 0], batch: [   44/  468], training loss: [0.95843184], train_acc: [0.7812], test_acc: [0.7985]\n",
      "Epoch: [ 0], batch: [   45/  468], training loss: [0.88873553], train_acc: [0.7969], test_acc: [0.8140]\n",
      "Epoch: [ 0], batch: [   46/  468], training loss: [0.87512112], train_acc: [0.8516], test_acc: [0.8303]\n",
      "Epoch: [ 0], batch: [   47/  468], training loss: [1.01144671], train_acc: [0.8281], test_acc: [0.8372]\n",
      "Epoch: [ 0], batch: [   48/  468], training loss: [0.81761062], train_acc: [0.8828], test_acc: [0.8385]\n",
      "Epoch: [ 0], batch: [   49/  468], training loss: [0.84188163], train_acc: [0.8125], test_acc: [0.8377]\n",
      "Epoch: [ 0], batch: [   50/  468], training loss: [0.66876125], train_acc: [0.8594], test_acc: [0.8364]\n",
      "Epoch: [ 0], batch: [   51/  468], training loss: [0.75592571], train_acc: [0.8516], test_acc: [0.8329]\n",
      "Epoch: [ 0], batch: [   52/  468], training loss: [0.74144936], train_acc: [0.8359], test_acc: [0.8326]\n",
      "Epoch: [ 0], batch: [   53/  468], training loss: [0.73241627], train_acc: [0.8672], test_acc: [0.8343]\n",
      "Epoch: [ 0], batch: [   54/  468], training loss: [0.69369566], train_acc: [0.8281], test_acc: [0.8425]\n",
      "Epoch: [ 0], batch: [   55/  468], training loss: [0.65423912], train_acc: [0.8594], test_acc: [0.8505]\n",
      "Epoch: [ 0], batch: [   56/  468], training loss: [0.69029188], train_acc: [0.8828], test_acc: [0.8525]\n",
      "Epoch: [ 0], batch: [   57/  468], training loss: [0.63537854], train_acc: [0.8438], test_acc: [0.8532]\n",
      "Epoch: [ 0], batch: [   58/  468], training loss: [0.76320207], train_acc: [0.8672], test_acc: [0.8548]\n",
      "Epoch: [ 0], batch: [   59/  468], training loss: [0.77856892], train_acc: [0.8828], test_acc: [0.8566]\n",
      "Epoch: [ 0], batch: [   60/  468], training loss: [0.72264588], train_acc: [0.8125], test_acc: [0.8621]\n",
      "Epoch: [ 0], batch: [   61/  468], training loss: [0.73510325], train_acc: [0.9062], test_acc: [0.8682]\n",
      "Epoch: [ 0], batch: [   62/  468], training loss: [0.62038660], train_acc: [0.8672], test_acc: [0.8711]\n",
      "Epoch: [ 0], batch: [   63/  468], training loss: [0.80957150], train_acc: [0.8438], test_acc: [0.8772]\n",
      "Epoch: [ 0], batch: [   64/  468], training loss: [0.57230425], train_acc: [0.9141], test_acc: [0.8796]\n",
      "Epoch: [ 0], batch: [   65/  468], training loss: [0.72065324], train_acc: [0.8516], test_acc: [0.8820]\n",
      "Epoch: [ 0], batch: [   66/  468], training loss: [0.50572169], train_acc: [0.8984], test_acc: [0.8832]\n",
      "Epoch: [ 0], batch: [   67/  468], training loss: [0.64568156], train_acc: [0.8906], test_acc: [0.8804]\n",
      "Epoch: [ 0], batch: [   68/  468], training loss: [0.47617644], train_acc: [0.9141], test_acc: [0.8795]\n",
      "Epoch: [ 0], batch: [   69/  468], training loss: [0.49102485], train_acc: [0.9141], test_acc: [0.8790]\n",
      "Epoch: [ 0], batch: [   70/  468], training loss: [0.54290062], train_acc: [0.8750], test_acc: [0.8814]\n",
      "Epoch: [ 0], batch: [   71/  468], training loss: [0.53628516], train_acc: [0.9141], test_acc: [0.8801]\n",
      "Epoch: [ 0], batch: [   72/  468], training loss: [0.61713541], train_acc: [0.8828], test_acc: [0.8823]\n",
      "Epoch: [ 0], batch: [   73/  468], training loss: [0.50892591], train_acc: [0.8984], test_acc: [0.8819]\n",
      "Epoch: [ 0], batch: [   74/  468], training loss: [0.55002373], train_acc: [0.8594], test_acc: [0.8827]\n",
      "Epoch: [ 0], batch: [   75/  468], training loss: [0.68073118], train_acc: [0.8750], test_acc: [0.8852]\n",
      "Epoch: [ 0], batch: [   76/  468], training loss: [0.53137141], train_acc: [0.8750], test_acc: [0.8837]\n",
      "Epoch: [ 0], batch: [   77/  468], training loss: [0.53593034], train_acc: [0.8672], test_acc: [0.8861]\n",
      "Epoch: [ 0], batch: [   78/  468], training loss: [0.59617996], train_acc: [0.8984], test_acc: [0.8932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [   79/  468], training loss: [0.57022578], train_acc: [0.8984], test_acc: [0.8962]\n",
      "Epoch: [ 0], batch: [   80/  468], training loss: [0.40485448], train_acc: [0.9453], test_acc: [0.8986]\n",
      "Epoch: [ 0], batch: [   81/  468], training loss: [0.48055804], train_acc: [0.8828], test_acc: [0.8997]\n",
      "Epoch: [ 0], batch: [   82/  468], training loss: [0.56918019], train_acc: [0.8594], test_acc: [0.9008]\n",
      "Epoch: [ 0], batch: [   83/  468], training loss: [0.51864123], train_acc: [0.8750], test_acc: [0.9010]\n",
      "Epoch: [ 0], batch: [   84/  468], training loss: [0.56237912], train_acc: [0.9141], test_acc: [0.9039]\n",
      "Epoch: [ 0], batch: [   85/  468], training loss: [0.53297913], train_acc: [0.8750], test_acc: [0.9062]\n",
      "Epoch: [ 0], batch: [   86/  468], training loss: [0.52222234], train_acc: [0.8906], test_acc: [0.9063]\n",
      "Epoch: [ 0], batch: [   87/  468], training loss: [0.51880038], train_acc: [0.8828], test_acc: [0.9041]\n",
      "Epoch: [ 0], batch: [   88/  468], training loss: [0.47013259], train_acc: [0.9062], test_acc: [0.9023]\n",
      "Epoch: [ 0], batch: [   89/  468], training loss: [0.50566924], train_acc: [0.8672], test_acc: [0.9022]\n",
      "Epoch: [ 0], batch: [   90/  468], training loss: [0.58346748], train_acc: [0.9375], test_acc: [0.9027]\n",
      "Epoch: [ 0], batch: [   91/  468], training loss: [0.58400619], train_acc: [0.8672], test_acc: [0.9034]\n",
      "Epoch: [ 0], batch: [   92/  468], training loss: [0.51635557], train_acc: [0.8750], test_acc: [0.9039]\n",
      "Epoch: [ 0], batch: [   93/  468], training loss: [0.53332269], train_acc: [0.9062], test_acc: [0.9047]\n",
      "Epoch: [ 0], batch: [   94/  468], training loss: [0.57339495], train_acc: [0.8672], test_acc: [0.9052]\n",
      "Epoch: [ 0], batch: [   95/  468], training loss: [0.60114717], train_acc: [0.8750], test_acc: [0.9062]\n",
      "Epoch: [ 0], batch: [   96/  468], training loss: [0.44464025], train_acc: [0.9141], test_acc: [0.9076]\n",
      "Epoch: [ 0], batch: [   97/  468], training loss: [0.46033537], train_acc: [0.9219], test_acc: [0.9086]\n",
      "Epoch: [ 0], batch: [   98/  468], training loss: [0.39948779], train_acc: [0.8984], test_acc: [0.9084]\n",
      "Epoch: [ 0], batch: [   99/  468], training loss: [0.32597029], train_acc: [0.9766], test_acc: [0.9096]\n",
      "Epoch: [ 0], batch: [  100/  468], training loss: [0.42820898], train_acc: [0.8594], test_acc: [0.9102]\n",
      "Epoch: [ 0], batch: [  101/  468], training loss: [0.47658557], train_acc: [0.9062], test_acc: [0.9114]\n",
      "Epoch: [ 0], batch: [  102/  468], training loss: [0.48468038], train_acc: [0.9141], test_acc: [0.9125]\n",
      "Epoch: [ 0], batch: [  103/  468], training loss: [0.40507954], train_acc: [0.9453], test_acc: [0.9125]\n",
      "Epoch: [ 0], batch: [  104/  468], training loss: [0.54264408], train_acc: [0.8984], test_acc: [0.9142]\n",
      "Epoch: [ 0], batch: [  105/  468], training loss: [0.39960623], train_acc: [0.9297], test_acc: [0.9162]\n",
      "Epoch: [ 0], batch: [  106/  468], training loss: [0.55870712], train_acc: [0.9141], test_acc: [0.9159]\n",
      "Epoch: [ 0], batch: [  107/  468], training loss: [0.41523647], train_acc: [0.8906], test_acc: [0.9171]\n",
      "Epoch: [ 0], batch: [  108/  468], training loss: [0.46419784], train_acc: [0.8828], test_acc: [0.9170]\n",
      "Epoch: [ 0], batch: [  109/  468], training loss: [0.41933978], train_acc: [0.9219], test_acc: [0.9174]\n",
      "Epoch: [ 0], batch: [  110/  468], training loss: [0.52590412], train_acc: [0.8594], test_acc: [0.9180]\n",
      "Epoch: [ 0], batch: [  111/  468], training loss: [0.54203057], train_acc: [0.9141], test_acc: [0.9181]\n",
      "Epoch: [ 0], batch: [  112/  468], training loss: [0.43206042], train_acc: [0.9062], test_acc: [0.9169]\n",
      "Epoch: [ 0], batch: [  113/  468], training loss: [0.45853454], train_acc: [0.9219], test_acc: [0.9159]\n",
      "Epoch: [ 0], batch: [  114/  468], training loss: [0.38672140], train_acc: [0.8984], test_acc: [0.9164]\n",
      "Epoch: [ 0], batch: [  115/  468], training loss: [0.41767824], train_acc: [0.9141], test_acc: [0.9158]\n",
      "Epoch: [ 0], batch: [  116/  468], training loss: [0.40230215], train_acc: [0.9375], test_acc: [0.9152]\n",
      "Epoch: [ 0], batch: [  117/  468], training loss: [0.61039889], train_acc: [0.9219], test_acc: [0.9146]\n",
      "Epoch: [ 0], batch: [  118/  468], training loss: [0.43964648], train_acc: [0.8984], test_acc: [0.9147]\n",
      "Epoch: [ 0], batch: [  119/  468], training loss: [0.43573040], train_acc: [0.8906], test_acc: [0.9138]\n",
      "Epoch: [ 0], batch: [  120/  468], training loss: [0.56405151], train_acc: [0.8672], test_acc: [0.9144]\n",
      "Epoch: [ 0], batch: [  121/  468], training loss: [0.50831640], train_acc: [0.8828], test_acc: [0.9163]\n",
      "Epoch: [ 0], batch: [  122/  468], training loss: [0.43735024], train_acc: [0.8906], test_acc: [0.9177]\n",
      "Epoch: [ 0], batch: [  123/  468], training loss: [0.43726271], train_acc: [0.9062], test_acc: [0.9186]\n",
      "Epoch: [ 0], batch: [  124/  468], training loss: [0.46850723], train_acc: [0.9062], test_acc: [0.9191]\n",
      "Epoch: [ 0], batch: [  125/  468], training loss: [0.28281575], train_acc: [0.9297], test_acc: [0.9186]\n",
      "Epoch: [ 0], batch: [  126/  468], training loss: [0.32910419], train_acc: [0.9375], test_acc: [0.9172]\n",
      "Epoch: [ 0], batch: [  127/  468], training loss: [0.38873237], train_acc: [0.9219], test_acc: [0.9162]\n",
      "Epoch: [ 0], batch: [  128/  468], training loss: [0.37882319], train_acc: [0.9219], test_acc: [0.9176]\n",
      "Epoch: [ 0], batch: [  129/  468], training loss: [0.29964691], train_acc: [0.8984], test_acc: [0.9214]\n",
      "Epoch: [ 0], batch: [  130/  468], training loss: [0.38429922], train_acc: [0.9453], test_acc: [0.9237]\n",
      "Epoch: [ 0], batch: [  131/  468], training loss: [0.47021407], train_acc: [0.8984], test_acc: [0.9267]\n",
      "Epoch: [ 0], batch: [  132/  468], training loss: [0.52935743], train_acc: [0.8672], test_acc: [0.9267]\n",
      "Epoch: [ 0], batch: [  133/  468], training loss: [0.28371027], train_acc: [0.9375], test_acc: [0.9282]\n",
      "Epoch: [ 0], batch: [  134/  468], training loss: [0.36999124], train_acc: [0.9453], test_acc: [0.9271]\n",
      "Epoch: [ 0], batch: [  135/  468], training loss: [0.36717921], train_acc: [0.9453], test_acc: [0.9262]\n",
      "Epoch: [ 0], batch: [  136/  468], training loss: [0.39193425], train_acc: [0.9375], test_acc: [0.9225]\n",
      "Epoch: [ 0], batch: [  137/  468], training loss: [0.41128600], train_acc: [0.9141], test_acc: [0.9186]\n",
      "Epoch: [ 0], batch: [  138/  468], training loss: [0.49238285], train_acc: [0.9219], test_acc: [0.9180]\n",
      "Epoch: [ 0], batch: [  139/  468], training loss: [0.44749242], train_acc: [0.9062], test_acc: [0.9194]\n",
      "Epoch: [ 0], batch: [  140/  468], training loss: [0.27875224], train_acc: [0.9297], test_acc: [0.9215]\n",
      "Epoch: [ 0], batch: [  141/  468], training loss: [0.42830789], train_acc: [0.9453], test_acc: [0.9234]\n",
      "Epoch: [ 0], batch: [  142/  468], training loss: [0.24659005], train_acc: [0.9531], test_acc: [0.9235]\n",
      "Epoch: [ 0], batch: [  143/  468], training loss: [0.52544630], train_acc: [0.9062], test_acc: [0.9248]\n",
      "Epoch: [ 0], batch: [  144/  468], training loss: [0.40169454], train_acc: [0.9141], test_acc: [0.9231]\n",
      "Epoch: [ 0], batch: [  145/  468], training loss: [0.40666765], train_acc: [0.9297], test_acc: [0.9224]\n",
      "Epoch: [ 0], batch: [  146/  468], training loss: [0.41854206], train_acc: [0.8750], test_acc: [0.9216]\n",
      "Epoch: [ 0], batch: [  147/  468], training loss: [0.44623950], train_acc: [0.9141], test_acc: [0.9198]\n",
      "Epoch: [ 0], batch: [  148/  468], training loss: [0.34132412], train_acc: [0.9375], test_acc: [0.9205]\n",
      "Epoch: [ 0], batch: [  149/  468], training loss: [0.27605343], train_acc: [0.9297], test_acc: [0.9233]\n",
      "Epoch: [ 0], batch: [  150/  468], training loss: [0.47578403], train_acc: [0.9141], test_acc: [0.9251]\n",
      "Epoch: [ 0], batch: [  151/  468], training loss: [0.28322771], train_acc: [0.9453], test_acc: [0.9286]\n",
      "Epoch: [ 0], batch: [  152/  468], training loss: [0.22948666], train_acc: [0.9453], test_acc: [0.9312]\n",
      "Epoch: [ 0], batch: [  153/  468], training loss: [0.33591130], train_acc: [0.9219], test_acc: [0.9320]\n",
      "Epoch: [ 0], batch: [  154/  468], training loss: [0.29486775], train_acc: [0.9531], test_acc: [0.9327]\n",
      "Epoch: [ 0], batch: [  155/  468], training loss: [0.32883868], train_acc: [0.9453], test_acc: [0.9301]\n",
      "Epoch: [ 0], batch: [  156/  468], training loss: [0.30960581], train_acc: [0.9219], test_acc: [0.9276]\n",
      "Epoch: [ 0], batch: [  157/  468], training loss: [0.47475493], train_acc: [0.8984], test_acc: [0.9279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  158/  468], training loss: [0.33273947], train_acc: [0.9297], test_acc: [0.9275]\n",
      "Epoch: [ 0], batch: [  159/  468], training loss: [0.29465669], train_acc: [0.8984], test_acc: [0.9275]\n",
      "Epoch: [ 0], batch: [  160/  468], training loss: [0.45888755], train_acc: [0.8984], test_acc: [0.9275]\n",
      "Epoch: [ 0], batch: [  161/  468], training loss: [0.38518536], train_acc: [0.9219], test_acc: [0.9264]\n",
      "Epoch: [ 0], batch: [  162/  468], training loss: [0.69223487], train_acc: [0.8750], test_acc: [0.9280]\n",
      "Epoch: [ 0], batch: [  163/  468], training loss: [0.42777279], train_acc: [0.9453], test_acc: [0.9290]\n",
      "Epoch: [ 0], batch: [  164/  468], training loss: [0.49097046], train_acc: [0.9297], test_acc: [0.9274]\n",
      "Epoch: [ 0], batch: [  165/  468], training loss: [0.33284491], train_acc: [0.9141], test_acc: [0.9258]\n",
      "Epoch: [ 0], batch: [  166/  468], training loss: [0.50880986], train_acc: [0.9219], test_acc: [0.9239]\n",
      "Epoch: [ 0], batch: [  167/  468], training loss: [0.55757934], train_acc: [0.9141], test_acc: [0.9219]\n",
      "Epoch: [ 0], batch: [  168/  468], training loss: [0.55448961], train_acc: [0.9141], test_acc: [0.9242]\n",
      "Epoch: [ 0], batch: [  169/  468], training loss: [0.48538241], train_acc: [0.9062], test_acc: [0.9284]\n",
      "Epoch: [ 0], batch: [  170/  468], training loss: [0.29841185], train_acc: [0.9531], test_acc: [0.9324]\n",
      "Epoch: [ 0], batch: [  171/  468], training loss: [0.44683617], train_acc: [0.9141], test_acc: [0.9345]\n",
      "Epoch: [ 0], batch: [  172/  468], training loss: [0.32599330], train_acc: [0.9375], test_acc: [0.9355]\n",
      "Epoch: [ 0], batch: [  173/  468], training loss: [0.51742256], train_acc: [0.9141], test_acc: [0.9351]\n",
      "Epoch: [ 0], batch: [  174/  468], training loss: [0.29396766], train_acc: [0.9453], test_acc: [0.9320]\n",
      "Epoch: [ 0], batch: [  175/  468], training loss: [0.41894555], train_acc: [0.8906], test_acc: [0.9288]\n",
      "Epoch: [ 0], batch: [  176/  468], training loss: [0.32776576], train_acc: [0.9531], test_acc: [0.9296]\n",
      "Epoch: [ 0], batch: [  177/  468], training loss: [0.22169366], train_acc: [0.9531], test_acc: [0.9300]\n",
      "Epoch: [ 0], batch: [  178/  468], training loss: [0.38450730], train_acc: [0.9531], test_acc: [0.9303]\n",
      "Epoch: [ 0], batch: [  179/  468], training loss: [0.51594222], train_acc: [0.9062], test_acc: [0.9333]\n",
      "Epoch: [ 0], batch: [  180/  468], training loss: [0.37692517], train_acc: [0.9219], test_acc: [0.9357]\n",
      "Epoch: [ 0], batch: [  181/  468], training loss: [0.61541224], train_acc: [0.8906], test_acc: [0.9369]\n",
      "Epoch: [ 0], batch: [  182/  468], training loss: [0.25817236], train_acc: [0.9453], test_acc: [0.9356]\n",
      "Epoch: [ 0], batch: [  183/  468], training loss: [0.34982753], train_acc: [0.9766], test_acc: [0.9332]\n",
      "Epoch: [ 0], batch: [  184/  468], training loss: [0.26060459], train_acc: [0.9453], test_acc: [0.9301]\n",
      "Epoch: [ 0], batch: [  185/  468], training loss: [0.33884674], train_acc: [0.9531], test_acc: [0.9304]\n",
      "Epoch: [ 0], batch: [  186/  468], training loss: [0.38517046], train_acc: [0.9453], test_acc: [0.9299]\n",
      "Epoch: [ 0], batch: [  187/  468], training loss: [0.40141851], train_acc: [0.9219], test_acc: [0.9300]\n",
      "Epoch: [ 0], batch: [  188/  468], training loss: [0.30028740], train_acc: [0.9297], test_acc: [0.9314]\n",
      "Epoch: [ 0], batch: [  189/  468], training loss: [0.34099096], train_acc: [0.9531], test_acc: [0.9292]\n",
      "Epoch: [ 0], batch: [  190/  468], training loss: [0.36256915], train_acc: [0.9141], test_acc: [0.9280]\n",
      "Epoch: [ 0], batch: [  191/  468], training loss: [0.31752101], train_acc: [0.9219], test_acc: [0.9284]\n",
      "Epoch: [ 0], batch: [  192/  468], training loss: [0.22457914], train_acc: [0.9531], test_acc: [0.9285]\n",
      "Epoch: [ 0], batch: [  193/  468], training loss: [0.32118309], train_acc: [0.9297], test_acc: [0.9286]\n",
      "Epoch: [ 0], batch: [  194/  468], training loss: [0.20418072], train_acc: [0.9531], test_acc: [0.9289]\n",
      "Epoch: [ 0], batch: [  195/  468], training loss: [0.25261438], train_acc: [0.9453], test_acc: [0.9289]\n",
      "Epoch: [ 0], batch: [  196/  468], training loss: [0.19839096], train_acc: [0.9297], test_acc: [0.9300]\n",
      "Epoch: [ 0], batch: [  197/  468], training loss: [0.32386619], train_acc: [0.9219], test_acc: [0.9319]\n",
      "Epoch: [ 0], batch: [  198/  468], training loss: [0.25508130], train_acc: [0.9531], test_acc: [0.9335]\n",
      "Epoch: [ 0], batch: [  199/  468], training loss: [0.32310089], train_acc: [0.9219], test_acc: [0.9346]\n",
      "Epoch: [ 0], batch: [  200/  468], training loss: [0.33591124], train_acc: [0.9375], test_acc: [0.9346]\n",
      "Epoch: [ 0], batch: [  201/  468], training loss: [0.32327911], train_acc: [0.9219], test_acc: [0.9350]\n",
      "Epoch: [ 0], batch: [  202/  468], training loss: [0.29157671], train_acc: [0.9531], test_acc: [0.9348]\n",
      "Epoch: [ 0], batch: [  203/  468], training loss: [0.22674274], train_acc: [0.9609], test_acc: [0.9341]\n",
      "Epoch: [ 0], batch: [  204/  468], training loss: [0.29353225], train_acc: [0.9531], test_acc: [0.9326]\n",
      "Epoch: [ 0], batch: [  205/  468], training loss: [0.25640687], train_acc: [0.9688], test_acc: [0.9324]\n",
      "Epoch: [ 0], batch: [  206/  468], training loss: [0.41498798], train_acc: [0.9297], test_acc: [0.9327]\n",
      "Epoch: [ 0], batch: [  207/  468], training loss: [0.26129594], train_acc: [0.9297], test_acc: [0.9326]\n",
      "Epoch: [ 0], batch: [  208/  468], training loss: [0.33140063], train_acc: [0.9375], test_acc: [0.9337]\n",
      "Epoch: [ 0], batch: [  209/  468], training loss: [0.22665465], train_acc: [0.9531], test_acc: [0.9358]\n",
      "Epoch: [ 0], batch: [  210/  468], training loss: [0.37814319], train_acc: [0.9141], test_acc: [0.9366]\n",
      "Epoch: [ 0], batch: [  211/  468], training loss: [0.38540244], train_acc: [0.9297], test_acc: [0.9377]\n",
      "Epoch: [ 0], batch: [  212/  468], training loss: [0.38764346], train_acc: [0.9297], test_acc: [0.9395]\n",
      "Epoch: [ 0], batch: [  213/  468], training loss: [0.30583474], train_acc: [0.9531], test_acc: [0.9404]\n",
      "Epoch: [ 0], batch: [  214/  468], training loss: [0.34606725], train_acc: [0.9297], test_acc: [0.9396]\n",
      "Epoch: [ 0], batch: [  215/  468], training loss: [0.49505651], train_acc: [0.9219], test_acc: [0.9392]\n",
      "Epoch: [ 0], batch: [  216/  468], training loss: [0.32886738], train_acc: [0.9375], test_acc: [0.9372]\n",
      "Epoch: [ 0], batch: [  217/  468], training loss: [0.39874426], train_acc: [0.9062], test_acc: [0.9358]\n",
      "Epoch: [ 0], batch: [  218/  468], training loss: [0.38587517], train_acc: [0.9375], test_acc: [0.9341]\n",
      "Epoch: [ 0], batch: [  219/  468], training loss: [0.50688565], train_acc: [0.9141], test_acc: [0.9340]\n",
      "Epoch: [ 0], batch: [  220/  468], training loss: [0.29731351], train_acc: [0.9297], test_acc: [0.9361]\n",
      "Epoch: [ 0], batch: [  221/  468], training loss: [0.28512955], train_acc: [0.9375], test_acc: [0.9375]\n",
      "Epoch: [ 0], batch: [  222/  468], training loss: [0.33559284], train_acc: [0.9219], test_acc: [0.9387]\n",
      "Epoch: [ 0], batch: [  223/  468], training loss: [0.39196327], train_acc: [0.9531], test_acc: [0.9395]\n",
      "Epoch: [ 0], batch: [  224/  468], training loss: [0.37595561], train_acc: [0.9453], test_acc: [0.9396]\n",
      "Epoch: [ 0], batch: [  225/  468], training loss: [0.45941418], train_acc: [0.9141], test_acc: [0.9387]\n",
      "Epoch: [ 0], batch: [  226/  468], training loss: [0.25008318], train_acc: [0.9453], test_acc: [0.9397]\n",
      "Epoch: [ 0], batch: [  227/  468], training loss: [0.23891197], train_acc: [0.9766], test_acc: [0.9393]\n",
      "Epoch: [ 0], batch: [  228/  468], training loss: [0.27976960], train_acc: [0.9453], test_acc: [0.9394]\n",
      "Epoch: [ 0], batch: [  229/  468], training loss: [0.24011698], train_acc: [0.9688], test_acc: [0.9398]\n",
      "Epoch: [ 0], batch: [  230/  468], training loss: [0.30417132], train_acc: [0.9375], test_acc: [0.9410]\n",
      "Epoch: [ 0], batch: [  231/  468], training loss: [0.31905186], train_acc: [0.9531], test_acc: [0.9426]\n",
      "Epoch: [ 0], batch: [  232/  468], training loss: [0.23518550], train_acc: [0.9453], test_acc: [0.9438]\n",
      "Epoch: [ 0], batch: [  233/  468], training loss: [0.27879277], train_acc: [0.9453], test_acc: [0.9443]\n",
      "Epoch: [ 0], batch: [  234/  468], training loss: [0.45398107], train_acc: [0.9297], test_acc: [0.9455]\n",
      "Epoch: [ 0], batch: [  235/  468], training loss: [0.47568357], train_acc: [0.9219], test_acc: [0.9464]\n",
      "Epoch: [ 0], batch: [  236/  468], training loss: [0.28250942], train_acc: [0.9375], test_acc: [0.9456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  237/  468], training loss: [0.44048688], train_acc: [0.9141], test_acc: [0.9455]\n",
      "Epoch: [ 0], batch: [  238/  468], training loss: [0.31967866], train_acc: [0.9297], test_acc: [0.9449]\n",
      "Epoch: [ 0], batch: [  239/  468], training loss: [0.42216760], train_acc: [0.9219], test_acc: [0.9441]\n",
      "Epoch: [ 0], batch: [  240/  468], training loss: [0.18395206], train_acc: [0.9531], test_acc: [0.9426]\n",
      "Epoch: [ 0], batch: [  241/  468], training loss: [0.48267773], train_acc: [0.8906], test_acc: [0.9428]\n",
      "Epoch: [ 0], batch: [  242/  468], training loss: [0.30560219], train_acc: [0.9609], test_acc: [0.9449]\n",
      "Epoch: [ 0], batch: [  243/  468], training loss: [0.40665999], train_acc: [0.9141], test_acc: [0.9440]\n",
      "Epoch: [ 0], batch: [  244/  468], training loss: [0.38220143], train_acc: [0.9375], test_acc: [0.9434]\n",
      "Epoch: [ 0], batch: [  245/  468], training loss: [0.33570322], train_acc: [0.9453], test_acc: [0.9412]\n",
      "Epoch: [ 0], batch: [  246/  468], training loss: [0.18978798], train_acc: [0.9609], test_acc: [0.9408]\n",
      "Epoch: [ 0], batch: [  247/  468], training loss: [0.30921856], train_acc: [0.9453], test_acc: [0.9400]\n",
      "Epoch: [ 0], batch: [  248/  468], training loss: [0.45983562], train_acc: [0.9141], test_acc: [0.9421]\n",
      "Epoch: [ 0], batch: [  249/  468], training loss: [0.35030690], train_acc: [0.9531], test_acc: [0.9428]\n",
      "Epoch: [ 0], batch: [  250/  468], training loss: [0.35842121], train_acc: [0.9141], test_acc: [0.9435]\n",
      "Epoch: [ 0], batch: [  251/  468], training loss: [0.28485042], train_acc: [0.9453], test_acc: [0.9443]\n",
      "Epoch: [ 0], batch: [  252/  468], training loss: [0.27560917], train_acc: [0.9688], test_acc: [0.9424]\n",
      "Epoch: [ 0], batch: [  253/  468], training loss: [0.38449064], train_acc: [0.9375], test_acc: [0.9426]\n",
      "Epoch: [ 0], batch: [  254/  468], training loss: [0.25374168], train_acc: [0.9297], test_acc: [0.9427]\n",
      "Epoch: [ 0], batch: [  255/  468], training loss: [0.27861458], train_acc: [0.9453], test_acc: [0.9427]\n",
      "Epoch: [ 0], batch: [  256/  468], training loss: [0.23055597], train_acc: [0.9531], test_acc: [0.9423]\n",
      "Epoch: [ 0], batch: [  257/  468], training loss: [0.27247432], train_acc: [0.9609], test_acc: [0.9416]\n",
      "Epoch: [ 0], batch: [  258/  468], training loss: [0.24794284], train_acc: [0.9453], test_acc: [0.9414]\n",
      "Epoch: [ 0], batch: [  259/  468], training loss: [0.23484197], train_acc: [0.9609], test_acc: [0.9424]\n",
      "Epoch: [ 0], batch: [  260/  468], training loss: [0.30849004], train_acc: [0.9453], test_acc: [0.9431]\n",
      "Epoch: [ 0], batch: [  261/  468], training loss: [0.45363688], train_acc: [0.9375], test_acc: [0.9435]\n",
      "Epoch: [ 0], batch: [  262/  468], training loss: [0.25684589], train_acc: [0.9375], test_acc: [0.9421]\n",
      "Epoch: [ 0], batch: [  263/  468], training loss: [0.37184176], train_acc: [0.9453], test_acc: [0.9410]\n",
      "Epoch: [ 0], batch: [  264/  468], training loss: [0.29250032], train_acc: [0.9531], test_acc: [0.9385]\n",
      "Epoch: [ 0], batch: [  265/  468], training loss: [0.37439695], train_acc: [0.9219], test_acc: [0.9361]\n",
      "Epoch: [ 0], batch: [  266/  468], training loss: [0.20954853], train_acc: [0.9531], test_acc: [0.9346]\n",
      "Epoch: [ 0], batch: [  267/  468], training loss: [0.39021897], train_acc: [0.9375], test_acc: [0.9344]\n",
      "Epoch: [ 0], batch: [  268/  468], training loss: [0.37958869], train_acc: [0.9141], test_acc: [0.9360]\n",
      "Epoch: [ 0], batch: [  269/  468], training loss: [0.40991974], train_acc: [0.9141], test_acc: [0.9372]\n",
      "Epoch: [ 0], batch: [  270/  468], training loss: [0.43555114], train_acc: [0.9219], test_acc: [0.9401]\n",
      "Epoch: [ 0], batch: [  271/  468], training loss: [0.22705796], train_acc: [0.9609], test_acc: [0.9419]\n",
      "Epoch: [ 0], batch: [  272/  468], training loss: [0.35297430], train_acc: [0.9531], test_acc: [0.9424]\n",
      "Epoch: [ 0], batch: [  273/  468], training loss: [0.21872181], train_acc: [0.9766], test_acc: [0.9445]\n",
      "Epoch: [ 0], batch: [  274/  468], training loss: [0.33314496], train_acc: [0.9219], test_acc: [0.9458]\n",
      "Epoch: [ 0], batch: [  275/  468], training loss: [0.31134489], train_acc: [0.9688], test_acc: [0.9458]\n",
      "Epoch: [ 0], batch: [  276/  468], training loss: [0.39183623], train_acc: [0.9219], test_acc: [0.9459]\n",
      "Epoch: [ 0], batch: [  277/  468], training loss: [0.42954308], train_acc: [0.9453], test_acc: [0.9463]\n",
      "Epoch: [ 0], batch: [  278/  468], training loss: [0.33969647], train_acc: [0.9453], test_acc: [0.9450]\n",
      "Epoch: [ 0], batch: [  279/  468], training loss: [0.39597255], train_acc: [0.9141], test_acc: [0.9466]\n",
      "Epoch: [ 0], batch: [  280/  468], training loss: [0.21960962], train_acc: [0.9688], test_acc: [0.9478]\n",
      "Epoch: [ 0], batch: [  281/  468], training loss: [0.33578515], train_acc: [0.9453], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  282/  468], training loss: [0.26962939], train_acc: [0.9531], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  283/  468], training loss: [0.25323433], train_acc: [0.9375], test_acc: [0.9487]\n",
      "Epoch: [ 0], batch: [  284/  468], training loss: [0.28840834], train_acc: [0.9297], test_acc: [0.9485]\n",
      "Epoch: [ 0], batch: [  285/  468], training loss: [0.39445347], train_acc: [0.9141], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  286/  468], training loss: [0.40818954], train_acc: [0.9141], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  287/  468], training loss: [0.15726659], train_acc: [0.9766], test_acc: [0.9479]\n",
      "Epoch: [ 0], batch: [  288/  468], training loss: [0.20836741], train_acc: [0.9531], test_acc: [0.9464]\n",
      "Epoch: [ 0], batch: [  289/  468], training loss: [0.26073760], train_acc: [0.9609], test_acc: [0.9456]\n",
      "Epoch: [ 0], batch: [  290/  468], training loss: [0.41026592], train_acc: [0.9219], test_acc: [0.9459]\n",
      "Epoch: [ 0], batch: [  291/  468], training loss: [0.21400021], train_acc: [0.9688], test_acc: [0.9464]\n",
      "Epoch: [ 0], batch: [  292/  468], training loss: [0.43317902], train_acc: [0.9062], test_acc: [0.9462]\n",
      "Epoch: [ 0], batch: [  293/  468], training loss: [0.30071887], train_acc: [0.9219], test_acc: [0.9464]\n",
      "Epoch: [ 0], batch: [  294/  468], training loss: [0.53991300], train_acc: [0.9062], test_acc: [0.9476]\n",
      "Epoch: [ 0], batch: [  295/  468], training loss: [0.25850070], train_acc: [0.9766], test_acc: [0.9494]\n",
      "Epoch: [ 0], batch: [  296/  468], training loss: [0.35574335], train_acc: [0.9297], test_acc: [0.9523]\n",
      "Epoch: [ 0], batch: [  297/  468], training loss: [0.23095168], train_acc: [0.9688], test_acc: [0.9516]\n",
      "Epoch: [ 0], batch: [  298/  468], training loss: [0.36457425], train_acc: [0.9375], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  299/  468], training loss: [0.33739486], train_acc: [0.9375], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  300/  468], training loss: [0.34036988], train_acc: [0.9375], test_acc: [0.9516]\n",
      "Epoch: [ 0], batch: [  301/  468], training loss: [0.31377125], train_acc: [0.9219], test_acc: [0.9518]\n",
      "Epoch: [ 0], batch: [  302/  468], training loss: [0.30290008], train_acc: [0.9531], test_acc: [0.9516]\n",
      "Epoch: [ 0], batch: [  303/  468], training loss: [0.19999862], train_acc: [0.9688], test_acc: [0.9508]\n",
      "Epoch: [ 0], batch: [  304/  468], training loss: [0.15585497], train_acc: [0.9766], test_acc: [0.9501]\n",
      "Epoch: [ 0], batch: [  305/  468], training loss: [0.20062937], train_acc: [0.9609], test_acc: [0.9496]\n",
      "Epoch: [ 0], batch: [  306/  468], training loss: [0.18814632], train_acc: [0.9766], test_acc: [0.9492]\n",
      "Epoch: [ 0], batch: [  307/  468], training loss: [0.24654329], train_acc: [0.9531], test_acc: [0.9486]\n",
      "Epoch: [ 0], batch: [  308/  468], training loss: [0.44826674], train_acc: [0.9297], test_acc: [0.9487]\n",
      "Epoch: [ 0], batch: [  309/  468], training loss: [0.16658224], train_acc: [0.9609], test_acc: [0.9483]\n",
      "Epoch: [ 0], batch: [  310/  468], training loss: [0.34712285], train_acc: [0.9531], test_acc: [0.9507]\n",
      "Epoch: [ 0], batch: [  311/  468], training loss: [0.36394712], train_acc: [0.9375], test_acc: [0.9509]\n",
      "Epoch: [ 0], batch: [  312/  468], training loss: [0.27377170], train_acc: [0.9531], test_acc: [0.9502]\n",
      "Epoch: [ 0], batch: [  313/  468], training loss: [0.31200883], train_acc: [0.9531], test_acc: [0.9497]\n",
      "Epoch: [ 0], batch: [  314/  468], training loss: [0.21296479], train_acc: [0.9609], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  315/  468], training loss: [0.50952661], train_acc: [0.9453], test_acc: [0.9479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  316/  468], training loss: [0.47668028], train_acc: [0.9297], test_acc: [0.9479]\n",
      "Epoch: [ 0], batch: [  317/  468], training loss: [0.19672750], train_acc: [0.9688], test_acc: [0.9484]\n",
      "Epoch: [ 0], batch: [  318/  468], training loss: [0.36621720], train_acc: [0.9453], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  319/  468], training loss: [0.16380998], train_acc: [0.9688], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  320/  468], training loss: [0.21746318], train_acc: [0.9531], test_acc: [0.9484]\n",
      "Epoch: [ 0], batch: [  321/  468], training loss: [0.18252905], train_acc: [0.9609], test_acc: [0.9496]\n",
      "Epoch: [ 0], batch: [  322/  468], training loss: [0.31927973], train_acc: [0.9453], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  323/  468], training loss: [0.23017788], train_acc: [0.9219], test_acc: [0.9493]\n",
      "Epoch: [ 0], batch: [  324/  468], training loss: [0.28302592], train_acc: [0.9297], test_acc: [0.9497]\n",
      "Epoch: [ 0], batch: [  325/  468], training loss: [0.22512150], train_acc: [0.9688], test_acc: [0.9515]\n",
      "Epoch: [ 0], batch: [  326/  468], training loss: [0.32147044], train_acc: [0.9531], test_acc: [0.9510]\n",
      "Epoch: [ 0], batch: [  327/  468], training loss: [0.25321034], train_acc: [0.9531], test_acc: [0.9513]\n",
      "Epoch: [ 0], batch: [  328/  468], training loss: [0.12904245], train_acc: [0.9922], test_acc: [0.9511]\n",
      "Epoch: [ 0], batch: [  329/  468], training loss: [0.41248554], train_acc: [0.9375], test_acc: [0.9499]\n",
      "Epoch: [ 0], batch: [  330/  468], training loss: [0.13784374], train_acc: [0.9766], test_acc: [0.9493]\n",
      "Epoch: [ 0], batch: [  331/  468], training loss: [0.30067128], train_acc: [0.9531], test_acc: [0.9485]\n",
      "Epoch: [ 0], batch: [  332/  468], training loss: [0.23997620], train_acc: [0.9688], test_acc: [0.9479]\n",
      "Epoch: [ 0], batch: [  333/  468], training loss: [0.17441836], train_acc: [0.9766], test_acc: [0.9480]\n",
      "Epoch: [ 0], batch: [  334/  468], training loss: [0.24482478], train_acc: [0.9375], test_acc: [0.9478]\n",
      "Epoch: [ 0], batch: [  335/  468], training loss: [0.25740677], train_acc: [0.9531], test_acc: [0.9488]\n",
      "Epoch: [ 0], batch: [  336/  468], training loss: [0.27815443], train_acc: [0.9297], test_acc: [0.9488]\n",
      "Epoch: [ 0], batch: [  337/  468], training loss: [0.19739822], train_acc: [0.9531], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  338/  468], training loss: [0.22001281], train_acc: [0.9688], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  339/  468], training loss: [0.13633376], train_acc: [0.9766], test_acc: [0.9487]\n",
      "Epoch: [ 0], batch: [  340/  468], training loss: [0.17877948], train_acc: [0.9609], test_acc: [0.9485]\n",
      "Epoch: [ 0], batch: [  341/  468], training loss: [0.20418929], train_acc: [0.9688], test_acc: [0.9486]\n",
      "Epoch: [ 0], batch: [  342/  468], training loss: [0.33076015], train_acc: [0.9531], test_acc: [0.9485]\n",
      "Epoch: [ 0], batch: [  343/  468], training loss: [0.23588009], train_acc: [0.9531], test_acc: [0.9495]\n",
      "Epoch: [ 0], batch: [  344/  468], training loss: [0.19172698], train_acc: [0.9688], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  345/  468], training loss: [0.29393405], train_acc: [0.9453], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  346/  468], training loss: [0.24408501], train_acc: [0.9609], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  347/  468], training loss: [0.25263664], train_acc: [0.9609], test_acc: [0.9498]\n",
      "Epoch: [ 0], batch: [  348/  468], training loss: [0.29413694], train_acc: [0.9531], test_acc: [0.9493]\n",
      "Epoch: [ 0], batch: [  349/  468], training loss: [0.27209759], train_acc: [0.9453], test_acc: [0.9501]\n",
      "Epoch: [ 0], batch: [  350/  468], training loss: [0.21104531], train_acc: [0.9609], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  351/  468], training loss: [0.31893325], train_acc: [0.9609], test_acc: [0.9502]\n",
      "Epoch: [ 0], batch: [  352/  468], training loss: [0.31671584], train_acc: [0.9453], test_acc: [0.9508]\n",
      "Epoch: [ 0], batch: [  353/  468], training loss: [0.38131559], train_acc: [0.9062], test_acc: [0.9508]\n",
      "Epoch: [ 0], batch: [  354/  468], training loss: [0.30454111], train_acc: [0.9375], test_acc: [0.9508]\n",
      "Epoch: [ 0], batch: [  355/  468], training loss: [0.40027836], train_acc: [0.9297], test_acc: [0.9508]\n",
      "Epoch: [ 0], batch: [  356/  468], training loss: [0.30960631], train_acc: [0.9531], test_acc: [0.9495]\n",
      "Epoch: [ 0], batch: [  357/  468], training loss: [0.23979354], train_acc: [0.9688], test_acc: [0.9482]\n",
      "Epoch: [ 0], batch: [  358/  468], training loss: [0.40554264], train_acc: [0.9297], test_acc: [0.9487]\n",
      "Epoch: [ 0], batch: [  359/  468], training loss: [0.31861210], train_acc: [0.9688], test_acc: [0.9481]\n",
      "Epoch: [ 0], batch: [  360/  468], training loss: [0.29576561], train_acc: [0.9609], test_acc: [0.9493]\n",
      "Epoch: [ 0], batch: [  361/  468], training loss: [0.13000327], train_acc: [0.9766], test_acc: [0.9504]\n",
      "Epoch: [ 0], batch: [  362/  468], training loss: [0.19132255], train_acc: [0.9531], test_acc: [0.9499]\n",
      "Epoch: [ 0], batch: [  363/  468], training loss: [0.26596397], train_acc: [0.9453], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  364/  468], training loss: [0.25742060], train_acc: [0.9844], test_acc: [0.9482]\n",
      "Epoch: [ 0], batch: [  365/  468], training loss: [0.31426400], train_acc: [0.9453], test_acc: [0.9479]\n",
      "Epoch: [ 0], batch: [  366/  468], training loss: [0.19978905], train_acc: [0.9453], test_acc: [0.9475]\n",
      "Epoch: [ 0], batch: [  367/  468], training loss: [0.28187329], train_acc: [0.9609], test_acc: [0.9477]\n",
      "Epoch: [ 0], batch: [  368/  468], training loss: [0.22769520], train_acc: [0.9688], test_acc: [0.9487]\n",
      "Epoch: [ 0], batch: [  369/  468], training loss: [0.25349075], train_acc: [0.9609], test_acc: [0.9506]\n",
      "Epoch: [ 0], batch: [  370/  468], training loss: [0.23961219], train_acc: [0.9688], test_acc: [0.9501]\n",
      "Epoch: [ 0], batch: [  371/  468], training loss: [0.26632553], train_acc: [0.9609], test_acc: [0.9494]\n",
      "Epoch: [ 0], batch: [  372/  468], training loss: [0.28132859], train_acc: [0.9609], test_acc: [0.9510]\n",
      "Epoch: [ 0], batch: [  373/  468], training loss: [0.16901985], train_acc: [0.9688], test_acc: [0.9501]\n",
      "Epoch: [ 0], batch: [  374/  468], training loss: [0.16715026], train_acc: [0.9688], test_acc: [0.9495]\n",
      "Epoch: [ 0], batch: [  375/  468], training loss: [0.17430714], train_acc: [0.9531], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  376/  468], training loss: [0.36560988], train_acc: [0.9453], test_acc: [0.9486]\n",
      "Epoch: [ 0], batch: [  377/  468], training loss: [0.34578851], train_acc: [0.9609], test_acc: [0.9495]\n",
      "Epoch: [ 0], batch: [  378/  468], training loss: [0.37414804], train_acc: [0.9297], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  379/  468], training loss: [0.29638717], train_acc: [0.9531], test_acc: [0.9505]\n",
      "Epoch: [ 0], batch: [  380/  468], training loss: [0.20795108], train_acc: [0.9531], test_acc: [0.9504]\n",
      "Epoch: [ 0], batch: [  381/  468], training loss: [0.39062795], train_acc: [0.8984], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  382/  468], training loss: [0.23383708], train_acc: [0.9531], test_acc: [0.9506]\n",
      "Epoch: [ 0], batch: [  383/  468], training loss: [0.40463203], train_acc: [0.9453], test_acc: [0.9512]\n",
      "Epoch: [ 0], batch: [  384/  468], training loss: [0.20189306], train_acc: [0.9844], test_acc: [0.9513]\n",
      "Epoch: [ 0], batch: [  385/  468], training loss: [0.23389846], train_acc: [0.9531], test_acc: [0.9515]\n",
      "Epoch: [ 0], batch: [  386/  468], training loss: [0.15525976], train_acc: [0.9609], test_acc: [0.9500]\n",
      "Epoch: [ 0], batch: [  387/  468], training loss: [0.32230061], train_acc: [0.9297], test_acc: [0.9497]\n",
      "Epoch: [ 0], batch: [  388/  468], training loss: [0.20652619], train_acc: [0.9688], test_acc: [0.9488]\n",
      "Epoch: [ 0], batch: [  389/  468], training loss: [0.41452459], train_acc: [0.8984], test_acc: [0.9491]\n",
      "Epoch: [ 0], batch: [  390/  468], training loss: [0.18084627], train_acc: [0.9922], test_acc: [0.9501]\n",
      "Epoch: [ 0], batch: [  391/  468], training loss: [0.39201313], train_acc: [0.9766], test_acc: [0.9514]\n",
      "Epoch: [ 0], batch: [  392/  468], training loss: [0.31431031], train_acc: [0.9375], test_acc: [0.9515]\n",
      "Epoch: [ 0], batch: [  393/  468], training loss: [0.26443136], train_acc: [0.9531], test_acc: [0.9515]\n",
      "Epoch: [ 0], batch: [  394/  468], training loss: [0.21001101], train_acc: [0.9688], test_acc: [0.9515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  395/  468], training loss: [0.21478081], train_acc: [0.9609], test_acc: [0.9506]\n",
      "Epoch: [ 0], batch: [  396/  468], training loss: [0.19520450], train_acc: [0.9531], test_acc: [0.9510]\n",
      "Epoch: [ 0], batch: [  397/  468], training loss: [0.23981965], train_acc: [0.9297], test_acc: [0.9522]\n",
      "Epoch: [ 0], batch: [  398/  468], training loss: [0.30557424], train_acc: [0.9531], test_acc: [0.9529]\n",
      "Epoch: [ 0], batch: [  399/  468], training loss: [0.30409974], train_acc: [0.9453], test_acc: [0.9535]\n",
      "Epoch: [ 0], batch: [  400/  468], training loss: [0.11583491], train_acc: [0.9766], test_acc: [0.9531]\n",
      "Epoch: [ 0], batch: [  401/  468], training loss: [0.18419145], train_acc: [0.9531], test_acc: [0.9531]\n",
      "Epoch: [ 0], batch: [  402/  468], training loss: [0.24837868], train_acc: [0.9688], test_acc: [0.9541]\n",
      "Epoch: [ 0], batch: [  403/  468], training loss: [0.21666071], train_acc: [0.9609], test_acc: [0.9534]\n",
      "Epoch: [ 0], batch: [  404/  468], training loss: [0.23958758], train_acc: [0.9297], test_acc: [0.9528]\n",
      "Epoch: [ 0], batch: [  405/  468], training loss: [0.37794453], train_acc: [0.9297], test_acc: [0.9525]\n",
      "Epoch: [ 0], batch: [  406/  468], training loss: [0.35137886], train_acc: [0.9453], test_acc: [0.9531]\n",
      "Epoch: [ 0], batch: [  407/  468], training loss: [0.22302002], train_acc: [0.9375], test_acc: [0.9534]\n",
      "Epoch: [ 0], batch: [  408/  468], training loss: [0.18033451], train_acc: [0.9609], test_acc: [0.9534]\n",
      "Epoch: [ 0], batch: [  409/  468], training loss: [0.29663384], train_acc: [0.9297], test_acc: [0.9528]\n",
      "Epoch: [ 0], batch: [  410/  468], training loss: [0.26179495], train_acc: [0.9609], test_acc: [0.9535]\n",
      "Epoch: [ 0], batch: [  411/  468], training loss: [0.25616214], train_acc: [0.9609], test_acc: [0.9517]\n",
      "Epoch: [ 0], batch: [  412/  468], training loss: [0.35963619], train_acc: [0.9062], test_acc: [0.9502]\n",
      "Epoch: [ 0], batch: [  413/  468], training loss: [0.22145784], train_acc: [0.9375], test_acc: [0.9495]\n",
      "Epoch: [ 0], batch: [  414/  468], training loss: [0.20094952], train_acc: [0.9609], test_acc: [0.9489]\n",
      "Epoch: [ 0], batch: [  415/  468], training loss: [0.40765589], train_acc: [0.9375], test_acc: [0.9490]\n",
      "Epoch: [ 0], batch: [  416/  468], training loss: [0.38019934], train_acc: [0.9453], test_acc: [0.9509]\n",
      "Epoch: [ 0], batch: [  417/  468], training loss: [0.27515820], train_acc: [0.9531], test_acc: [0.9523]\n",
      "Epoch: [ 0], batch: [  418/  468], training loss: [0.23522949], train_acc: [0.9453], test_acc: [0.9538]\n",
      "Epoch: [ 0], batch: [  419/  468], training loss: [0.27018368], train_acc: [0.9531], test_acc: [0.9546]\n",
      "Epoch: [ 0], batch: [  420/  468], training loss: [0.19999990], train_acc: [0.9766], test_acc: [0.9547]\n",
      "Epoch: [ 0], batch: [  421/  468], training loss: [0.41569930], train_acc: [0.9219], test_acc: [0.9543]\n",
      "Epoch: [ 0], batch: [  422/  468], training loss: [0.23410934], train_acc: [0.9688], test_acc: [0.9538]\n",
      "Epoch: [ 0], batch: [  423/  468], training loss: [0.22010008], train_acc: [0.9688], test_acc: [0.9547]\n",
      "Epoch: [ 0], batch: [  424/  468], training loss: [0.37483400], train_acc: [0.9531], test_acc: [0.9544]\n",
      "Epoch: [ 0], batch: [  425/  468], training loss: [0.33236730], train_acc: [0.9375], test_acc: [0.9545]\n",
      "Epoch: [ 0], batch: [  426/  468], training loss: [0.22662817], train_acc: [0.9688], test_acc: [0.9549]\n",
      "Epoch: [ 0], batch: [  427/  468], training loss: [0.21431974], train_acc: [0.9531], test_acc: [0.9556]\n",
      "Epoch: [ 0], batch: [  428/  468], training loss: [0.19396146], train_acc: [0.9453], test_acc: [0.9552]\n",
      "Epoch: [ 0], batch: [  429/  468], training loss: [0.19060470], train_acc: [0.9844], test_acc: [0.9554]\n",
      "Epoch: [ 0], batch: [  430/  468], training loss: [0.42685664], train_acc: [0.9141], test_acc: [0.9554]\n",
      "Epoch: [ 0], batch: [  431/  468], training loss: [0.15740289], train_acc: [0.9766], test_acc: [0.9551]\n",
      "Epoch: [ 0], batch: [  432/  468], training loss: [0.40018058], train_acc: [0.9297], test_acc: [0.9560]\n",
      "Epoch: [ 0], batch: [  433/  468], training loss: [0.22957382], train_acc: [0.9766], test_acc: [0.9561]\n",
      "Epoch: [ 0], batch: [  434/  468], training loss: [0.28630394], train_acc: [0.9609], test_acc: [0.9560]\n",
      "Epoch: [ 0], batch: [  435/  468], training loss: [0.28152570], train_acc: [0.9531], test_acc: [0.9552]\n",
      "Epoch: [ 0], batch: [  436/  468], training loss: [0.20494932], train_acc: [0.9688], test_acc: [0.9562]\n",
      "Epoch: [ 0], batch: [  437/  468], training loss: [0.19063362], train_acc: [0.9531], test_acc: [0.9573]\n",
      "Epoch: [ 0], batch: [  438/  468], training loss: [0.18891390], train_acc: [0.9844], test_acc: [0.9563]\n",
      "Epoch: [ 0], batch: [  439/  468], training loss: [0.18517199], train_acc: [0.9609], test_acc: [0.9564]\n",
      "Epoch: [ 0], batch: [  440/  468], training loss: [0.27528396], train_acc: [0.9609], test_acc: [0.9552]\n",
      "Epoch: [ 0], batch: [  441/  468], training loss: [0.15087987], train_acc: [0.9609], test_acc: [0.9542]\n",
      "Epoch: [ 0], batch: [  442/  468], training loss: [0.19807827], train_acc: [0.9766], test_acc: [0.9545]\n",
      "Epoch: [ 0], batch: [  443/  468], training loss: [0.15116553], train_acc: [0.9766], test_acc: [0.9546]\n",
      "Epoch: [ 0], batch: [  444/  468], training loss: [0.33913824], train_acc: [0.9531], test_acc: [0.9553]\n",
      "Epoch: [ 0], batch: [  445/  468], training loss: [0.44613785], train_acc: [0.9375], test_acc: [0.9560]\n",
      "Epoch: [ 0], batch: [  446/  468], training loss: [0.31146032], train_acc: [0.9531], test_acc: [0.9570]\n",
      "Epoch: [ 0], batch: [  447/  468], training loss: [0.22011855], train_acc: [0.9766], test_acc: [0.9579]\n",
      "Epoch: [ 0], batch: [  448/  468], training loss: [0.10920338], train_acc: [0.9922], test_acc: [0.9584]\n",
      "Epoch: [ 0], batch: [  449/  468], training loss: [0.23526651], train_acc: [0.9453], test_acc: [0.9586]\n",
      "Epoch: [ 0], batch: [  450/  468], training loss: [0.33243072], train_acc: [0.9297], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  451/  468], training loss: [0.19293928], train_acc: [0.9609], test_acc: [0.9592]\n",
      "Epoch: [ 0], batch: [  452/  468], training loss: [0.22038397], train_acc: [0.9531], test_acc: [0.9587]\n",
      "Epoch: [ 0], batch: [  453/  468], training loss: [0.18010305], train_acc: [0.9844], test_acc: [0.9576]\n",
      "Epoch: [ 0], batch: [  454/  468], training loss: [0.07947385], train_acc: [0.9766], test_acc: [0.9563]\n",
      "Epoch: [ 0], batch: [  455/  468], training loss: [0.21543129], train_acc: [0.9766], test_acc: [0.9554]\n",
      "Epoch: [ 0], batch: [  456/  468], training loss: [0.29685208], train_acc: [0.9062], test_acc: [0.9542]\n",
      "Epoch: [ 0], batch: [  457/  468], training loss: [0.15598559], train_acc: [0.9766], test_acc: [0.9546]\n",
      "Epoch: [ 0], batch: [  458/  468], training loss: [0.27860504], train_acc: [0.9766], test_acc: [0.9543]\n",
      "Epoch: [ 0], batch: [  459/  468], training loss: [0.20100363], train_acc: [0.9688], test_acc: [0.9550]\n",
      "Epoch: [ 0], batch: [  460/  468], training loss: [0.35188976], train_acc: [0.9219], test_acc: [0.9558]\n",
      "Epoch: [ 0], batch: [  461/  468], training loss: [0.22504562], train_acc: [0.9531], test_acc: [0.9560]\n",
      "Epoch: [ 0], batch: [  462/  468], training loss: [0.27517143], train_acc: [0.9453], test_acc: [0.9574]\n",
      "Epoch: [ 0], batch: [  463/  468], training loss: [0.22770743], train_acc: [0.9531], test_acc: [0.9590]\n",
      "Epoch: [ 0], batch: [  464/  468], training loss: [0.17753856], train_acc: [0.9531], test_acc: [0.9588]\n",
      "Epoch: [ 0], batch: [  465/  468], training loss: [0.27551639], train_acc: [0.9375], test_acc: [0.9583]\n",
      "Epoch: [ 0], batch: [  466/  468], training loss: [0.22357778], train_acc: [0.9531], test_acc: [0.9573]\n",
      "Epoch: [ 0], batch: [  467/  468], training loss: [0.41216004], train_acc: [0.9062], test_acc: [0.9575]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    for idx, (train_input, train_label) in enumerate(train_dataset):\n",
    "        grads = grad(network, train_input, train_label)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\n",
    "        \n",
    "        train_loss = loss_fn(network, train_input, train_label)\n",
    "        train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "        \n",
    "        for test_input, test_label in test_dataset:\n",
    "            test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "       \n",
    "        print(\"Epoch: [%2d], batch: [%5d/%5d], training loss: [%.8f], train_acc: [%.4f], test_acc: [%.4f]\"\\\n",
    "              %(epoch, idx, training_iterations,train_loss, train_accuracy,test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7544e87",
   "metadata": {},
   "source": [
    "1번의 에포크 만으로도 95% 이상의 테스트 정확도를 보임을 알 수 있습니다! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf498f",
   "metadata": {},
   "source": [
    "## Batch Normalization 을 통한 성능 향상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7599abb",
   "metadata": {},
   "source": [
    "### 학습내용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba8e47",
   "metadata": {},
   "source": [
    "##### Internal Covariate Shift \n",
    "![ICS.png](https://kjhov195.github.io/post_img/200109/image3.png)\n",
    "* 신경망을 학습시키는 과정에서, 입력 데이터가 각 Layer를 거칠 때 마다 분포가 비 정형적으로 왜곡되는 현상을 Internal Covariatr Shift 문제라고 한다. \n",
    "* 이러한 현상이 발생하는 경우에는 데이터의 의미가 소실되어 학습을 저하하는 문제릴 발생시킵니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d96bc",
   "metadata": {},
   "source": [
    "##### 구원투수 : Batch Normalization \n",
    "* 이러한 문제를 해소하기 위한 방식으로는 Batch nornalization이 있습니다. (사실 이후 수식을 살펴보면 정규화보다는 표준화 방식이라고 말하는게 적합해 보이기는 합니다:) )[정규화와 표준화](https://rucrazia.tistory.com/90)  \n",
    "* Batch nornalization은 각 layer에서 다음 층으로 출력되는 값을 중심성향을 기준으로 한정시킴으로써, 데이터의 분포를 안정적으로 만드는 방식입니다. \n",
    "* 해당 변환 방식은 아래와 같습니다. \n",
    "    1. n-1번째 layer에서 출력되는 값 X에 대해 표준화를 각각 적용합니다. \n",
    "$$\\bar{x} = \\frac{x-\\mu_{Batch}}{\\sqrt{\\sigma_{Batch}^{2}+\\epsilon}}$$  \n",
    "    2. 각각의 출력에 가중치와 편향을 적용해 값을 전달합니다. \n",
    "$$\\hat{x} = \\gamma\\bar{x} + \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9eab9",
   "metadata": {},
   "source": [
    "### 실제 코드를 통한 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9154ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# 연산을 위한 라이브러리 \n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "# 레이블 원핫인코딩 \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터셋 \n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "04a423f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 호출 함수 \n",
    "# 상세사항은 위에서 정리한것을 참조합니다. \n",
    "def load_mnist():\n",
    "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "    \n",
    "    train_data = np.expand_dims(train_data, axis=-1)\n",
    "    test_data = np.expand_dims(test_data, axis =-1)\n",
    "    \n",
    "    train_data = train_data.astype(np.float32)/255.0\n",
    "    test_data = test_data.astype(np.float32)/255.0\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "82f96405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 정의 \n",
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training = True)\n",
    "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits,\n",
    "                                                                  y_true=labels,\n",
    "                                                                  from_logits = True))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a25e2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 함수 정의 \n",
    "def accuracy_fn(model, images, labels):\n",
    "    logits = model(images, training= False)\n",
    "    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
    "    acc = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6af24794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 추적 함수 \n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "\n",
    "    x= tape.gradient(loss, model.trainable_variables) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867b4e2",
   "metadata": {},
   "source": [
    "그런데 이전 코드에서 batch_norm() 만 추가해서 실행시키면 에러가 나더라고요. 그래서 뭔인을 찾아보니, 다음의 코드에 수정이 필요했습니다.\n",
    "\n",
    "return tape.gradient(loss, model.variables) 를 return tape.gradient(loss, model.trainable_variables) 로 바꿔주어야 오류 없이 코드가 돌아가는 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b92a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 구성 층 함수 \n",
    "\n",
    "def flatten() :\n",
    "    return tf.keras.layers.Flatten()\n",
    "\n",
    "def dense(label_dim, weight_init) :\n",
    "    return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init)\n",
    "\n",
    "def relu() :\n",
    "    return tf.keras.layers.Activation(tf.keras.activations.relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5e5347eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(rate) :\n",
    "    return tf.keras.layers.Dropout(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0b9a4",
   "metadata": {},
   "source": [
    "##### Batch norm. 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c36bd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm() :\n",
    "    return tf.keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010e6b0",
   "metadata": {},
   "source": [
    "##### Batch norm 층을 포함한 모형 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e40e5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_function(label_dim) :\n",
    "    weight_init = tf.keras.initializers.glorot_uniform()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(flatten())\n",
    "\n",
    "    for i in range(4) :\n",
    "        model.add(dense(512, weight_init))\n",
    "        model.add(batch_norm())\n",
    "        model.add(relu())\n",
    "        #model.add(dropout(rate=0.5))\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39bd8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_function_alpha(label_dim) :\n",
    "    weight_init = tf.keras.initializers.glorot_uniform()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(flatten())\n",
    "\n",
    "    for i in range(4) :\n",
    "        model.add(dense(512, weight_init))\n",
    "        model.add(batch_norm())\n",
    "        model.add(relu())\n",
    "        model.add(dropout(rate=0.5))\n",
    "\n",
    "    model.add(dense(label_dim, weight_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "716eb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "train_x, train_y, test_x, test_y = load_mnist()\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "training_epochs = 1\n",
    "training_iterations = len(train_x) // batch_size\n",
    "\n",
    "label_dim = 10\n",
    "\n",
    "train_flag = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "86fd76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Graph Input using Dataset API \"\"\"\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=batch_size).\\\n",
    "    batch(batch_size, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
    "    shuffle(buffer_size=100000).\\\n",
    "    prefetch(buffer_size=len(test_x)).\\\n",
    "    batch(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d06922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "network = create_model_function(label_dim)\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "647d8319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [    0/  468], training loss: [0.03354156], train_acc: [0.9922], test_acc: [0.9658]\n",
      "Epoch: [ 0], batch: [    1/  468], training loss: [0.05275929], train_acc: [0.9844], test_acc: [0.9662]\n",
      "Epoch: [ 0], batch: [    2/  468], training loss: [0.05619360], train_acc: [0.9766], test_acc: [0.9650]\n",
      "Epoch: [ 0], batch: [    3/  468], training loss: [0.07026189], train_acc: [0.9766], test_acc: [0.9652]\n",
      "Epoch: [ 0], batch: [    4/  468], training loss: [0.05155361], train_acc: [0.9922], test_acc: [0.9656]\n",
      "Epoch: [ 0], batch: [    5/  468], training loss: [0.03694142], train_acc: [0.9922], test_acc: [0.9660]\n",
      "Epoch: [ 0], batch: [    6/  468], training loss: [0.04054080], train_acc: [0.9844], test_acc: [0.9658]\n",
      "Epoch: [ 0], batch: [    7/  468], training loss: [0.05321918], train_acc: [0.9688], test_acc: [0.9654]\n",
      "Epoch: [ 0], batch: [    8/  468], training loss: [0.06080364], train_acc: [0.9766], test_acc: [0.9656]\n",
      "Epoch: [ 0], batch: [    9/  468], training loss: [0.03280349], train_acc: [0.9844], test_acc: [0.9666]\n",
      "Epoch: [ 0], batch: [   10/  468], training loss: [0.06517524], train_acc: [0.9609], test_acc: [0.9668]\n",
      "Epoch: [ 0], batch: [   11/  468], training loss: [0.04355824], train_acc: [0.9688], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [   12/  468], training loss: [0.06435943], train_acc: [0.9688], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [   13/  468], training loss: [0.08545059], train_acc: [0.9766], test_acc: [0.9674]\n",
      "Epoch: [ 0], batch: [   14/  468], training loss: [0.02121280], train_acc: [1.0000], test_acc: [0.9690]\n",
      "Epoch: [ 0], batch: [   15/  468], training loss: [0.07805164], train_acc: [0.9844], test_acc: [0.9687]\n",
      "Epoch: [ 0], batch: [   16/  468], training loss: [0.04940145], train_acc: [0.9922], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [   17/  468], training loss: [0.07015168], train_acc: [0.9766], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [   18/  468], training loss: [0.09797678], train_acc: [0.9766], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [   19/  468], training loss: [0.03190714], train_acc: [0.9766], test_acc: [0.9680]\n",
      "Epoch: [ 0], batch: [   20/  468], training loss: [0.04478758], train_acc: [0.9922], test_acc: [0.9680]\n",
      "Epoch: [ 0], batch: [   21/  468], training loss: [0.01822381], train_acc: [1.0000], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [   22/  468], training loss: [0.05270696], train_acc: [0.9922], test_acc: [0.9681]\n",
      "Epoch: [ 0], batch: [   23/  468], training loss: [0.02719283], train_acc: [0.9922], test_acc: [0.9683]\n",
      "Epoch: [ 0], batch: [   24/  468], training loss: [0.05177422], train_acc: [0.9922], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [   25/  468], training loss: [0.03009956], train_acc: [0.9844], test_acc: [0.9693]\n",
      "Epoch: [ 0], batch: [   26/  468], training loss: [0.03800381], train_acc: [0.9922], test_acc: [0.9698]\n",
      "Epoch: [ 0], batch: [   27/  468], training loss: [0.02328325], train_acc: [0.9922], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [   28/  468], training loss: [0.04722154], train_acc: [0.9766], test_acc: [0.9701]\n",
      "Epoch: [ 0], batch: [   29/  468], training loss: [0.05146360], train_acc: [0.9922], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [   30/  468], training loss: [0.01447754], train_acc: [1.0000], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [   31/  468], training loss: [0.06229952], train_acc: [0.9688], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [   32/  468], training loss: [0.02684569], train_acc: [0.9922], test_acc: [0.9722]\n",
      "Epoch: [ 0], batch: [   33/  468], training loss: [0.02781112], train_acc: [0.9844], test_acc: [0.9731]\n",
      "Epoch: [ 0], batch: [   34/  468], training loss: [0.01690472], train_acc: [1.0000], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [   35/  468], training loss: [0.05228101], train_acc: [0.9922], test_acc: [0.9736]\n",
      "Epoch: [ 0], batch: [   36/  468], training loss: [0.06282855], train_acc: [0.9766], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [   37/  468], training loss: [0.06557027], train_acc: [0.9922], test_acc: [0.9748]\n",
      "Epoch: [ 0], batch: [   38/  468], training loss: [0.02306645], train_acc: [1.0000], test_acc: [0.9756]\n",
      "Epoch: [ 0], batch: [   39/  468], training loss: [0.04736615], train_acc: [0.9922], test_acc: [0.9757]\n",
      "Epoch: [ 0], batch: [   40/  468], training loss: [0.04553430], train_acc: [0.9922], test_acc: [0.9757]\n",
      "Epoch: [ 0], batch: [   41/  468], training loss: [0.03665642], train_acc: [0.9922], test_acc: [0.9759]\n",
      "Epoch: [ 0], batch: [   42/  468], training loss: [0.05729897], train_acc: [0.9844], test_acc: [0.9758]\n",
      "Epoch: [ 0], batch: [   43/  468], training loss: [0.01672944], train_acc: [0.9922], test_acc: [0.9760]\n",
      "Epoch: [ 0], batch: [   44/  468], training loss: [0.02451218], train_acc: [1.0000], test_acc: [0.9754]\n",
      "Epoch: [ 0], batch: [   45/  468], training loss: [0.00851761], train_acc: [1.0000], test_acc: [0.9751]\n",
      "Epoch: [ 0], batch: [   46/  468], training loss: [0.04242603], train_acc: [0.9922], test_acc: [0.9747]\n",
      "Epoch: [ 0], batch: [   47/  468], training loss: [0.03936761], train_acc: [1.0000], test_acc: [0.9744]\n",
      "Epoch: [ 0], batch: [   48/  468], training loss: [0.05528138], train_acc: [0.9844], test_acc: [0.9736]\n",
      "Epoch: [ 0], batch: [   49/  468], training loss: [0.08717640], train_acc: [0.9688], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [   50/  468], training loss: [0.06118353], train_acc: [0.9766], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [   51/  468], training loss: [0.06134960], train_acc: [0.9844], test_acc: [0.9744]\n",
      "Epoch: [ 0], batch: [   52/  468], training loss: [0.05847498], train_acc: [0.9844], test_acc: [0.9748]\n",
      "Epoch: [ 0], batch: [   53/  468], training loss: [0.04811085], train_acc: [0.9922], test_acc: [0.9738]\n",
      "Epoch: [ 0], batch: [   54/  468], training loss: [0.05762597], train_acc: [0.9922], test_acc: [0.9737]\n",
      "Epoch: [ 0], batch: [   55/  468], training loss: [0.03276853], train_acc: [0.9844], test_acc: [0.9729]\n",
      "Epoch: [ 0], batch: [   56/  468], training loss: [0.07628509], train_acc: [0.9922], test_acc: [0.9724]\n",
      "Epoch: [ 0], batch: [   57/  468], training loss: [0.03449320], train_acc: [1.0000], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [   58/  468], training loss: [0.01700439], train_acc: [0.9922], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [   59/  468], training loss: [0.03851816], train_acc: [0.9922], test_acc: [0.9698]\n",
      "Epoch: [ 0], batch: [   60/  468], training loss: [0.04877294], train_acc: [0.9844], test_acc: [0.9687]\n",
      "Epoch: [ 0], batch: [   61/  468], training loss: [0.04102634], train_acc: [0.9844], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [   62/  468], training loss: [0.04701113], train_acc: [0.9688], test_acc: [0.9695]\n",
      "Epoch: [ 0], batch: [   63/  468], training loss: [0.02890050], train_acc: [1.0000], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [   64/  468], training loss: [0.06631432], train_acc: [0.9766], test_acc: [0.9698]\n",
      "Epoch: [ 0], batch: [   65/  468], training loss: [0.03047014], train_acc: [0.9844], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [   66/  468], training loss: [0.02468617], train_acc: [1.0000], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [   67/  468], training loss: [0.02904658], train_acc: [0.9688], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [   68/  468], training loss: [0.07245311], train_acc: [0.9609], test_acc: [0.9702]\n",
      "Epoch: [ 0], batch: [   69/  468], training loss: [0.08182192], train_acc: [0.9922], test_acc: [0.9697]\n",
      "Epoch: [ 0], batch: [   70/  468], training loss: [0.05200553], train_acc: [0.9688], test_acc: [0.9699]\n",
      "Epoch: [ 0], batch: [   71/  468], training loss: [0.06139011], train_acc: [0.9844], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [   72/  468], training loss: [0.04714022], train_acc: [0.9766], test_acc: [0.9708]\n",
      "Epoch: [ 0], batch: [   73/  468], training loss: [0.02381171], train_acc: [0.9922], test_acc: [0.9704]\n",
      "Epoch: [ 0], batch: [   74/  468], training loss: [0.04300677], train_acc: [0.9844], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [   75/  468], training loss: [0.04008842], train_acc: [0.9609], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [   76/  468], training loss: [0.04250643], train_acc: [0.9688], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [   77/  468], training loss: [0.03483802], train_acc: [0.9688], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [   78/  468], training loss: [0.05271897], train_acc: [0.9922], test_acc: [0.9705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [   79/  468], training loss: [0.01816952], train_acc: [1.0000], test_acc: [0.9705]\n",
      "Epoch: [ 0], batch: [   80/  468], training loss: [0.04216591], train_acc: [0.9844], test_acc: [0.9708]\n",
      "Epoch: [ 0], batch: [   81/  468], training loss: [0.04500364], train_acc: [0.9688], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [   82/  468], training loss: [0.03818472], train_acc: [1.0000], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [   83/  468], training loss: [0.06615342], train_acc: [0.9688], test_acc: [0.9728]\n",
      "Epoch: [ 0], batch: [   84/  468], training loss: [0.03053495], train_acc: [0.9922], test_acc: [0.9736]\n",
      "Epoch: [ 0], batch: [   85/  468], training loss: [0.08848628], train_acc: [0.9688], test_acc: [0.9737]\n",
      "Epoch: [ 0], batch: [   86/  468], training loss: [0.06213185], train_acc: [0.9688], test_acc: [0.9741]\n",
      "Epoch: [ 0], batch: [   87/  468], training loss: [0.02823506], train_acc: [0.9844], test_acc: [0.9747]\n",
      "Epoch: [ 0], batch: [   88/  468], training loss: [0.02729086], train_acc: [0.9922], test_acc: [0.9755]\n",
      "Epoch: [ 0], batch: [   89/  468], training loss: [0.02578228], train_acc: [0.9922], test_acc: [0.9751]\n",
      "Epoch: [ 0], batch: [   90/  468], training loss: [0.03083625], train_acc: [0.9766], test_acc: [0.9746]\n",
      "Epoch: [ 0], batch: [   91/  468], training loss: [0.01297982], train_acc: [0.9922], test_acc: [0.9750]\n",
      "Epoch: [ 0], batch: [   92/  468], training loss: [0.03465655], train_acc: [0.9844], test_acc: [0.9743]\n",
      "Epoch: [ 0], batch: [   93/  468], training loss: [0.03550677], train_acc: [0.9766], test_acc: [0.9737]\n",
      "Epoch: [ 0], batch: [   94/  468], training loss: [0.06705466], train_acc: [0.9766], test_acc: [0.9738]\n",
      "Epoch: [ 0], batch: [   95/  468], training loss: [0.02149870], train_acc: [0.9922], test_acc: [0.9733]\n",
      "Epoch: [ 0], batch: [   96/  468], training loss: [0.01855828], train_acc: [0.9922], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [   97/  468], training loss: [0.05092438], train_acc: [0.9922], test_acc: [0.9726]\n",
      "Epoch: [ 0], batch: [   98/  468], training loss: [0.09596342], train_acc: [0.9844], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [   99/  468], training loss: [0.04067389], train_acc: [0.9766], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  100/  468], training loss: [0.01800359], train_acc: [1.0000], test_acc: [0.9712]\n",
      "Epoch: [ 0], batch: [  101/  468], training loss: [0.03681020], train_acc: [0.9609], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [  102/  468], training loss: [0.03427927], train_acc: [1.0000], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  103/  468], training loss: [0.10680532], train_acc: [0.9531], test_acc: [0.9697]\n",
      "Epoch: [ 0], batch: [  104/  468], training loss: [0.07461879], train_acc: [0.9766], test_acc: [0.9696]\n",
      "Epoch: [ 0], batch: [  105/  468], training loss: [0.02401584], train_acc: [0.9844], test_acc: [0.9687]\n",
      "Epoch: [ 0], batch: [  106/  468], training loss: [0.02494629], train_acc: [0.9844], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  107/  468], training loss: [0.06030179], train_acc: [0.9844], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [  108/  468], training loss: [0.05325758], train_acc: [0.9766], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [  109/  468], training loss: [0.02424304], train_acc: [0.9922], test_acc: [0.9682]\n",
      "Epoch: [ 0], batch: [  110/  468], training loss: [0.04985972], train_acc: [0.9688], test_acc: [0.9681]\n",
      "Epoch: [ 0], batch: [  111/  468], training loss: [0.07511674], train_acc: [0.9766], test_acc: [0.9677]\n",
      "Epoch: [ 0], batch: [  112/  468], training loss: [0.08068231], train_acc: [0.9609], test_acc: [0.9674]\n",
      "Epoch: [ 0], batch: [  113/  468], training loss: [0.04113450], train_acc: [0.9922], test_acc: [0.9664]\n",
      "Epoch: [ 0], batch: [  114/  468], training loss: [0.03892022], train_acc: [0.9844], test_acc: [0.9668]\n",
      "Epoch: [ 0], batch: [  115/  468], training loss: [0.01402462], train_acc: [0.9922], test_acc: [0.9660]\n",
      "Epoch: [ 0], batch: [  116/  468], training loss: [0.01370665], train_acc: [1.0000], test_acc: [0.9642]\n",
      "Epoch: [ 0], batch: [  117/  468], training loss: [0.05479705], train_acc: [0.9766], test_acc: [0.9639]\n",
      "Epoch: [ 0], batch: [  118/  468], training loss: [0.07027982], train_acc: [0.9609], test_acc: [0.9652]\n",
      "Epoch: [ 0], batch: [  119/  468], training loss: [0.06876496], train_acc: [0.9844], test_acc: [0.9648]\n",
      "Epoch: [ 0], batch: [  120/  468], training loss: [0.03018653], train_acc: [0.9844], test_acc: [0.9652]\n",
      "Epoch: [ 0], batch: [  121/  468], training loss: [0.04240847], train_acc: [0.9922], test_acc: [0.9647]\n",
      "Epoch: [ 0], batch: [  122/  468], training loss: [0.02888272], train_acc: [0.9922], test_acc: [0.9648]\n",
      "Epoch: [ 0], batch: [  123/  468], training loss: [0.03055340], train_acc: [0.9844], test_acc: [0.9650]\n",
      "Epoch: [ 0], batch: [  124/  468], training loss: [0.09450220], train_acc: [0.9609], test_acc: [0.9647]\n",
      "Epoch: [ 0], batch: [  125/  468], training loss: [0.03030350], train_acc: [0.9844], test_acc: [0.9648]\n",
      "Epoch: [ 0], batch: [  126/  468], training loss: [0.03522089], train_acc: [0.9766], test_acc: [0.9639]\n",
      "Epoch: [ 0], batch: [  127/  468], training loss: [0.05226127], train_acc: [0.9688], test_acc: [0.9658]\n",
      "Epoch: [ 0], batch: [  128/  468], training loss: [0.02450587], train_acc: [0.9844], test_acc: [0.9671]\n",
      "Epoch: [ 0], batch: [  129/  468], training loss: [0.01512814], train_acc: [0.9922], test_acc: [0.9679]\n",
      "Epoch: [ 0], batch: [  130/  468], training loss: [0.03634646], train_acc: [0.9844], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  131/  468], training loss: [0.08198866], train_acc: [0.9844], test_acc: [0.9688]\n",
      "Epoch: [ 0], batch: [  132/  468], training loss: [0.02790783], train_acc: [0.9844], test_acc: [0.9695]\n",
      "Epoch: [ 0], batch: [  133/  468], training loss: [0.03263990], train_acc: [0.9922], test_acc: [0.9688]\n",
      "Epoch: [ 0], batch: [  134/  468], training loss: [0.01641093], train_acc: [0.9922], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  135/  468], training loss: [0.07380807], train_acc: [0.9844], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  136/  468], training loss: [0.01266386], train_acc: [0.9922], test_acc: [0.9680]\n",
      "Epoch: [ 0], batch: [  137/  468], training loss: [0.01783232], train_acc: [1.0000], test_acc: [0.9673]\n",
      "Epoch: [ 0], batch: [  138/  468], training loss: [0.03592569], train_acc: [0.9844], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  139/  468], training loss: [0.02434168], train_acc: [0.9922], test_acc: [0.9676]\n",
      "Epoch: [ 0], batch: [  140/  468], training loss: [0.04763002], train_acc: [0.9844], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  141/  468], training loss: [0.03825919], train_acc: [0.9922], test_acc: [0.9686]\n",
      "Epoch: [ 0], batch: [  142/  468], training loss: [0.01215722], train_acc: [0.9922], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [  143/  468], training loss: [0.04049012], train_acc: [0.9922], test_acc: [0.9694]\n",
      "Epoch: [ 0], batch: [  144/  468], training loss: [0.04849400], train_acc: [0.9766], test_acc: [0.9699]\n",
      "Epoch: [ 0], batch: [  145/  468], training loss: [0.03098881], train_acc: [0.9922], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [  146/  468], training loss: [0.02058335], train_acc: [0.9922], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [  147/  468], training loss: [0.02043894], train_acc: [0.9766], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  148/  468], training loss: [0.02354174], train_acc: [0.9922], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  149/  468], training loss: [0.05541927], train_acc: [0.9922], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  150/  468], training loss: [0.02215813], train_acc: [1.0000], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  151/  468], training loss: [0.02722637], train_acc: [0.9766], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  152/  468], training loss: [0.04349886], train_acc: [0.9766], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  153/  468], training loss: [0.02766721], train_acc: [0.9922], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  154/  468], training loss: [0.02184224], train_acc: [0.9922], test_acc: [0.9708]\n",
      "Epoch: [ 0], batch: [  155/  468], training loss: [0.09603707], train_acc: [0.9922], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  156/  468], training loss: [0.01262926], train_acc: [1.0000], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  157/  468], training loss: [0.02192340], train_acc: [1.0000], test_acc: [0.9716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  158/  468], training loss: [0.05025548], train_acc: [0.9766], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  159/  468], training loss: [0.05190221], train_acc: [0.9844], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  160/  468], training loss: [0.02309888], train_acc: [0.9922], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  161/  468], training loss: [0.03060837], train_acc: [0.9922], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  162/  468], training loss: [0.02145824], train_acc: [1.0000], test_acc: [0.9716]\n",
      "Epoch: [ 0], batch: [  163/  468], training loss: [0.07117575], train_acc: [0.9844], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  164/  468], training loss: [0.03292055], train_acc: [1.0000], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  165/  468], training loss: [0.03533996], train_acc: [0.9844], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [  166/  468], training loss: [0.03311861], train_acc: [0.9844], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [  167/  468], training loss: [0.01660478], train_acc: [1.0000], test_acc: [0.9701]\n",
      "Epoch: [ 0], batch: [  168/  468], training loss: [0.04391460], train_acc: [0.9922], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [  169/  468], training loss: [0.04871432], train_acc: [1.0000], test_acc: [0.9704]\n",
      "Epoch: [ 0], batch: [  170/  468], training loss: [0.01727839], train_acc: [1.0000], test_acc: [0.9699]\n",
      "Epoch: [ 0], batch: [  171/  468], training loss: [0.02834293], train_acc: [1.0000], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [  172/  468], training loss: [0.01483490], train_acc: [1.0000], test_acc: [0.9698]\n",
      "Epoch: [ 0], batch: [  173/  468], training loss: [0.10461208], train_acc: [0.9688], test_acc: [0.9707]\n",
      "Epoch: [ 0], batch: [  174/  468], training loss: [0.03644977], train_acc: [0.9844], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  175/  468], training loss: [0.05301850], train_acc: [0.9844], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  176/  468], training loss: [0.02676780], train_acc: [1.0000], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  177/  468], training loss: [0.03609422], train_acc: [0.9688], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  178/  468], training loss: [0.01119821], train_acc: [0.9922], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  179/  468], training loss: [0.02734421], train_acc: [0.9766], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  180/  468], training loss: [0.06864403], train_acc: [0.9844], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  181/  468], training loss: [0.05132120], train_acc: [0.9766], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  182/  468], training loss: [0.02607270], train_acc: [0.9766], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  183/  468], training loss: [0.01351655], train_acc: [1.0000], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  184/  468], training loss: [0.02062929], train_acc: [0.9922], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  185/  468], training loss: [0.00914994], train_acc: [0.9922], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  186/  468], training loss: [0.01685012], train_acc: [1.0000], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  187/  468], training loss: [0.04942510], train_acc: [0.9844], test_acc: [0.9712]\n",
      "Epoch: [ 0], batch: [  188/  468], training loss: [0.02802690], train_acc: [0.9922], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  189/  468], training loss: [0.01182791], train_acc: [0.9922], test_acc: [0.9716]\n",
      "Epoch: [ 0], batch: [  190/  468], training loss: [0.01173602], train_acc: [1.0000], test_acc: [0.9712]\n",
      "Epoch: [ 0], batch: [  191/  468], training loss: [0.08223635], train_acc: [0.9766], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  192/  468], training loss: [0.04297803], train_acc: [0.9922], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  193/  468], training loss: [0.05415957], train_acc: [0.9844], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  194/  468], training loss: [0.02508150], train_acc: [0.9922], test_acc: [0.9722]\n",
      "Epoch: [ 0], batch: [  195/  468], training loss: [0.00884829], train_acc: [0.9922], test_acc: [0.9726]\n",
      "Epoch: [ 0], batch: [  196/  468], training loss: [0.03815503], train_acc: [0.9844], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  197/  468], training loss: [0.02586623], train_acc: [0.9844], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  198/  468], training loss: [0.02087792], train_acc: [1.0000], test_acc: [0.9729]\n",
      "Epoch: [ 0], batch: [  199/  468], training loss: [0.02600294], train_acc: [0.9922], test_acc: [0.9720]\n",
      "Epoch: [ 0], batch: [  200/  468], training loss: [0.02026921], train_acc: [1.0000], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  201/  468], training loss: [0.07246649], train_acc: [0.9844], test_acc: [0.9708]\n",
      "Epoch: [ 0], batch: [  202/  468], training loss: [0.07118358], train_acc: [0.9766], test_acc: [0.9712]\n",
      "Epoch: [ 0], batch: [  203/  468], training loss: [0.06386736], train_acc: [0.9844], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  204/  468], training loss: [0.10549170], train_acc: [0.9688], test_acc: [0.9722]\n",
      "Epoch: [ 0], batch: [  205/  468], training loss: [0.03362881], train_acc: [1.0000], test_acc: [0.9724]\n",
      "Epoch: [ 0], batch: [  206/  468], training loss: [0.09696181], train_acc: [0.9609], test_acc: [0.9723]\n",
      "Epoch: [ 0], batch: [  207/  468], training loss: [0.06109848], train_acc: [0.9922], test_acc: [0.9728]\n",
      "Epoch: [ 0], batch: [  208/  468], training loss: [0.05466812], train_acc: [0.9922], test_acc: [0.9733]\n",
      "Epoch: [ 0], batch: [  209/  468], training loss: [0.03038371], train_acc: [0.9922], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [  210/  468], training loss: [0.03112235], train_acc: [0.9844], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [  211/  468], training loss: [0.01663587], train_acc: [1.0000], test_acc: [0.9729]\n",
      "Epoch: [ 0], batch: [  212/  468], training loss: [0.03297403], train_acc: [0.9922], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  213/  468], training loss: [0.02727124], train_acc: [1.0000], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  214/  468], training loss: [0.05405002], train_acc: [0.9844], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [  215/  468], training loss: [0.03546088], train_acc: [0.9844], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [  216/  468], training loss: [0.06149259], train_acc: [1.0000], test_acc: [0.9687]\n",
      "Epoch: [ 0], batch: [  217/  468], training loss: [0.07037046], train_acc: [0.9844], test_acc: [0.9680]\n",
      "Epoch: [ 0], batch: [  218/  468], training loss: [0.04278533], train_acc: [0.9766], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [  219/  468], training loss: [0.03804988], train_acc: [1.0000], test_acc: [0.9683]\n",
      "Epoch: [ 0], batch: [  220/  468], training loss: [0.09524625], train_acc: [0.9531], test_acc: [0.9682]\n",
      "Epoch: [ 0], batch: [  221/  468], training loss: [0.05863252], train_acc: [0.9844], test_acc: [0.9683]\n",
      "Epoch: [ 0], batch: [  222/  468], training loss: [0.02565977], train_acc: [0.9844], test_acc: [0.9683]\n",
      "Epoch: [ 0], batch: [  223/  468], training loss: [0.08920953], train_acc: [0.9531], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  224/  468], training loss: [0.13897231], train_acc: [0.9609], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [  225/  468], training loss: [0.05644925], train_acc: [0.9766], test_acc: [0.9673]\n",
      "Epoch: [ 0], batch: [  226/  468], training loss: [0.06979395], train_acc: [0.9766], test_acc: [0.9686]\n",
      "Epoch: [ 0], batch: [  227/  468], training loss: [0.01378748], train_acc: [0.9922], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [  228/  468], training loss: [0.03238506], train_acc: [0.9922], test_acc: [0.9676]\n",
      "Epoch: [ 0], batch: [  229/  468], training loss: [0.04913404], train_acc: [0.9844], test_acc: [0.9672]\n",
      "Epoch: [ 0], batch: [  230/  468], training loss: [0.02081357], train_acc: [0.9922], test_acc: [0.9667]\n",
      "Epoch: [ 0], batch: [  231/  468], training loss: [0.02563597], train_acc: [0.9844], test_acc: [0.9648]\n",
      "Epoch: [ 0], batch: [  232/  468], training loss: [0.05473308], train_acc: [0.9688], test_acc: [0.9650]\n",
      "Epoch: [ 0], batch: [  233/  468], training loss: [0.05287448], train_acc: [0.9844], test_acc: [0.9658]\n",
      "Epoch: [ 0], batch: [  234/  468], training loss: [0.02848566], train_acc: [1.0000], test_acc: [0.9659]\n",
      "Epoch: [ 0], batch: [  235/  468], training loss: [0.04167541], train_acc: [0.9766], test_acc: [0.9655]\n",
      "Epoch: [ 0], batch: [  236/  468], training loss: [0.04303003], train_acc: [0.9766], test_acc: [0.9654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  237/  468], training loss: [0.04298863], train_acc: [0.9844], test_acc: [0.9659]\n",
      "Epoch: [ 0], batch: [  238/  468], training loss: [0.03523767], train_acc: [0.9922], test_acc: [0.9660]\n",
      "Epoch: [ 0], batch: [  239/  468], training loss: [0.03993823], train_acc: [0.9766], test_acc: [0.9673]\n",
      "Epoch: [ 0], batch: [  240/  468], training loss: [0.05241570], train_acc: [0.9922], test_acc: [0.9658]\n",
      "Epoch: [ 0], batch: [  241/  468], training loss: [0.09863783], train_acc: [0.9375], test_acc: [0.9649]\n",
      "Epoch: [ 0], batch: [  242/  468], training loss: [0.01460557], train_acc: [0.9922], test_acc: [0.9646]\n",
      "Epoch: [ 0], batch: [  243/  468], training loss: [0.06195747], train_acc: [0.9453], test_acc: [0.9654]\n",
      "Epoch: [ 0], batch: [  244/  468], training loss: [0.05186494], train_acc: [0.9844], test_acc: [0.9682]\n",
      "Epoch: [ 0], batch: [  245/  468], training loss: [0.01969795], train_acc: [0.9922], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  246/  468], training loss: [0.04112409], train_acc: [0.9609], test_acc: [0.9671]\n",
      "Epoch: [ 0], batch: [  247/  468], training loss: [0.06014571], train_acc: [0.9766], test_acc: [0.9674]\n",
      "Epoch: [ 0], batch: [  248/  468], training loss: [0.04477288], train_acc: [0.9922], test_acc: [0.9671]\n",
      "Epoch: [ 0], batch: [  249/  468], training loss: [0.03762943], train_acc: [0.9922], test_acc: [0.9664]\n",
      "Epoch: [ 0], batch: [  250/  468], training loss: [0.05980592], train_acc: [0.9531], test_acc: [0.9661]\n",
      "Epoch: [ 0], batch: [  251/  468], training loss: [0.06572852], train_acc: [0.9609], test_acc: [0.9659]\n",
      "Epoch: [ 0], batch: [  252/  468], training loss: [0.01302959], train_acc: [0.9922], test_acc: [0.9655]\n",
      "Epoch: [ 0], batch: [  253/  468], training loss: [0.03323241], train_acc: [0.9844], test_acc: [0.9653]\n",
      "Epoch: [ 0], batch: [  254/  468], training loss: [0.05640417], train_acc: [0.9766], test_acc: [0.9650]\n",
      "Epoch: [ 0], batch: [  255/  468], training loss: [0.10603503], train_acc: [0.9844], test_acc: [0.9657]\n",
      "Epoch: [ 0], batch: [  256/  468], training loss: [0.02184970], train_acc: [1.0000], test_acc: [0.9677]\n",
      "Epoch: [ 0], batch: [  257/  468], training loss: [0.07512522], train_acc: [0.9766], test_acc: [0.9679]\n",
      "Epoch: [ 0], batch: [  258/  468], training loss: [0.04178755], train_acc: [1.0000], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  259/  468], training loss: [0.09253936], train_acc: [0.9766], test_acc: [0.9673]\n",
      "Epoch: [ 0], batch: [  260/  468], training loss: [0.02764833], train_acc: [0.9922], test_acc: [0.9675]\n",
      "Epoch: [ 0], batch: [  261/  468], training loss: [0.03450011], train_acc: [0.9766], test_acc: [0.9671]\n",
      "Epoch: [ 0], batch: [  262/  468], training loss: [0.04816237], train_acc: [0.9844], test_acc: [0.9682]\n",
      "Epoch: [ 0], batch: [  263/  468], training loss: [0.07712874], train_acc: [0.9844], test_acc: [0.9668]\n",
      "Epoch: [ 0], batch: [  264/  468], training loss: [0.11489026], train_acc: [0.9766], test_acc: [0.9665]\n",
      "Epoch: [ 0], batch: [  265/  468], training loss: [0.03933517], train_acc: [0.9844], test_acc: [0.9652]\n",
      "Epoch: [ 0], batch: [  266/  468], training loss: [0.01981017], train_acc: [0.9922], test_acc: [0.9651]\n",
      "Epoch: [ 0], batch: [  267/  468], training loss: [0.11146499], train_acc: [0.9688], test_acc: [0.9653]\n",
      "Epoch: [ 0], batch: [  268/  468], training loss: [0.07324582], train_acc: [0.9844], test_acc: [0.9648]\n",
      "Epoch: [ 0], batch: [  269/  468], training loss: [0.05871291], train_acc: [0.9766], test_acc: [0.9649]\n",
      "Epoch: [ 0], batch: [  270/  468], training loss: [0.08617648], train_acc: [0.9766], test_acc: [0.9657]\n",
      "Epoch: [ 0], batch: [  271/  468], training loss: [0.04334586], train_acc: [0.9844], test_acc: [0.9666]\n",
      "Epoch: [ 0], batch: [  272/  468], training loss: [0.04574613], train_acc: [0.9688], test_acc: [0.9665]\n",
      "Epoch: [ 0], batch: [  273/  468], training loss: [0.05233583], train_acc: [0.9766], test_acc: [0.9675]\n",
      "Epoch: [ 0], batch: [  274/  468], training loss: [0.01446123], train_acc: [1.0000], test_acc: [0.9665]\n",
      "Epoch: [ 0], batch: [  275/  468], training loss: [0.06983429], train_acc: [0.9609], test_acc: [0.9671]\n",
      "Epoch: [ 0], batch: [  276/  468], training loss: [0.03735535], train_acc: [0.9922], test_acc: [0.9674]\n",
      "Epoch: [ 0], batch: [  277/  468], training loss: [0.03060176], train_acc: [0.9766], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  278/  468], training loss: [0.03012795], train_acc: [0.9922], test_acc: [0.9685]\n",
      "Epoch: [ 0], batch: [  279/  468], training loss: [0.05088880], train_acc: [0.9844], test_acc: [0.9699]\n",
      "Epoch: [ 0], batch: [  280/  468], training loss: [0.02670527], train_acc: [0.9922], test_acc: [0.9695]\n",
      "Epoch: [ 0], batch: [  281/  468], training loss: [0.08361480], train_acc: [0.9688], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [  282/  468], training loss: [0.08561745], train_acc: [0.9609], test_acc: [0.9702]\n",
      "Epoch: [ 0], batch: [  283/  468], training loss: [0.06174435], train_acc: [0.9766], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [  284/  468], training loss: [0.11188144], train_acc: [0.9531], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  285/  468], training loss: [0.05997790], train_acc: [0.9766], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  286/  468], training loss: [0.08444228], train_acc: [0.9844], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  287/  468], training loss: [0.01704148], train_acc: [1.0000], test_acc: [0.9721]\n",
      "Epoch: [ 0], batch: [  288/  468], training loss: [0.02438416], train_acc: [1.0000], test_acc: [0.9720]\n",
      "Epoch: [ 0], batch: [  289/  468], training loss: [0.05691379], train_acc: [0.9844], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  290/  468], training loss: [0.04993514], train_acc: [0.9766], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  291/  468], training loss: [0.10758360], train_acc: [0.9531], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  292/  468], training loss: [0.03275709], train_acc: [0.9922], test_acc: [0.9715]\n",
      "Epoch: [ 0], batch: [  293/  468], training loss: [0.03345275], train_acc: [0.9844], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  294/  468], training loss: [0.06634369], train_acc: [0.9844], test_acc: [0.9701]\n",
      "Epoch: [ 0], batch: [  295/  468], training loss: [0.04599070], train_acc: [0.9609], test_acc: [0.9695]\n",
      "Epoch: [ 0], batch: [  296/  468], training loss: [0.01994027], train_acc: [0.9922], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [  297/  468], training loss: [0.04253791], train_acc: [0.9766], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [  298/  468], training loss: [0.06001543], train_acc: [0.9766], test_acc: [0.9678]\n",
      "Epoch: [ 0], batch: [  299/  468], training loss: [0.06826874], train_acc: [0.9766], test_acc: [0.9685]\n",
      "Epoch: [ 0], batch: [  300/  468], training loss: [0.04439308], train_acc: [0.9844], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [  301/  468], training loss: [0.08009931], train_acc: [0.9688], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [  302/  468], training loss: [0.10052795], train_acc: [0.9609], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  303/  468], training loss: [0.06342795], train_acc: [0.9844], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  304/  468], training loss: [0.05675368], train_acc: [0.9844], test_acc: [0.9739]\n",
      "Epoch: [ 0], batch: [  305/  468], training loss: [0.17292479], train_acc: [0.9688], test_acc: [0.9753]\n",
      "Epoch: [ 0], batch: [  306/  468], training loss: [0.04730920], train_acc: [0.9844], test_acc: [0.9756]\n",
      "Epoch: [ 0], batch: [  307/  468], training loss: [0.02225985], train_acc: [0.9844], test_acc: [0.9756]\n",
      "Epoch: [ 0], batch: [  308/  468], training loss: [0.02442696], train_acc: [1.0000], test_acc: [0.9750]\n",
      "Epoch: [ 0], batch: [  309/  468], training loss: [0.03498050], train_acc: [0.9844], test_acc: [0.9749]\n",
      "Epoch: [ 0], batch: [  310/  468], training loss: [0.02608359], train_acc: [1.0000], test_acc: [0.9739]\n",
      "Epoch: [ 0], batch: [  311/  468], training loss: [0.02495392], train_acc: [1.0000], test_acc: [0.9724]\n",
      "Epoch: [ 0], batch: [  312/  468], training loss: [0.02715713], train_acc: [0.9922], test_acc: [0.9712]\n",
      "Epoch: [ 0], batch: [  313/  468], training loss: [0.05853534], train_acc: [0.9766], test_acc: [0.9695]\n",
      "Epoch: [ 0], batch: [  314/  468], training loss: [0.04182801], train_acc: [1.0000], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [  315/  468], training loss: [0.06657653], train_acc: [0.9922], test_acc: [0.9685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  316/  468], training loss: [0.06863812], train_acc: [0.9688], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  317/  468], training loss: [0.02297287], train_acc: [0.9922], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [  318/  468], training loss: [0.03108316], train_acc: [0.9922], test_acc: [0.9694]\n",
      "Epoch: [ 0], batch: [  319/  468], training loss: [0.03287807], train_acc: [0.9766], test_acc: [0.9707]\n",
      "Epoch: [ 0], batch: [  320/  468], training loss: [0.05368334], train_acc: [0.9766], test_acc: [0.9712]\n",
      "Epoch: [ 0], batch: [  321/  468], training loss: [0.05699257], train_acc: [0.9844], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  322/  468], training loss: [0.04870804], train_acc: [0.9766], test_acc: [0.9722]\n",
      "Epoch: [ 0], batch: [  323/  468], training loss: [0.02870574], train_acc: [1.0000], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  324/  468], training loss: [0.03178144], train_acc: [0.9844], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  325/  468], training loss: [0.02992922], train_acc: [0.9844], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  326/  468], training loss: [0.01612067], train_acc: [0.9922], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  327/  468], training loss: [0.02751640], train_acc: [1.0000], test_acc: [0.9703]\n",
      "Epoch: [ 0], batch: [  328/  468], training loss: [0.04338928], train_acc: [0.9688], test_acc: [0.9708]\n",
      "Epoch: [ 0], batch: [  329/  468], training loss: [0.09269389], train_acc: [0.9688], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [  330/  468], training loss: [0.04917153], train_acc: [0.9688], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [  331/  468], training loss: [0.02962384], train_acc: [0.9844], test_acc: [0.9699]\n",
      "Epoch: [ 0], batch: [  332/  468], training loss: [0.04590951], train_acc: [0.9766], test_acc: [0.9690]\n",
      "Epoch: [ 0], batch: [  333/  468], training loss: [0.09369941], train_acc: [0.9766], test_acc: [0.9694]\n",
      "Epoch: [ 0], batch: [  334/  468], training loss: [0.07897941], train_acc: [0.9844], test_acc: [0.9693]\n",
      "Epoch: [ 0], batch: [  335/  468], training loss: [0.05963521], train_acc: [0.9922], test_acc: [0.9686]\n",
      "Epoch: [ 0], batch: [  336/  468], training loss: [0.04244468], train_acc: [1.0000], test_acc: [0.9688]\n",
      "Epoch: [ 0], batch: [  337/  468], training loss: [0.01662051], train_acc: [0.9922], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [  338/  468], training loss: [0.09847066], train_acc: [0.9766], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [  339/  468], training loss: [0.06337489], train_acc: [0.9844], test_acc: [0.9694]\n",
      "Epoch: [ 0], batch: [  340/  468], training loss: [0.04555801], train_acc: [0.9844], test_acc: [0.9699]\n",
      "Epoch: [ 0], batch: [  341/  468], training loss: [0.02153002], train_acc: [0.9922], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [  342/  468], training loss: [0.03558040], train_acc: [1.0000], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  343/  468], training loss: [0.06714915], train_acc: [0.9766], test_acc: [0.9710]\n",
      "Epoch: [ 0], batch: [  344/  468], training loss: [0.02065592], train_acc: [1.0000], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  345/  468], training loss: [0.02350814], train_acc: [0.9922], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  346/  468], training loss: [0.02190293], train_acc: [0.9922], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  347/  468], training loss: [0.01988433], train_acc: [0.9922], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  348/  468], training loss: [0.04126564], train_acc: [0.9844], test_acc: [0.9707]\n",
      "Epoch: [ 0], batch: [  349/  468], training loss: [0.16351327], train_acc: [0.9688], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  350/  468], training loss: [0.02049732], train_acc: [0.9922], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  351/  468], training loss: [0.07105867], train_acc: [0.9609], test_acc: [0.9730]\n",
      "Epoch: [ 0], batch: [  352/  468], training loss: [0.03089949], train_acc: [0.9922], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [  353/  468], training loss: [0.07281359], train_acc: [0.9766], test_acc: [0.9737]\n",
      "Epoch: [ 0], batch: [  354/  468], training loss: [0.04938144], train_acc: [0.9688], test_acc: [0.9729]\n",
      "Epoch: [ 0], batch: [  355/  468], training loss: [0.02817604], train_acc: [0.9844], test_acc: [0.9731]\n",
      "Epoch: [ 0], batch: [  356/  468], training loss: [0.03272220], train_acc: [0.9922], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  357/  468], training loss: [0.02795702], train_acc: [0.9922], test_acc: [0.9728]\n",
      "Epoch: [ 0], batch: [  358/  468], training loss: [0.05050012], train_acc: [0.9766], test_acc: [0.9723]\n",
      "Epoch: [ 0], batch: [  359/  468], training loss: [0.01557469], train_acc: [1.0000], test_acc: [0.9728]\n",
      "Epoch: [ 0], batch: [  360/  468], training loss: [0.01825329], train_acc: [0.9844], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  361/  468], training loss: [0.04023158], train_acc: [1.0000], test_acc: [0.9723]\n",
      "Epoch: [ 0], batch: [  362/  468], training loss: [0.08262511], train_acc: [0.9922], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  363/  468], training loss: [0.02614632], train_acc: [1.0000], test_acc: [0.9727]\n",
      "Epoch: [ 0], batch: [  364/  468], training loss: [0.06183400], train_acc: [0.9688], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [  365/  468], training loss: [0.03268594], train_acc: [0.9688], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [  366/  468], training loss: [0.02025134], train_acc: [1.0000], test_acc: [0.9745]\n",
      "Epoch: [ 0], batch: [  367/  468], training loss: [0.02537971], train_acc: [0.9922], test_acc: [0.9743]\n",
      "Epoch: [ 0], batch: [  368/  468], training loss: [0.05133744], train_acc: [0.9844], test_acc: [0.9748]\n",
      "Epoch: [ 0], batch: [  369/  468], training loss: [0.06360082], train_acc: [0.9844], test_acc: [0.9745]\n",
      "Epoch: [ 0], batch: [  370/  468], training loss: [0.06319095], train_acc: [0.9766], test_acc: [0.9743]\n",
      "Epoch: [ 0], batch: [  371/  468], training loss: [0.08799422], train_acc: [0.9688], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [  372/  468], training loss: [0.06662650], train_acc: [0.9922], test_acc: [0.9738]\n",
      "Epoch: [ 0], batch: [  373/  468], training loss: [0.03037560], train_acc: [1.0000], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  374/  468], training loss: [0.05158357], train_acc: [0.9688], test_acc: [0.9729]\n",
      "Epoch: [ 0], batch: [  375/  468], training loss: [0.03987295], train_acc: [0.9766], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  376/  468], training loss: [0.05157375], train_acc: [0.9844], test_acc: [0.9732]\n",
      "Epoch: [ 0], batch: [  377/  468], training loss: [0.04348313], train_acc: [0.9922], test_acc: [0.9730]\n",
      "Epoch: [ 0], batch: [  378/  468], training loss: [0.05643609], train_acc: [0.9844], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  379/  468], training loss: [0.03563932], train_acc: [0.9766], test_acc: [0.9697]\n",
      "Epoch: [ 0], batch: [  380/  468], training loss: [0.03411495], train_acc: [0.9922], test_acc: [0.9682]\n",
      "Epoch: [ 0], batch: [  381/  468], training loss: [0.06001811], train_acc: [0.9688], test_acc: [0.9667]\n",
      "Epoch: [ 0], batch: [  382/  468], training loss: [0.03703510], train_acc: [0.9922], test_acc: [0.9656]\n",
      "Epoch: [ 0], batch: [  383/  468], training loss: [0.05965076], train_acc: [0.9531], test_acc: [0.9651]\n",
      "Epoch: [ 0], batch: [  384/  468], training loss: [0.03528745], train_acc: [0.9688], test_acc: [0.9667]\n",
      "Epoch: [ 0], batch: [  385/  468], training loss: [0.02090332], train_acc: [0.9922], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  386/  468], training loss: [0.03506522], train_acc: [0.9922], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  387/  468], training loss: [0.06643558], train_acc: [0.9609], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  388/  468], training loss: [0.01157153], train_acc: [1.0000], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  389/  468], training loss: [0.05681332], train_acc: [0.9844], test_acc: [0.9724]\n",
      "Epoch: [ 0], batch: [  390/  468], training loss: [0.03261688], train_acc: [0.9844], test_acc: [0.9719]\n",
      "Epoch: [ 0], batch: [  391/  468], training loss: [0.05446781], train_acc: [0.9844], test_acc: [0.9729]\n",
      "Epoch: [ 0], batch: [  392/  468], training loss: [0.05343566], train_acc: [0.9844], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [  393/  468], training loss: [0.01239108], train_acc: [0.9844], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  394/  468], training loss: [0.07895575], train_acc: [0.9844], test_acc: [0.9741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0], batch: [  395/  468], training loss: [0.06292757], train_acc: [0.9844], test_acc: [0.9741]\n",
      "Epoch: [ 0], batch: [  396/  468], training loss: [0.04205932], train_acc: [0.9766], test_acc: [0.9740]\n",
      "Epoch: [ 0], batch: [  397/  468], training loss: [0.07023285], train_acc: [0.9688], test_acc: [0.9737]\n",
      "Epoch: [ 0], batch: [  398/  468], training loss: [0.01554001], train_acc: [1.0000], test_acc: [0.9734]\n",
      "Epoch: [ 0], batch: [  399/  468], training loss: [0.03150307], train_acc: [0.9766], test_acc: [0.9733]\n",
      "Epoch: [ 0], batch: [  400/  468], training loss: [0.05385920], train_acc: [0.9844], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  401/  468], training loss: [0.01333956], train_acc: [1.0000], test_acc: [0.9725]\n",
      "Epoch: [ 0], batch: [  402/  468], training loss: [0.02677396], train_acc: [1.0000], test_acc: [0.9720]\n",
      "Epoch: [ 0], batch: [  403/  468], training loss: [0.06360447], train_acc: [0.9766], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  404/  468], training loss: [0.05259874], train_acc: [0.9766], test_acc: [0.9704]\n",
      "Epoch: [ 0], batch: [  405/  468], training loss: [0.01983552], train_acc: [0.9922], test_acc: [0.9706]\n",
      "Epoch: [ 0], batch: [  406/  468], training loss: [0.03842712], train_acc: [0.9922], test_acc: [0.9702]\n",
      "Epoch: [ 0], batch: [  407/  468], training loss: [0.04845086], train_acc: [0.9609], test_acc: [0.9704]\n",
      "Epoch: [ 0], batch: [  408/  468], training loss: [0.03217606], train_acc: [0.9922], test_acc: [0.9707]\n",
      "Epoch: [ 0], batch: [  409/  468], training loss: [0.03937491], train_acc: [0.9844], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  410/  468], training loss: [0.13818955], train_acc: [0.9688], test_acc: [0.9714]\n",
      "Epoch: [ 0], batch: [  411/  468], training loss: [0.02481824], train_acc: [0.9922], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  412/  468], training loss: [0.06009823], train_acc: [0.9766], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  413/  468], training loss: [0.04842729], train_acc: [0.9844], test_acc: [0.9724]\n",
      "Epoch: [ 0], batch: [  414/  468], training loss: [0.04082026], train_acc: [0.9766], test_acc: [0.9726]\n",
      "Epoch: [ 0], batch: [  415/  468], training loss: [0.02190965], train_acc: [0.9922], test_acc: [0.9717]\n",
      "Epoch: [ 0], batch: [  416/  468], training loss: [0.05228285], train_acc: [0.9766], test_acc: [0.9723]\n",
      "Epoch: [ 0], batch: [  417/  468], training loss: [0.07841048], train_acc: [0.9844], test_acc: [0.9731]\n",
      "Epoch: [ 0], batch: [  418/  468], training loss: [0.03145130], train_acc: [0.9844], test_acc: [0.9728]\n",
      "Epoch: [ 0], batch: [  419/  468], training loss: [0.03300109], train_acc: [0.9922], test_acc: [0.9733]\n",
      "Epoch: [ 0], batch: [  420/  468], training loss: [0.04744875], train_acc: [0.9766], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  421/  468], training loss: [0.03170738], train_acc: [0.9922], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  422/  468], training loss: [0.01851501], train_acc: [0.9922], test_acc: [0.9731]\n",
      "Epoch: [ 0], batch: [  423/  468], training loss: [0.01829482], train_acc: [0.9922], test_acc: [0.9738]\n",
      "Epoch: [ 0], batch: [  424/  468], training loss: [0.03695744], train_acc: [0.9922], test_acc: [0.9737]\n",
      "Epoch: [ 0], batch: [  425/  468], training loss: [0.03349055], train_acc: [1.0000], test_acc: [0.9736]\n",
      "Epoch: [ 0], batch: [  426/  468], training loss: [0.01504281], train_acc: [1.0000], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  427/  468], training loss: [0.03940262], train_acc: [0.9844], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  428/  468], training loss: [0.03607484], train_acc: [0.9766], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  429/  468], training loss: [0.02500605], train_acc: [1.0000], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [  430/  468], training loss: [0.10773531], train_acc: [0.9766], test_acc: [0.9741]\n",
      "Epoch: [ 0], batch: [  431/  468], training loss: [0.04799653], train_acc: [0.9844], test_acc: [0.9742]\n",
      "Epoch: [ 0], batch: [  432/  468], training loss: [0.02303738], train_acc: [0.9922], test_acc: [0.9735]\n",
      "Epoch: [ 0], batch: [  433/  468], training loss: [0.02498170], train_acc: [0.9922], test_acc: [0.9728]\n",
      "Epoch: [ 0], batch: [  434/  468], training loss: [0.04583461], train_acc: [0.9844], test_acc: [0.9732]\n",
      "Epoch: [ 0], batch: [  435/  468], training loss: [0.04324409], train_acc: [0.9922], test_acc: [0.9723]\n",
      "Epoch: [ 0], batch: [  436/  468], training loss: [0.05157691], train_acc: [0.9688], test_acc: [0.9724]\n",
      "Epoch: [ 0], batch: [  437/  468], training loss: [0.11543936], train_acc: [0.9609], test_acc: [0.9718]\n",
      "Epoch: [ 0], batch: [  438/  468], training loss: [0.02675262], train_acc: [0.9844], test_acc: [0.9711]\n",
      "Epoch: [ 0], batch: [  439/  468], training loss: [0.05871239], train_acc: [0.9844], test_acc: [0.9701]\n",
      "Epoch: [ 0], batch: [  440/  468], training loss: [0.01938229], train_acc: [0.9844], test_acc: [0.9687]\n",
      "Epoch: [ 0], batch: [  441/  468], training loss: [0.03076371], train_acc: [0.9922], test_acc: [0.9668]\n",
      "Epoch: [ 0], batch: [  442/  468], training loss: [0.02004911], train_acc: [0.9922], test_acc: [0.9658]\n",
      "Epoch: [ 0], batch: [  443/  468], training loss: [0.02632810], train_acc: [0.9844], test_acc: [0.9651]\n",
      "Epoch: [ 0], batch: [  444/  468], training loss: [0.13976261], train_acc: [0.9688], test_acc: [0.9648]\n",
      "Epoch: [ 0], batch: [  445/  468], training loss: [0.05732135], train_acc: [0.9688], test_acc: [0.9638]\n",
      "Epoch: [ 0], batch: [  446/  468], training loss: [0.04374781], train_acc: [0.9844], test_acc: [0.9628]\n",
      "Epoch: [ 0], batch: [  447/  468], training loss: [0.12343419], train_acc: [0.9531], test_acc: [0.9630]\n",
      "Epoch: [ 0], batch: [  448/  468], training loss: [0.01971901], train_acc: [1.0000], test_acc: [0.9638]\n",
      "Epoch: [ 0], batch: [  449/  468], training loss: [0.05911996], train_acc: [0.9609], test_acc: [0.9636]\n",
      "Epoch: [ 0], batch: [  450/  468], training loss: [0.07729816], train_acc: [0.9766], test_acc: [0.9652]\n",
      "Epoch: [ 0], batch: [  451/  468], training loss: [0.04672323], train_acc: [0.9844], test_acc: [0.9666]\n",
      "Epoch: [ 0], batch: [  452/  468], training loss: [0.06162968], train_acc: [0.9609], test_acc: [0.9674]\n",
      "Epoch: [ 0], batch: [  453/  468], training loss: [0.04890299], train_acc: [0.9844], test_acc: [0.9690]\n",
      "Epoch: [ 0], batch: [  454/  468], training loss: [0.01974465], train_acc: [1.0000], test_acc: [0.9691]\n",
      "Epoch: [ 0], batch: [  455/  468], training loss: [0.02097992], train_acc: [0.9766], test_acc: [0.9697]\n",
      "Epoch: [ 0], batch: [  456/  468], training loss: [0.10309509], train_acc: [0.9688], test_acc: [0.9698]\n",
      "Epoch: [ 0], batch: [  457/  468], training loss: [0.05447098], train_acc: [0.9844], test_acc: [0.9704]\n",
      "Epoch: [ 0], batch: [  458/  468], training loss: [0.01418387], train_acc: [1.0000], test_acc: [0.9709]\n",
      "Epoch: [ 0], batch: [  459/  468], training loss: [0.09286726], train_acc: [0.9844], test_acc: [0.9713]\n",
      "Epoch: [ 0], batch: [  460/  468], training loss: [0.03720627], train_acc: [1.0000], test_acc: [0.9700]\n",
      "Epoch: [ 0], batch: [  461/  468], training loss: [0.00656587], train_acc: [1.0000], test_acc: [0.9692]\n",
      "Epoch: [ 0], batch: [  462/  468], training loss: [0.11828841], train_acc: [0.9609], test_acc: [0.9695]\n",
      "Epoch: [ 0], batch: [  463/  468], training loss: [0.01606732], train_acc: [0.9766], test_acc: [0.9688]\n",
      "Epoch: [ 0], batch: [  464/  468], training loss: [0.12311310], train_acc: [0.9531], test_acc: [0.9689]\n",
      "Epoch: [ 0], batch: [  465/  468], training loss: [0.01174982], train_acc: [0.9922], test_acc: [0.9684]\n",
      "Epoch: [ 0], batch: [  466/  468], training loss: [0.04900890], train_acc: [0.9688], test_acc: [0.9696]\n",
      "Epoch: [ 0], batch: [  467/  468], training loss: [0.03930691], train_acc: [0.9844], test_acc: [0.9701]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    for idx, (train_input, train_label) in enumerate(train_dataset):\n",
    "        grads = grad(network, train_input, train_label)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, network.trainable_variables))\n",
    "        \n",
    "        train_loss = loss_fn(network, train_input, train_label)\n",
    "        train_accuracy = accuracy_fn(network, train_input, train_label)\n",
    "        \n",
    "        for test_input, test_label in test_dataset:\n",
    "            test_accuracy = accuracy_fn(network, test_input, test_label)\n",
    "       \n",
    "        print(\"Epoch: [%2d], batch: [%5d/%5d], training loss: [%.8f], train_acc: [%.4f], test_acc: [%.4f]\"\\\n",
    "              %(epoch, idx, training_iterations,train_loss, train_accuracy,test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cbe7e",
   "metadata": {},
   "source": [
    "학습에서 기울기를 적용할때에도 optimizer.apply_gradients(grads_and_vars=zip(grads, network.trainable_variables))로 바꿔줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b8b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32a4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197.514px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
